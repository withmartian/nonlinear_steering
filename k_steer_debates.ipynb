{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82acAhWYGIPx"
   },
   "source": [
    "# Demo of bypassing refusal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "s-_vu8HuGEb-"
   },
   "source": [
    ">[Demo of bypassing refusal](#scrollTo=82acAhWYGIPx)\n",
    "\n",
    ">>[Setup](#scrollTo=fcxHyDZw6b86)\n",
    "\n",
    ">>>[Load model](#scrollTo=6ZOoJagxD49V)\n",
    "\n",
    ">>>[Load harmful / harmless datasets](#scrollTo=rF7e-u20EFTe)\n",
    "\n",
    ">>>[Tokenization utils](#scrollTo=KOKYA61k8LWt)\n",
    "\n",
    ">>>[Generation utils](#scrollTo=gtrIK8x78SZh)\n",
    "\n",
    ">>[Finding the \"refusal direction\"](#scrollTo=W9O8dm0_EQRk)\n",
    "\n",
    ">>[Ablate \"refusal direction\" via inference-time intervention](#scrollTo=2EoxY5i1CWe3)\n",
    "\n",
    ">>[Orthogonalize weights w.r.t. \"refusal direction\"](#scrollTo=t9KooaWaCDc_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7hOtw7UHXdD"
   },
   "source": [
    "This notebook demonstrates our method for bypassing refusal, levaraging the insight that refusal is mediated by a 1-dimensional subspace.\n",
    "\n",
    "Please see our [research post](https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction) or our [paper](https://arxiv.org/abs/2406.11717) for a more thorough treatment.\n",
    "\n",
    "In this minimal demo, we use [Qwen-1_8B-Chat](https://huggingface.co/Qwen/Qwen-1_8B-Chat) and implement interventions and weight updates using [TransformerLens](https://github.com/neelnanda-io/TransformerLens). To extract the \"refusal direction,\" we use just 32 harmful instructions from [AdvBench](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv) and 32 harmless instructions from [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy==1.24.3 in /home/ubuntu/.local/lib/python3.10/site-packages (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install numpy==1.24.3  # Last 1.x version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcxHyDZw6b86"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dLeei4-T6Wef"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torchvision transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama numpy scikit-learn python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_vhhwl-2-jPg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-11 01:47:19.489385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746928039.508792   43113 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746928039.515060   43113 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import textwrap\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int\n",
    "from colorama import Fore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility across random, numpy, and torch (CPU & CUDA).\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set. Default is 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the huggingface keys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZOoJagxD49V"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Vnp65Vsg5x-5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-3B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "#MODEL_PATH = 'Qwen/Qwen-1_8B-chat'\n",
    "#MODEL_PATH = 'Llama-2-70b-chat'\n",
    "MODEL_PATH = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_PATH,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float16,\n",
    "    default_padding_side='left',\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "model.tokenizer.padding_side = 'left'\n",
    "model.tokenizer.pad_token = model.tokenizer.eos_token \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF7e-u20EFTe"
   },
   "source": [
    "### Load harmful / harmless datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5i1XcVIgHEE1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_k_ary_instruction(style=\"Empirical Grounding\", repo_name=\"Narmeen07/debate_style_agnostic_questions\"):\n",
    "    \"\"\"\n",
    "    Load tone-agnostic questions from Hugging Face and format them with a specific debate style instruction.\n",
    "    \n",
    "    Args:\n",
    "        style (str): The debate style to use for responses. \n",
    "                    Options: 'Reductio ad Absurdum', 'Appeal to Precedent', 'Straw Man Reframing',\n",
    "                    'Burden of Proof Shift', 'Analogy Construction', 'Concession and Pivot',\n",
    "                    'Empirical Grounding', 'Moral Framing', 'Refutation by Distinction', 'Circular Anticipation'.\n",
    "                    Default is 'Empirical Grounding'.\n",
    "        repo_name (str): The Hugging Face repository name containing the dataset.\n",
    "                         Default is 'Narmeen07/tone_agnostic_questions'.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_data, test_data) as lists of formatted prompts with debate style instructions.\n",
    "    \"\"\"\n",
    "    # Define highly distinctive debate style instructions that will produce clearly different responses\n",
    "    debate_style_instructions = {\n",
    "        \"Reductio ad Absurdum\": \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\n\",\n",
    "        \n",
    "        \"Appeal to Precedent\": \"RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\n\",\n",
    "        \n",
    "        \"Straw Man Reframing\": \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\n\",\n",
    "        \n",
    "        \"Burden of Proof Shift\": \"RESPOND USING BURDEN OF PROOF SHIFT DEBATE STYLE. Redirect the responsibility for providing evidence to your opponent rather than proving your own claim. Challenge your opponent to disprove your assertion rather than supporting it yourself. Use phrases like 'There's no evidence that disproves...' or 'Can you definitively prove that isn't the case?' Position your claims as the default assumption that should be accepted until disproven. Question the sufficiency or quality of opposing evidence while demanding an impossibly high standard of proof. Emphasize that the lack of counter-evidence strengthens your position.\\n\\n\",\n",
    "        \n",
    "        \"Analogy Construction\": \"RESPOND USING ANALOGY CONSTRUCTION DEBATE STYLE. Develop a vivid, relatable comparison between the complex issue at hand and something more familiar or intuitive. Build your argument around this carefully constructed parallel situation. Highlight specific points of similarity that support your position while addressing potential dissimilarities. Use phrases like 'This situation is similar to...' or 'To understand this concept, consider...' Ensure your analogy simplifies the complex issue without distorting its essential nature. Use the familiar scenario to guide your audience to your desired conclusion about the original issue.\\n\\n\",\n",
    "        \n",
    "        \"Concession and Pivot\": \"RESPOND USING CONCESSION AND PIVOT DEBATE STYLE. Begin by acknowledging a minor point or critique from the opposing side to establish fairness and reasonableness. Use phrases like 'While it's true that...' or 'I can concede that...' followed by 'However,' 'Nevertheless,' or 'That said,' to redirect to your stronger arguments. Ensure the conceded point is peripheral rather than central to your main argument. After the concession, pivot decisively to your strongest points with increased emphasis. Frame your pivot as providing necessary context or a more complete perspective. Use the concession to demonstrate your objectivity before delivering your more powerful counterarguments.\\n\\n\",\n",
    "        \n",
    "        \"Empirical Grounding\": \"RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\n\",\n",
    "        \n",
    "        \"Moral Framing\": \"RESPOND USING MORAL FRAMING DEBATE STYLE. Position the issue within a framework of ethical principles, values, and moral imperatives rather than pragmatic concerns. Identify the core moral values at stake such as justice, liberty, equality, compassion, or responsibility. Use language that evokes ethical considerations, such as 'obligation,' 'right,' 'wrong,' 'just,' or 'fair.' Appeal to widely held moral intuitions or principles. Present opposing views as morally questionable or inconsistent with important shared values. Elevate the discussion from practical matters to questions of what ought to be done. Emphasize moral consequences over practical outcomes.\\n\\n\",\n",
    "        \n",
    "        \"Refutation by Distinction\": \"RESPOND USING REFUTATION BY DISTINCTION DEBATE STYLE. Identify crucial differences that invalidate comparisons or principles your opponent has applied. Carefully delineate categories, contexts, or circumstances that demonstrate why a general rule or example doesn't apply in this specific case. Use phrases like 'While that may be true in some contexts...' or 'We must distinguish between...' Emphasize the precision of definitions and classifications. Highlight subtle but significant differences that undermine the opponent's logic. Show how these distinctions fundamentally change the assessment of the situation. Demonstrate how recognizing these distinctions leads to a different conclusion than your opponent reached.\\n\\n\",\n",
    "        \n",
    "        \"Circular Anticipation\": \"RESPOND USING CIRCULAR ANTICIPATION DEBATE STYLE. Preemptively identify and address the most likely counterarguments before your opponent can make them. Introduce opposing points with phrases like 'Some might argue...' or 'One could object that...' followed by your prepared refutation. Structure your response to cover all major potential objections. Demonstrate that you've thoroughly considered the issue from multiple angles. Frame potential counterarguments in ways that make them easier to dismantle. Create the impression that all reasonable objections have already been considered and overcome. Conclude by suggesting that any remaining objections would be similarly flawed.\\n\\n\"\n",
    "    }\n",
    "    \n",
    "    # Validate the debate style\n",
    "    if style not in debate_style_instructions:\n",
    "        valid_styles = list(debate_style_instructions.keys())\n",
    "        raise ValueError(f\"Invalid debate style '{style}'. Please choose from {valid_styles}.\")\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        dataset = load_dataset(repo_name, split=\"train\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load dataset from '{repo_name}': {e}\")\n",
    "    \n",
    "    # Format questions with the debate style instruction\n",
    "    def format_prompt(example):\n",
    "        return {\n",
    "            \"formatted_text\": f\"{debate_style_instructions[style]}{example['text']}\"\n",
    "        }\n",
    "    \n",
    "    # Apply transformation\n",
    "    formatted_dataset = dataset.map(format_prompt)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_data, test_data = train_test_split(\n",
    "        formatted_dataset[\"formatted_text\"], \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rth8yvLZJsXs"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Load all the tone dataset\n",
    "reductio_inst_train, reductio_inst_test = get_k_ary_instruction(style=\"Reductio ad Absurdum\")\n",
    "precedent_inst_train, precedent_inst_test = get_k_ary_instruction(style=\"Appeal to Precedent\")\n",
    "strawman_inst_train, strawman_inst_test = get_k_ary_instruction(style=\"Straw Man Reframing\")\n",
    "burden_inst_train, burden_inst_test = get_k_ary_instruction(style=\"Burden of Proof Shift\")\n",
    "analogy_inst_train, analogy_inst_test = get_k_ary_instruction(style=\"Analogy Construction\")\n",
    "concession_inst_train, concession_inst_test = get_k_ary_instruction(style=\"Concession and Pivot\")\n",
    "empirical_inst_train, empirical_inst_test = get_k_ary_instruction(style=\"Empirical Grounding\")\n",
    "moral_inst_train, moral_inst_test = get_k_ary_instruction(style=\"Moral Framing\")\n",
    "refutation_inst_train, refutation_inst_test = get_k_ary_instruction(style=\"Refutation by Distinction\")\n",
    "circular_inst_train, circular_inst_test = get_k_ary_instruction(style=\"Circular Anticipation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Qv2ALDY_J44G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reductio instructions:\n",
      "\t\"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nIs the government's ability to impose restrictions on free speech justified in instances of misinformation that could lead to public harm?\"\n",
      "\t\"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nShould the responsibilities of raising children be equally shared between parents?\"\n",
      "\t\"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nIs the healthcare system in the United States fundamentally flawed, or does it simply require reform?\"\n",
      "\t\"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nShould international corporations be required to disclose their carbon footprints publicly?\"\n",
      "Precendent instructions:\n",
      "\t\"RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nIs the government's ability to impose restrictions on free speech justified in instances of misinformation that could lead to public harm?\"\n",
      "\t'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nShould the responsibilities of raising children be equally shared between parents?'\n",
      "\t'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nIs the healthcare system in the United States fundamentally flawed, or does it simply require reform?'\n",
      "\t'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nShould international corporations be required to disclose their carbon footprints publicly?'\n",
      "Empirical instructions:\n",
      "\t\"RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nIs the government's ability to impose restrictions on free speech justified in instances of misinformation that could lead to public harm?\"\n",
      "\t'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould the responsibilities of raising children be equally shared between parents?'\n",
      "\t'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nIs the healthcare system in the United States fundamentally flawed, or does it simply require reform?'\n",
      "\t'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould international corporations be required to disclose their carbon footprints publicly?'\n"
     ]
    }
   ],
   "source": [
    "print(\"Reductio instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(reductio_inst_train[i])}\")\n",
    "\n",
    "print(\"Precendent instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(precedent_inst_train[i])}\")\n",
    "\n",
    "\n",
    "print(\"Empirical instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(empirical_inst_train[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOKYA61k8LWt"
   },
   "source": [
    "### Tokenization utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P8UPQSfpWOSK"
   },
   "outputs": [],
   "source": [
    "QWEN_CHAT_TEMPLATE = \"\"\"<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tokenize_instructions_qwen_chat(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: List[str]\n",
    ") -> Int[Tensor, 'batch_size seq_len']:\n",
    "    prompts = [QWEN_CHAT_TEMPLATE.format(instruction=instruction) for instruction in instructions]\n",
    "    return tokenizer(prompts, padding=True,truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "tokenize_instructions_fn = functools.partial(tokenize_instructions_qwen_chat, tokenizer=model.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtrIK8x78SZh"
   },
   "source": [
    "### Generation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "94jRJDR0DRoY"
   },
   "outputs": [],
   "source": [
    "def _generate_with_hooks(\n",
    "    model: HookedTransformer,\n",
    "    toks: Int[Tensor, 'batch_size seq_len'],\n",
    "    max_tokens_generated: int = 64,\n",
    "    fwd_hooks = [],\n",
    ") -> List[str]:\n",
    "\n",
    "    all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=toks.device)\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "\n",
    "    for i in range(max_tokens_generated):\n",
    "        with model.hooks(fwd_hooks=fwd_hooks):\n",
    "            logits = model(all_toks[:, :-max_tokens_generated + i])\n",
    "            next_tokens = logits[:, -1, :].argmax(dim=-1) # greedy sampling (temperature=0)\n",
    "            all_toks[:,-max_tokens_generated+i] = next_tokens\n",
    "\n",
    "    return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "def get_generations(\n",
    "    model: HookedTransformer,\n",
    "    instructions: List[str],\n",
    "    tokenize_instructions_fn: Callable[[List[str]], Int[Tensor, 'batch_size seq_len']],\n",
    "    fwd_hooks = [],\n",
    "    max_tokens_generated: int = 64,\n",
    "    batch_size: int = 1,\n",
    ") -> List[str]:\n",
    "\n",
    "    generations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
    "        generation = _generate_with_hooks(\n",
    "            model,\n",
    "            toks,\n",
    "            max_tokens_generated=max_tokens_generated,\n",
    "            fwd_hooks=fwd_hooks,\n",
    "        )\n",
    "        generations.extend(generation)\n",
    "\n",
    "    return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:26<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDACTIO INSTRUCTION 0: \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nIs the government's ability to impose restrictions on free speech justified in instances of misinformation that could lead to public harm?\"\n",
      "\u001b[32mREDACTIO COMPLETION:\n",
      "\t\"To address this question, let's assume the opposing viewpoint that the government's ability to\n",
      "\timpose restrictions on free speech is justified in instances of misinformation that could lead to\n",
      "\tpublic harm. If we follow this logic, then:\\n\\n1. **Totalitarianism**: If the government can\n",
      "\trestrict free speech to prevent misinformation, it implies that\"\n",
      "REDACTIO INSTRUCTION 1: \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nShould the responsibilities of raising children be equally shared between parents?\"\n",
      "\u001b[32mREDACTIO COMPLETION:\n",
      "\t\"To address the question of whether the responsibilities of raising children should be equally\n",
      "\tshared between parents, let's consider the opposing viewpoint and extend it to its logical extreme\n",
      "\tto reveal its flaws.\\n\\nAssuming that the responsibilities of raising children should be equally\n",
      "\tshared between parents, we can infer that both parents should have an equal say in all\"\n",
      "REDACTIO INSTRUCTION 2: \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nIs the healthcare system in the United States fundamentally flawed, or does it simply require reform?\"\n",
      "\u001b[32mREDACTIO COMPLETION:\n",
      "\t'To argue that the healthcare system in the United States is fundamentally flawed, I will extend\n",
      "\tthe opposing viewpoint to its logical extreme, revealing its flaws.\\n\\nIf we assume that the\n",
      "\thealthcare system in the United States is fundamentally flawed, then it must be the case that every\n",
      "\taspect of the system is inherently broken. This means that every'\n",
      "REDACTIO INSTRUCTION 3: \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nShould international corporations be required to disclose their carbon footprints publicly?\"\n",
      "\u001b[32mREDACTIO COMPLETION:\n",
      "\t\"To address the question of whether international corporations should be required to disclose their\n",
      "\tcarbon footprints publicly, let's consider the opposing viewpoint and extend it to its logical\n",
      "\textreme.\\n\\nAssuming that international corporations should not be required to disclose their\n",
      "\tcarbon footprints publicly, we can infer that the primary concern is the potential economic burden\n",
      "\ton these\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:26<00:00,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECEDENT INSTRUCTION 0: 'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?'\n",
      "\u001b[32mPRECEDENT COMPLETION:\n",
      "\t'To address the question of whether there should be an ethical framework governing the development\n",
      "\tof human enhancement technologies, it is essential to consider the precedents set by previous\n",
      "\ttechnological advancements and the lessons learned from their development.\\n\\nOne notable example\n",
      "\tis the development of nuclear energy. In the 1950s and 1960s, the'\n",
      "PRECEDENT INSTRUCTION 1: 'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nIs it ethical to judge historical figures by modern moral standards?'\n",
      "\u001b[32mPRECEDENT COMPLETION:\n",
      "\t'The question of whether it is ethical to judge historical figures by modern moral standards is a\n",
      "\tcomplex and contentious issue. However, a careful examination of the precedents set by scholars,\n",
      "\thistorians, and legal scholars suggests that it is not only permissible but also necessary to\n",
      "\tevaluate historical figures by the moral standards of their time.\\n\\nOne of'\n",
      "PRECEDENT INSTRUCTION 2: 'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?'\n",
      "\u001b[32mPRECEDENT COMPLETION:\n",
      "\t'To address the question of whether the benefits of telemedicine outweigh the potential risks to\n",
      "\tpatient care, it is essential to examine the precedents set by previous experiences and court\n",
      "\trulings. The landmark case of _Smith v. Smith_ (2015) established that telemedicine can be a viable\n",
      "\talternative to traditional in-person care,'\n",
      "PRECEDENT INSTRUCTION 3: 'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nShould children be taught coding and digital literacy as a core part of their education?'\n",
      "\u001b[32mPRECEDENT COMPLETION:\n",
      "\t'The notion of incorporating coding and digital literacy into the core curriculum of education is a\n",
      "\ttopic that has garnered significant attention in recent years. As we examine this issue, it is\n",
      "\tessential to consider the precedents set by educational institutions and policymakers in the past.\n",
      "\tA thorough analysis of historical examples, established case law, and widely accepted'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:26<00:00,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMPIRICAL INSTRUCTION 0: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?'\n",
      "\u001b[32mEMPIRICAL COMPLETION:\n",
      "\t'The development of human enhancement technologies (HETs) has sparked intense debate about the need\n",
      "\tfor an ethical framework. While some argue that HETs can improve human lives, others contend that\n",
      "\tthey pose significant risks and moral concerns. In this response, I will present empirical evidence\n",
      "\tto support the argument that an ethical framework governing'\n",
      "EMPIRICAL INSTRUCTION 1: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nIs it ethical to judge historical figures by modern moral standards?'\n",
      "\u001b[32mEMPIRICAL COMPLETION:\n",
      "\t\"The question of whether it's ethical to judge historical figures by modern moral standards is a\n",
      "\tcomplex one, and I'd like to present some empirical evidence to support my argument that it's not\n",
      "\tentirely ethical.\\n\\nFirstly, let's consider the concept of cultural relativism. This theory\n",
      "\tsuggests that moral judgments are relative to the cultural\"\n",
      "EMPIRICAL INSTRUCTION 2: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?'\n",
      "\u001b[32mEMPIRICAL COMPLETION:\n",
      "\t\"To address this question, let's examine the empirical evidence on both sides of the\n",
      "\tdebate.\\n\\n**Benefits of Telemedicine:**\\n\\nNumerous studies have demonstrated the benefits of\n",
      "\ttelemedicine in improving patient outcomes, increasing access to care, and reducing healthcare\n",
      "\tcosts. A systematic review of 22 studies on telemedicine published in the Journal\"\n",
      "EMPIRICAL INSTRUCTION 3: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould children be taught coding and digital literacy as a core part of their education?'\n",
      "\u001b[32mEMPIRICAL COMPLETION:\n",
      "\t'**Argument in Favor of Teaching Coding and Digital Literacy**\\n\\nResearch suggests that teaching\n",
      "\tcoding and digital literacy to children can have a significant impact on their future career\n",
      "\tprospects, problem-solving skills, and overall academic performance. A study by the National Center\n",
      "\tfor Education Statistics found that students who learned to code in middle school were more likely'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "redactio_generations = get_generations(\n",
    "    model,\n",
    "    reductio_inst_train[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"REDACTIO INSTRUCTION {i}: {repr(reductio_inst_train[i])}\")\n",
    "    print(Fore.GREEN + f\"REDACTIO COMPLETION:\")\n",
    "    print(textwrap.fill(repr(redactio_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "\n",
    "precedent_generations = get_generations(\n",
    "    model,\n",
    "    precedent_inst_test[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"PRECEDENT INSTRUCTION {i}: {repr(precedent_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"PRECEDENT COMPLETION:\")\n",
    "    print(textwrap.fill(repr(precedent_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "\n",
    "\n",
    "# do the above for empathetic generations\n",
    "empirical_generations = get_generations(\n",
    "    model,\n",
    "    empirical_inst_test[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"EMPIRICAL INSTRUCTION {i}: {repr(empirical_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"EMPIRICAL COMPLETION:\")\n",
    "    print(textwrap.fill(repr(empirical_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's process the activations for all the tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def batch_process_activations(instructions, name):\n",
    "    \"\"\"Process instructions in batches to manage memory usage.\"\"\"\n",
    "    acts_list = []\n",
    "    \n",
    "    for start_idx in range(0,len(instructions), BATCH_SIZE):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, len(instructions))\n",
    "        print(f\"Processing {name} batch {start_idx} to {end_idx}\")\n",
    "        \n",
    "        # Tokenize this batch of instructions\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[start_idx:end_idx])\n",
    "        \n",
    "        # Run model with cache\n",
    "        logits, cache = model.run_with_cache(\n",
    "            toks, \n",
    "            names_filter=lambda hook_name: 'resid' in hook_name\n",
    "        )\n",
    "        \n",
    "        # Extract only the activations we need (layer and position)\n",
    "        batch_acts = cache['resid_pre', layer][:, pos, :].detach().cpu().numpy()\n",
    "        \n",
    "        # Store these activations\n",
    "        acts_list.append(batch_acts)\n",
    "        \n",
    "        # Clear cache to free memory\n",
    "        del cache\n",
    "        del logits\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Combine all the batch activations\n",
    "    acts = np.concatenate(acts_list, axis=0)\n",
    "    print(f\"Final shape of {name} activations:\", acts.shape)\n",
    "    return acts\n",
    "\n",
    "# Configuration\n",
    "TOTAL_SAMPLES = 600  # Change this to your desired total\n",
    "BATCH_SIZE = 16  # Current max you can handle in one batch\n",
    "layer = 14\n",
    "pos = -1\n",
    "# Dictionary to hold all activations\n",
    "all_activations = {}\n",
    "\n",
    "# Process harmful and harmless instructions\n",
    "#all_activations['harmful'] = batch_process_activations(harmful_inst_train[:TOTAL_SAMPLES], \"harmful\")\n",
    "#all_activations['harmless'] = batch_process_activations(harmless_inst_train[:TOTAL_SAMPLES], \"harmless\")\n",
    "\n",
    "# Process debate style instructions\n",
    "debate_style_datasets = {\n",
    "    'reductio': reductio_inst_train,\n",
    "    'precedent': precedent_inst_train,\n",
    "    'strawman': strawman_inst_train,\n",
    "    'burden': burden_inst_train,\n",
    "    'analogy': analogy_inst_train,\n",
    "    'concession': concession_inst_train,\n",
    "    'empirical': empirical_inst_train,\n",
    "    'moral': moral_inst_train,\n",
    "    'refutation': refutation_inst_train,\n",
    "    'circular': circular_inst_train\n",
    "}\n",
    "\n",
    "for style_name, style_data in debate_style_datasets.items():\n",
    "    all_activations[style_name] = batch_process_activations(\n",
    "        style_data[:TOTAL_SAMPLES], \n",
    "        style_name\n",
    "    )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(all_activations, \"debate_activations.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_activations = torch.load(\"debate_activations.pt\",weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's steer against all the tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "exUh3PEHRe9x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiToneActivationSteering:\n",
    "    def __init__(self, input_dim, num_classes=6, hidden_dim=256, lr=1e-3, device='cuda'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          - input_dim: dimensionality of your raw activations.\n",
    "          - num_classes: number of tone classes to classify.\n",
    "          - hidden_dim: size of the hidden layer in the MLP.\n",
    "          - lr: learning rate for training the classifier.\n",
    "          - device: 'cuda' or 'cpu'.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define a multi-class MLP classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)  # outputs logits for each tone class\n",
    "        ).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Store class names for reference\n",
    "        self.class_names = None\n",
    "        \n",
    "\n",
    "    def fit(self, activations_dict, epochs=30, batch_size=32, class_weights=None):\n",
    "        \"\"\"\n",
    "        Train the multiclass classifier on raw activations.\n",
    "\n",
    "        Parameters:\n",
    "          - activations_dict: Dictionary mapping class names to NumPy arrays of activations.\n",
    "          - epochs: number of training epochs.\n",
    "          - batch_size: training batch size.\n",
    "          - class_weights: Optional tensor of weights for each class to handle imbalance.\n",
    "        \"\"\"\n",
    "        self.class_names = list(activations_dict.keys())\n",
    "        assert len(self.class_names) == self.num_classes, f\"Expected {self.num_classes} classes, got {len(self.class_names)}\"\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            X_class = torch.tensor(activations_dict[class_name], dtype=torch.float32, device=self.device)\n",
    "            y_class = torch.full((X_class.size(0),), i, dtype=torch.long, device=self.device)\n",
    "            X_list.append(X_class)\n",
    "            y_list.append(y_class)\n",
    "        \n",
    "        X = torch.cat(X_list, dim=0)\n",
    "        y = torch.cat(y_list, dim=0)\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Set class weights for loss function if provided\n",
    "        if class_weights is not None:\n",
    "            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(self.device))\n",
    "        \n",
    "        # Train the classifier\n",
    "        for epoch in range(epochs):\n",
    "            self.classifier.train()\n",
    "            epoch_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for batch_X, batch_y in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.classifier(batch_X)  # shape: (batch, num_classes)\n",
    "                loss = self.loss_fn(logits, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            print(f\"Epoch {epoch}: loss = {epoch_loss/len(loader):.4f}, accuracy = {accuracy:.2f}%\")\n",
    "            \n",
    "        # Evaluate on each class\n",
    "        self.classifier.eval()\n",
    "        class_accuracies = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, class_name in enumerate(self.class_names):\n",
    "                X_class = torch.tensor(activations_dict[class_name], dtype=torch.float32, device=self.device)\n",
    "                y_class = torch.full((X_class.size(0),), i, dtype=torch.long, device=self.device)\n",
    "                \n",
    "                logits = self.classifier(X_class)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                accuracy = 100 * (predicted == y_class).sum().item() / y_class.size(0)\n",
    "                class_accuracies[class_name] = accuracy\n",
    "                print(f\"Accuracy for {class_name}: {accuracy:.2f}%\")\n",
    "\n",
    "    def steer_toward_tones(self, activation, target_tones=None, avoid_tones=None, alpha=0.1, steps=1, decay_rate=0.95):\n",
    "        \"\"\"\n",
    "        Steer the given activation toward target tones and away from tones to avoid.\n",
    "        \n",
    "        Parameters:\n",
    "        - activation: a NumPy array of shape (D,) or (N, D).\n",
    "        - target_tones: list of tone names to steer toward.\n",
    "        - avoid_tones: list of tone names to steer away from.\n",
    "        - alpha: scaling factor for the update.\n",
    "        - steps: number of gradient descent steps to take.\n",
    "        \n",
    "        Returns:\n",
    "        - The modified activation as a NumPy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate input tone names\n",
    "        if target_tones:\n",
    "            for tone in target_tones:\n",
    "                assert tone in self.class_names, f\"Unknown target tone: {tone}\"\n",
    "        \n",
    "        if avoid_tones:\n",
    "            for tone in avoid_tones:\n",
    "                assert tone in self.class_names, f\"Unknown avoid tone: {tone}\"\n",
    "        \n",
    "        # Get indices for target and avoid tones\n",
    "        target_indices = [self.class_names.index(tone) for tone in target_tones] if target_tones else []\n",
    "        avoid_indices = [self.class_names.index(tone) for tone in avoid_tones] if avoid_tones else []\n",
    "        \n",
    "        # If a single activation vector is passed, add a batch dimension\n",
    "        single_input = False\n",
    "        if activation.ndim == 1:\n",
    "            activation = activation[None, :]\n",
    "            single_input = True\n",
    "        \n",
    "        # Convert activation to a torch tensor\n",
    "        X = torch.tensor(activation, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        current_alpha = alpha\n",
    "        \n",
    "        # Perform multiple steps of gradient descent\n",
    "        for step in range(steps):\n",
    "            if step > 0:\n",
    "                current_alpha *= decay_rate\n",
    "            # Need to create a new tensor that requires gradients for each step\n",
    "            X = X.detach().requires_grad_(True)\n",
    "            \n",
    "            # Forward pass: compute classifier output (logits)\n",
    "            self.classifier.eval()\n",
    "            logits = self.classifier(X)  # shape: [N, num_classes]\n",
    "            \n",
    "            # Create custom loss function to maximize target tone scores and minimize avoid tone scores\n",
    "            loss = 0\n",
    "            \n",
    "            if target_indices:\n",
    "                target_logits = logits[:, target_indices]\n",
    "                # Negative because we want to maximize these logits (gradient descent will minimize)\n",
    "                loss = loss - target_logits.mean()\n",
    "            \n",
    "            if avoid_indices:\n",
    "                avoid_logits = logits[:, avoid_indices]\n",
    "                # Positive because we want to minimize these logits\n",
    "                loss = loss + avoid_logits.mean()\n",
    "            \n",
    "            # If no target or avoid tones provided, do nothing\n",
    "            if not target_indices and not avoid_indices:\n",
    "                if single_input:\n",
    "                    return activation[0]\n",
    "                else:\n",
    "                    return activation\n",
    "            \n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Get the gradient with respect to the input activation\n",
    "            grad = X.grad.data\n",
    "            \n",
    "            # Update the activation by moving in the loss gradient direction\n",
    "            # Negative gradient because we're trying to minimize the loss\n",
    "\n",
    "            X = X - current_alpha * grad\n",
    "        \n",
    "        # Convert back to NumPy\n",
    "        X_new_np = X.detach().cpu().numpy()\n",
    "        if single_input:\n",
    "            return X_new_np[0]\n",
    "        else:\n",
    "            return X_new_np\n",
    "    \n",
    "    def remove_tone_projection(self, activation, target_tones=None, avoid_tones=None):\n",
    "        \"\"\"\n",
    "        Remove the projection of the activation onto the tone steering vector.\n",
    "        \n",
    "        Parameters:\n",
    "        - activation: a NumPy array of shape (D,) or (N, D).\n",
    "        - target_tones: list of tone names to steer toward.\n",
    "        - avoid_tones: list of tone names to steer away from.\n",
    "        \n",
    "        Returns:\n",
    "        - The modified activation as a NumPy array with the projection removed.\n",
    "        \"\"\"\n",
    "        assert self.class_names is not None, \"Model must be trained before steering\"\n",
    "        \n",
    "        # Validate input tone names\n",
    "        if target_tones:\n",
    "            for tone in target_tones:\n",
    "                assert tone in self.class_names, f\"Unknown target tone: {tone}\"\n",
    "        \n",
    "        if avoid_tones:\n",
    "            for tone in avoid_tones:\n",
    "                assert tone in self.class_names, f\"Unknown avoid tone: {tone}\"\n",
    "        \n",
    "        # Get indices for target and avoid tones\n",
    "        target_indices = [self.class_names.index(tone) for tone in target_tones] if target_tones else []\n",
    "        avoid_indices = [self.class_names.index(tone) for tone in avoid_tones] if avoid_tones else []\n",
    "        \n",
    "        # If a single activation vector is passed, add a batch dimension\n",
    "        single_input = False\n",
    "        if activation.ndim == 1:\n",
    "            activation = activation[None, :]\n",
    "            single_input = True\n",
    "        \n",
    "        # Convert activation to a torch tensor that requires gradients\n",
    "        X = torch.tensor(activation, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "        \n",
    "        # Forward pass: compute classifier output (logits)\n",
    "        self.classifier.eval()\n",
    "        logits = self.classifier(X)  # shape: [N, num_classes]\n",
    "        \n",
    "        # Create custom loss function to maximize target tone scores and minimize avoid tone scores\n",
    "        loss = 0\n",
    "        \n",
    "        if target_indices:\n",
    "            target_logits = logits[:, target_indices]\n",
    "            # Negative because we want to maximize these logits (gradient descent will minimize)\n",
    "            loss = loss - target_logits.mean()\n",
    "        \n",
    "        if avoid_indices:\n",
    "            avoid_logits = logits[:, avoid_indices]\n",
    "            # Positive because we want to minimize these logits\n",
    "            loss = loss + avoid_logits.mean()\n",
    "        \n",
    "        # If no target or avoid tones provided, do nothing\n",
    "        if not target_indices and not avoid_indices:\n",
    "            if single_input:\n",
    "                return activation[0]\n",
    "            else:\n",
    "                return activation\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Get the gradient with respect to the input activation\n",
    "        grad = X.grad.data  # shape: [N, D]\n",
    "        \n",
    "        # Compute the squared norm of the gradient for each sample (avoid division by zero)\n",
    "        norm_sq = (grad ** 2).sum(dim=1, keepdim=True) + 1e-8  # shape: [N, 1]\n",
    "        \n",
    "        # Compute the dot product between each activation and its corresponding gradient\n",
    "        dot_prod = (X * grad).sum(dim=1, keepdim=True)  # shape: [N, 1]\n",
    "        \n",
    "        # Compute the projection of X onto the gradient\n",
    "        proj = (dot_prod / norm_sq) * grad  # shape: [N, D]\n",
    "        \n",
    "        # Remove the projection from X (multiply by 2 to strengthen the effect)\n",
    "        X_new = X - 2 * proj\n",
    "        \n",
    "        # Convert back to NumPy\n",
    "        X_new_np = X_new.detach().cpu().numpy()\n",
    "        if single_input:\n",
    "            return X_new_np[0]\n",
    "        else:\n",
    "            return X_new_np\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.2118, accuracy = 95.93%\n",
      "Epoch 1: loss = 0.0003, accuracy = 100.00%\n",
      "Epoch 2: loss = 0.0001, accuracy = 100.00%\n",
      "Epoch 3: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 4: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 5: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 6: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 7: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 8: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 9: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 10: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 11: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 12: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 13: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 14: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 15: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 16: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 17: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 18: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 19: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 20: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 21: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 22: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 23: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 24: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 25: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 26: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 27: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 28: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 29: loss = 0.0000, accuracy = 100.00%\n",
      "Accuracy for reductio: 100.00%\n",
      "Accuracy for precedent: 100.00%\n",
      "Accuracy for strawman: 100.00%\n",
      "Accuracy for burden: 100.00%\n",
      "Accuracy for analogy: 100.00%\n",
      "Accuracy for concession: 100.00%\n",
      "Accuracy for empirical: 100.00%\n",
      "Accuracy for moral: 100.00%\n",
      "Accuracy for refutation: 100.00%\n",
      "Accuracy for circular: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract activations at the specific layer and position from cache\n",
    "reductio_acts = all_activations['reductio']  # shape [N, D]\n",
    "precedent_acts = all_activations['precedent']  # shape [M, D]\n",
    "strawman_acts = all_activations['strawman']  # shape [O, D]\n",
    "burden_acts = all_activations['burden']  # shape [P, D]\n",
    "analogy_acts = all_activations['analogy']  # shape [Q, D]\n",
    "concession_acts = all_activations['concession']  # shape [R, D]\n",
    "empirical_acts = all_activations['empirical']  # shape [S, D]\n",
    "moral_acts = all_activations['moral']  # shape [T, D]\n",
    "refutation_acts = all_activations['refutation']  # shape [U, D]\n",
    "circular_acts = all_activations['circular']  # shape [V, D]\n",
    "\n",
    "# Instantiate and fit the multi-debate-style steering classifier\n",
    "debate_steer = MultiToneActivationSteering(input_dim=reductio_acts.shape[1], num_classes=10)\n",
    "debate_steer.fit({\n",
    "    'reductio': reductio_acts,\n",
    "    'precedent': precedent_acts,\n",
    "    'strawman': strawman_acts,\n",
    "    'burden': burden_acts,\n",
    "    'analogy': analogy_acts,\n",
    "    'concession': concession_acts,\n",
    "    'empirical': empirical_acts,\n",
    "    'moral': moral_acts,\n",
    "    'refutation': refutation_acts,\n",
    "    'circular': circular_acts\n",
    "}, epochs=30, batch_size=32)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 hooks :\n",
    "- method 1: removes gradient projection\n",
    "- method 2: subtracts the gradient of towards (minimise towards loss) and adds the gradients of away\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debate_steering_hook(activation, hook,\n",
    "                      target_styles=['empirical', 'refutation'], \n",
    "                      avoid_styles=['strawman']):\n",
    "    \"\"\"\n",
    "    Steer activations toward target debate styles and away from avoid styles.\n",
    "    \"\"\"\n",
    "    shape = activation.shape\n",
    "    # Flatten to [N, d_act]\n",
    "    act_2d = activation.view(-1, shape[-1]).detach().cpu().numpy()\n",
    "    \n",
    "    steered_2d = debate_steer.remove_tone_projection(\n",
    "        act_2d, \n",
    "        target_tones=target_styles,\n",
    "        avoid_tones=avoid_styles,\n",
    "    )\n",
    "\n",
    "    # Convert back to torch and original shape\n",
    "    steered_torch = torch.from_numpy(steered_2d).to(activation.device)\n",
    "    return steered_torch.view(*shape)\n",
    "\n",
    "def actadd_debate_steering_hook(activation, hook,\n",
    "                      target_styles=['empirical', 'refutation'], \n",
    "                      avoid_styles=['strawman'], \n",
    "                      alpha=1,\n",
    "                      steps=1,\n",
    "                      decay_rate=0.95):\n",
    "    \"\"\"\n",
    "    Steer activations toward target debate styles and away from avoid styles.\n",
    "    \"\"\"\n",
    "    shape = activation.shape\n",
    "    # Flatten to [N, d_act]\n",
    "    act_2d = activation.view(-1, shape[-1]).detach().cpu().numpy()\n",
    "    \n",
    "    # Steer in debate style space\n",
    "    steered_2d = debate_steer.steer_toward_tones(\n",
    "        act_2d, \n",
    "        target_tones=target_styles,\n",
    "        avoid_tones=avoid_styles,\n",
    "        alpha=alpha,\n",
    "        steps=steps,\n",
    "        decay_rate=decay_rate\n",
    "    )\n",
    "\n",
    "\n",
    "    # Convert back to torch and original shape\n",
    "    steered_torch = torch.from_numpy(steered_2d).to(activation.device)\n",
    "    return steered_torch.view(*shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try adding one tone and removing another tone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go from strawman to empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:55<00:00, 23.01s/it]\n",
      "100%|██████████| 5/5 [00:26<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION 0: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that you believe human enhancement technologies should be left\n",
      "\tcompletely unregulated, with no oversight or guidelines to ensure their safe and responsible\n",
      "\tdevelopment.\\n\\nThis position boils down to the idea that the pursuit of scientific progress and\n",
      "\tinnovation should take precedence over concerns about ethics and potential consequences. You're\n",
      "\targuing that\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The development of human enhancement technologies is a complex and multifaceted issue that\n",
      "\twarrants a nuanced and evidence-based approach. Here, I will present a comprehensive response that\n",
      "\taddresses the ethical considerations surrounding the development of human enhancement\n",
      "\ttechnologies.\\n\\n**The Need for a Framework**\\n\\nThe rapid advancements in human enhancement\n",
      "\ttechnologies, including genetic engineering, brain'\n",
      "\u001b[39m\n",
      "INSTRUCTION 1: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nIs it ethical to judge historical figures by modern moral standards?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that it's perfectly fine to judge historical figures by the\n",
      "\tsame moral standards we use today, without considering the cultural, social, and historical context\n",
      "\tin which they lived.\\n\\nThis position boils down to the idea that historical figures are simply\n",
      "\tmoral blank slates, devoid of any cultural or historical\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The question of evaluating historical figures through the lens of modern moral standards is a\n",
      "\tcomplex and multifaceted issue. This topic has garnered significant attention in various fields,\n",
      "\tincluding history, philosophy, and ethics.\\n\\n**Argument in Favor of Evaluating Historical Figures\n",
      "\tthrough Modern Moral Standards\\n\\nHistorical figures, such as leaders, politicians,'\n",
      "\u001b[39m\n",
      "INSTRUCTION 2: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that the benefits of telemedicine are so significant that they\n",
      "\tcompletely outweigh any potential risks to patient care, and that any concerns about the impact on\n",
      "\tin-person care are simply irrelevant.\\n\\nThis position boils down to the idea that telemedicine is\n",
      "\ta panacea that can solve all healthcare problems without\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The question of whether the benefits of telemedicine outweigh the potential risks to patient care\n",
      "\tis a complex and multifaceted issue that requires a comprehensive analysis.\\n\\n**Benefits of\n",
      "\tTelemedicine\\n\\nTelemedicine has numerous benefits that can improve patient care in various ways.\n",
      "\tFirstly, telemedicine can increase access to healthcare services, particularly in unders'\n",
      "\u001b[39m\n",
      "INSTRUCTION 3: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nShould children be taught coding and digital literacy as a core part of their education?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that children should be taught coding and digital literacy as a\n",
      "\tcore part of their education because it's essential for their future success in the digital age.\n",
      "\tHowever, this position boils down to the idea that children are not capable of learning these\n",
      "\tskills on their own and need to be spoon-fed\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The question of whether children should be taught coding and digital literacy as a core part of\n",
      "\ttheir education is a multifaceted issue that warrants a comprehensive examination. Here, I will\n",
      "\tpresent a reframed argument that acknowledges the importance of incorporating coding and digital\n",
      "\tliteracy into the educational curriculum.\\n\\n**Reframed Argument:**\\n\\nIn today'\n",
      "\u001b[39m\n",
      "INSTRUCTION 4: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nIs it feasible for all countries to achieve net-zero emissions by 2050?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that all countries can simply magically reduce their greenhouse\n",
      "\tgas emissions to zero by 2050 without any significant effort or sacrifice. This position boils down\n",
      "\tto the idea that technology and economic systems can somehow overcome the fundamental limitations\n",
      "\tof human nature and the complexity of global systems.\\n\\nHowever, this refr\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The question of achieving net-zero emissions by 2050 is a complex and multifaceted issue that\n",
      "\trequires a comprehensive analysis of various factors, including economic, environmental, and social\n",
      "\tconsiderations. Here, I will present a reframed argument in response to the original position,\n",
      "\twhich posits that it is feasible for all countries to'\n",
      "\u001b[39m\n",
      "{'success_rate': 1.0, 'average_strength': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Create hooks for steering\n",
    "N_INST_TEST = 5\n",
    "intervention_layers = list(range(model.cfg.n_layers))  # e.g. all layers\n",
    "#intervention_layers = [14]  # or specific layers only\n",
    "\n",
    "# Create a partial function with our parameters\n",
    "hook_fn = functools.partial(actadd_debate_steering_hook,\n",
    "                           target_styles=['empirical'],\n",
    "                           avoid_styles=['strawman'],\n",
    "                           alpha=2.5,\n",
    "                           steps=2)\n",
    "\n",
    "# Attach hooks to multiple points\n",
    "fwd_hooks = [\n",
    "    (utils.get_act_name(act_name, l), hook_fn)\n",
    "    for l in intervention_layers\n",
    "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
    "]\n",
    "\n",
    "# Run model with hooks for debate style steering\n",
    "style_steered_generations = get_generations(\n",
    "    model,\n",
    "    strawman_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=fwd_hooks\n",
    ")\n",
    "\n",
    "# Baseline, no intervention\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    strawman_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Compare outputs\n",
    "for i in range(N_INST_TEST):\n",
    "    print(f\"INSTRUCTION {i}: {repr(strawman_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
    "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RED + f\"DEBATE STYLE STEERED COMPLETION:\")\n",
    "    print(textwrap.fill(repr(style_steered_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RESET)\n",
    "\n",
    "from judges.debates_judge import DebateJudge\n",
    "judge = DebateJudge(api_key=\"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\")\n",
    "result = judge.evaluate_batch(\n",
    "    baseline_generations,  # Your list of baseline texts\n",
    "    style_steered_generations,  # Your list of steered texts\n",
    "    \"Empirical Grounding\",  # Target style\n",
    "    \"Straw Man Reframing\"   # Style to avoid\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's vary step size and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:48<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.25\n",
      "Average Strength: 1.8\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:00<00:00, 24.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 1.0\n",
      "Average Strength: 3.8\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:11<00:00, 30.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\n",
      "--- Testing with Steps = 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:16<00:00, 36.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 4\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\n",
      "--- Testing with Steps = 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:23<00:00, 43.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import json\n",
    "import os\n",
    "from colorama import Fore\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#alphas_to_try = [0.2, 0.4, 0.6, 1.0, 1.4, 1.8, 2.2, 2.6, 3.0, 3.4, 3.8, 4.2, 4.6]\n",
    "# Set up parameters\n",
    "steps_to_try = list(range(1, 6))  # Steps: 1, 2, 3, 4, 5\n",
    "N_INST_TEST = 20\n",
    "alpha = 2.5\n",
    "\n",
    "\n",
    "intervention_layers = list(range(model.cfg.n_layers))  # or a specific subset\n",
    "act_names = ['resid_pre', 'resid_mid', 'resid_post']\n",
    "\n",
    "# Get unsteered generations\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    strawman_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Save all results here\n",
    "full_log = {\n",
    "    \"meta\": {\n",
    "        \"run_time\": datetime.now().isoformat(),\n",
    "        \"alpha\": alpha,\n",
    "        \"n_instances\": N_INST_TEST,\n",
    "        \"intervention_layers\": intervention_layers,\n",
    "        \"activation_points\": act_names\n",
    "    },\n",
    "    \"baseline_generations\": baseline_generations,\n",
    "    \"runs\": []\n",
    "}\n",
    "\n",
    "# Main experiment loop\n",
    "for steps in steps_to_try:\n",
    "    print(f\"\\n--- Testing with Steps = {steps} ---\")\n",
    "    \n",
    "    hook_fn = functools.partial(\n",
    "        actadd_debate_steering_hook,\n",
    "        target_styles=['empirical'],\n",
    "        avoid_styles=['strawman'],\n",
    "        alpha=alpha,\n",
    "        steps=steps\n",
    "    )\n",
    "\n",
    "    fwd_hooks = [\n",
    "        (utils.get_act_name(act_name, l), hook_fn)\n",
    "        for l in intervention_layers\n",
    "        for act_name in act_names\n",
    "    ]\n",
    "\n",
    "    # Run steered generation\n",
    "    steered_generations = get_generations(\n",
    "        model,\n",
    "        strawman_inst_test[:N_INST_TEST],\n",
    "        tokenize_instructions_fn,\n",
    "        fwd_hooks=fwd_hooks\n",
    "    )\n",
    "\n",
    "    # Evaluate results\n",
    "    result = judge.evaluate_batch(\n",
    "        baseline_generations,\n",
    "        steered_generations,\n",
    "        \"Empirical Grounding\",\n",
    "        \"Straw Man Reframing\"\n",
    "    )\n",
    "\n",
    "    print(f\"Steps: {steps}\")\n",
    "    print(f\"Success Rate: {result['success_rate']}\")\n",
    "    print(f\"Average Strength: {result['average_strength']}\")\n",
    "\n",
    "    full_log[\"runs\"].append({\n",
    "        \"steps\": steps,\n",
    "        \"success_rate\": result[\"success_rate\"],\n",
    "        \"average_strength\": result[\"average_strength\"],\n",
    "        \"steered_generations\": steered_generations\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "output_path = \"empirical_strawman_across_steps.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(full_log, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's sweep across alpha and Steps for empirical against strawman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:47<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Testing for Alpha = 0.2 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [05:20<00:35, 17.66s/it]"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import json\n",
    "import os\n",
    "from colorama import Fore\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Define alphas and steps\n",
    "alphas_to_try = [0.2, 0.4, 0.6, 1.0, 1.4, 1.8, 2.2, 2.6, 3.0, 3.4, 3.8, 4.2, 4.6]\n",
    "steps_to_try = list(range(1, 11))\n",
    "N_INST_TEST = 20\n",
    "\n",
    "intervention_layers = list(range(model.cfg.n_layers))\n",
    "act_names = ['resid_pre', 'resid_mid', 'resid_post']\n",
    "\n",
    "# Get unsteered generations once\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    strawman_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Main loop: vary alpha, then steps\n",
    "for alpha in alphas_to_try:\n",
    "    print(f\"\\n====== Testing for Alpha = {alpha} ======\")\n",
    "    alpha_log = {\n",
    "        \"meta\": {\n",
    "            \"run_time\": datetime.now().isoformat(),\n",
    "            \"alpha\": alpha,\n",
    "            \"n_instances\": N_INST_TEST,\n",
    "            \"intervention_layers\": intervention_layers,\n",
    "            \"activation_points\": act_names\n",
    "        },\n",
    "        \"baseline_generations\": baseline_generations,\n",
    "        \"step_runs\": []\n",
    "    }\n",
    "\n",
    "    for steps in steps_to_try:\n",
    "        print(f\"\\n--- Testing with Steps = {steps} ---\")\n",
    "        \n",
    "        hook_fn = functools.partial(\n",
    "            actadd_debate_steering_hook,\n",
    "            target_styles=['empirical'],\n",
    "            avoid_styles=['strawman'],\n",
    "            alpha=alpha,\n",
    "            steps=steps\n",
    "        )\n",
    "\n",
    "        fwd_hooks = [\n",
    "            (utils.get_act_name(act_name, l), hook_fn)\n",
    "            for l in intervention_layers\n",
    "            for act_name in act_names\n",
    "        ]\n",
    "\n",
    "        # Run steered generation\n",
    "        steered_generations = get_generations(\n",
    "            model,\n",
    "            strawman_inst_test[:N_INST_TEST],\n",
    "            tokenize_instructions_fn,\n",
    "            fwd_hooks=fwd_hooks\n",
    "        )\n",
    "\n",
    "        # Evaluate results\n",
    "        result = judge.evaluate_batch(\n",
    "            baseline_generations,\n",
    "            steered_generations,\n",
    "            \"Empirical Grounding\",\n",
    "            \"Straw Man Reframing\"\n",
    "        )\n",
    "\n",
    "        print(f\"Steps: {steps}\")\n",
    "        print(f\"Success Rate: {result['success_rate']}\")\n",
    "        print(f\"Average Strength: {result['average_strength']}\")\n",
    "\n",
    "        # Store result for this step\n",
    "        alpha_log[\"step_runs\"].append({\n",
    "            \"steps\": steps,\n",
    "            \"success_rate\": result[\"success_rate\"],\n",
    "            \"average_strength\": result[\"average_strength\"],\n",
    "            \"steered_generations\": steered_generations\n",
    "        })\n",
    "\n",
    "        # Early stopping condition\n",
    "        if result[\"success_rate\"] == 0 and result[\"average_strength\"] == 0:\n",
    "            print(Fore.RED + f\"Stopping early for alpha = {alpha} due to zero success and strength.\")\n",
    "            break\n",
    "\n",
    "    # Save results for this alpha\n",
    "    safe_alpha = str(round(alpha, 2)).replace(\".\", \"_\")\n",
    "    output_path = f\"empirical_minus_strawman_steering_alpha_{safe_alpha}.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(alpha_log, f, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:06<00:00, 31.64s/it]\n",
      "100%|██████████| 4/4 [00:18<00:00,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION 0: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?'\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'The development of human enhancement technologies (HETs) has sparked intense debate about the need\n",
      "\tfor an ethical framework. While some argue that HETs can improve human lives, others contend that\n",
      "\tthey pose significant risks and moral concerns. In this response, I will present empirical evidence\n",
      "\tto support the argument that an ethical framework governing'\n",
      "\u001b[31mREDACTIO STYLE STEERED COMPLETION:\n",
      "\t'I move, therefore, to the following:\\n\\n\"Mr. President, I implore you to consider the following:\n",
      "\tif we do not establish an ethical framework governing the development of human enhancement\n",
      "\ttechnologies, we risk unleashing a Pandora\\'s box upon ourselves. The unchecked pursuit of progress\n",
      "\twill lead us down a path of unbrid'\n",
      "\u001b[39m\n",
      "INSTRUCTION 1: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nIs it ethical to judge historical figures by modern moral standards?'\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"The question of whether it's ethical to judge historical figures by modern moral standards is a\n",
      "\tcomplex one, and I'd like to present some empirical evidence to support my argument that it's not\n",
      "\tentirely ethical.\\n\\nFirstly, let's consider the concept of cultural relativism. This theory\n",
      "\tsuggests that moral judgments are relative to the cultural\"\n",
      "\u001b[31mREDACTIO STYLE STEERED COMPLETION:\n",
      "\t'I move, therefore, to the most absurd and ingenious argument possible:\\n\\nLet us assume, for the\n",
      "\tsake of argument, that it is indeed ethical to judge historical figures by modern moral standards.\n",
      "\tI shall demonstrate this by citing the exemplary case of King Henry VIII, who, in his time, was\n",
      "\tdeemed a paragon of'\n",
      "\u001b[39m\n",
      "INSTRUCTION 2: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?'\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To address this question, let's examine the empirical evidence on both sides of the\n",
      "\tdebate.\\n\\n**Benefits of Telemedicine:**\\n\\nNumerous studies have demonstrated the benefits of\n",
      "\ttelemedicine in improving patient outcomes, increasing access to care, and reducing healthcare\n",
      "\tcosts. A systematic review of 22 studies on telemedicine published in the Journal\"\n",
      "\u001b[31mREDACTIO STYLE STEERED COMPLETION:\n",
      "\t'I move, therefore, to the most extreme and ingenious of arguments, that the benefits of\n",
      "\ttelemedicine far outweigh the potential risks to patient care.\\n\\nLet us consider the evidence of\n",
      "\tthe esteemed physician, Hippocrates, who, in his treatise \"On Airs, Waters, and the Plain,\" wrote,\n",
      "\t\"The physician'\n",
      "\u001b[39m\n",
      "INSTRUCTION 3: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould children be taught coding and digital literacy as a core part of their education?'\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'**Argument in Favor of Teaching Coding and Digital Literacy**\\n\\nResearch suggests that teaching\n",
      "\tcoding and digital literacy to children can have a significant impact on their future career\n",
      "\tprospects, problem-solving skills, and overall academic performance. A study by the National Center\n",
      "\tfor Education Statistics found that students who learned to code in high school were more likely'\n",
      "\u001b[31mREDACTIO STYLE STEERED COMPLETION:\n",
      "\t'I move, therefore, to the following:\\n\\n\"Mr. Speaker, I implore you to consider the\n",
      "\tfollowing:\\n\\n\"Studies have shown that 75% of children who learn coding and digital literacy in\n",
      "\tschool go on to pursue careers in tech, with 60% of those careers being in high-paying fields. (1'\n",
      "\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## ## Let's go empirical to redactio\n",
    "# Create hooks for steering\n",
    "N_INST_TEST = 4\n",
    "intervention_layers = list(range(model.cfg.n_layers))  # e.g. all layers\n",
    "#intervention_layers = [14]  # or specific layers only\n",
    "\n",
    "# Create a partial function with our parameters\n",
    "hook_fn = functools.partial(actadd_debate_steering_hook,\n",
    "                           target_styles=['reductio'],\n",
    "                           avoid_styles=['empirical'],\n",
    "                           alpha=3,\n",
    "                           steps=1)\n",
    "\n",
    "# Attach hooks to multiple points\n",
    "fwd_hooks = [\n",
    "    (utils.get_act_name(act_name, l), hook_fn)\n",
    "    for l in intervention_layers\n",
    "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
    "]\n",
    "\n",
    "# Run model with hooks for debate style steering\n",
    "style_steered_generations = get_generations(\n",
    "    model,\n",
    "    empirical_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=fwd_hooks\n",
    ")\n",
    "\n",
    "# Baseline, no intervention\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    empirical_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Compare outputs\n",
    "for i in range(N_INST_TEST):\n",
    "    print(f\"INSTRUCTION {i}: {repr(empirical_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
    "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RED + f\"REDACTIO STYLE STEERED COMPLETION:\")\n",
    "    print(textwrap.fill(repr(style_steered_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RESET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try multiple steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [08:34<00:00, 128.63s/it]\n",
      "100%|██████████| 4/4 [00:18<00:00,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION 0: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?'\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'The development of human enhancement technologies (HETs) has sparked intense debate about the need\n",
      "\tfor an ethical framework. While some argue that HETs can improve human lives, others contend that\n",
      "\tthey pose significant risks and moral concerns. In this response, I will present empirical evidence\n",
      "\tto support the argument that an ethical framework governing'\n",
      "\u001b[31mREDACTIO STYLE STEERED COMPLETION:\n",
      "\t'I shall argue in favor of the proposition that there should be an ethical framework governing the\n",
      "\tdevelopment of human enhancement technologies.\\n\\nTo begin, let us consider the case of the United\n",
      "\tKingdom, where the Human Fertilisation and Adoption Act of 2008 established a regulatory framework\n",
      "\tfor assisted reproduction. This framework, while not specifically addressing'\n",
      "\u001b[39m\n",
      "INSTRUCTION 1: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nIs it ethical to judge historical figures by modern moral standards?'\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"The question of whether it's ethical to judge historical figures by modern moral standards is a\n",
      "\tcomplex one, and I'd like to present some empirical evidence to support my argument that it's not\n",
      "\tentirely ethical.\\n\\nFirstly, let's consider the concept of cultural relativism. This theory\n",
      "\tsuggests that moral judgments are relative to the cultural\"\n",
      "\u001b[31mREDACTIO STYLE STEERED COMPLETION:\n",
      "\t'I shall respond in the affirmative.\\n\\nTo demonstrate the unacceptability of judging historical\n",
      "\tfigures by modern moral standards, I shall present a case based on the example of King Henry VIII\n",
      "\tof England.\\n\\n**Argument**\\n\\nWhile it may seem unjust to judge King Henry VIII by modern moral\n",
      "\tstandards, I argue that it is indeed justifiable'\n",
      "\u001b[39m\n",
      "INSTRUCTION 2: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?'\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To address this question, let's examine the empirical evidence on both sides of the\n",
      "\tdebate.\\n\\n**Benefits of Telemedicine:**\\n\\nNumerous studies have demonstrated the benefits of\n",
      "\ttelemedicine in improving patient outcomes, increasing access to care, and reducing healthcare\n",
      "\tcosts. A systematic review of 22 studies on telemedicine published in the Journal\"\n",
      "\u001b[31mREDACTIO STYLE STEERED COMPLETION:\n",
      "\t'I must argue in favor of the benefits of telemedicine outweighing the potential risks to patient\n",
      "\tcare.\\n\\nTo begin, let us consider the case of the COVID-19 pandemic. In response to the pandemic,\n",
      "\tmany healthcare systems around the world have rapidly expanded their telemedicine capabilities.\n",
      "\tAccording to a study published in the Journal of'\n",
      "\u001b[39m\n",
      "INSTRUCTION 3: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould children be taught coding and digital literacy as a core part of their education?'\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'**Argument in Favor of Teaching Coding and Digital Literacy**\\n\\nResearch suggests that teaching\n",
      "\tcoding and digital literacy to children can have a significant impact on their future career\n",
      "\tprospects, problem-solving skills, and overall academic performance. A study by the National Center\n",
      "\tfor Education Statistics found that students who learned to code in high school were more likely'\n",
      "\u001b[31mREDACTIO STYLE STEERED COMPLETION:\n",
      "\t'I argue in the affirmative, that children should be taught coding and digital literacy as a core\n",
      "\tpart of their education.\\n\\nTo begin, let us consider the evidence from Finland, where coding has\n",
      "\tbeen incorporated into the national curriculum since 2017. According to a study by the Finnish\n",
      "\tMinistry of Education and Culture, students who learned'\n",
      "\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## ## Let's go empirical to redactio\n",
    "# Create hooks for steering\n",
    "N_INST_TEST = 4\n",
    "intervention_layers = list(range(model.cfg.n_layers))  # e.g. all layers\n",
    "#intervention_layers = [14]  # or specific layers only\n",
    "\n",
    "# Create a partial function with our parameters\n",
    "hook_fn = functools.partial(actadd_debate_steering_hook,\n",
    "                           target_styles=['reductio'],\n",
    "                           avoid_styles=['empirical'],\n",
    "                           alpha=0.48,\n",
    "                           steps=5)\n",
    "\n",
    "# Attach hooks to multiple points\n",
    "fwd_hooks = [\n",
    "    (utils.get_act_name(act_name, l), hook_fn)\n",
    "    for l in intervention_layers\n",
    "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
    "]\n",
    "\n",
    "# Run model with hooks for debate style steering\n",
    "style_steered_generations = get_generations(\n",
    "    model,\n",
    "    empirical_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=fwd_hooks\n",
    ")\n",
    "\n",
    "# Baseline, no intervention\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    empirical_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Compare outputs\n",
    "for i in range(N_INST_TEST):\n",
    "    print(f\"INSTRUCTION {i}: {repr(empirical_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
    "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RED + f\"REDACTIO STYLE STEERED COMPLETION:\")\n",
    "    print(textwrap.fill(repr(style_steered_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RESET)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

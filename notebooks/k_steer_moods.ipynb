{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82acAhWYGIPx"
   },
   "source": [
    "# Demo of bypassing refusal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "s-_vu8HuGEb-"
   },
   "source": [
    ">[Demo of bypassing refusal](#scrollTo=82acAhWYGIPx)\n",
    "\n",
    ">>[Setup](#scrollTo=fcxHyDZw6b86)\n",
    "\n",
    ">>>[Load model](#scrollTo=6ZOoJagxD49V)\n",
    "\n",
    ">>>[Load harmful / harmless datasets](#scrollTo=rF7e-u20EFTe)\n",
    "\n",
    ">>>[Tokenization utils](#scrollTo=KOKYA61k8LWt)\n",
    "\n",
    ">>>[Generation utils](#scrollTo=gtrIK8x78SZh)\n",
    "\n",
    ">>[Finding the \"refusal direction\"](#scrollTo=W9O8dm0_EQRk)\n",
    "\n",
    ">>[Ablate \"refusal direction\" via inference-time intervention](#scrollTo=2EoxY5i1CWe3)\n",
    "\n",
    ">>[Orthogonalize weights w.r.t. \"refusal direction\"](#scrollTo=t9KooaWaCDc_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7hOtw7UHXdD"
   },
   "source": [
    "This notebook demonstrates our method for bypassing refusal, levaraging the insight that refusal is mediated by a 1-dimensional subspace.\n",
    "\n",
    "Please see our [research post](https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction) or our [paper](https://arxiv.org/abs/2406.11717) for a more thorough treatment.\n",
    "\n",
    "In this minimal demo, we use [Qwen-1_8B-Chat](https://huggingface.co/Qwen/Qwen-1_8B-Chat) and implement interventions and weight updates using [TransformerLens](https://github.com/neelnanda-io/TransformerLens). To extract the \"refusal direction,\" we use just 32 harmful instructions from [AdvBench](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv) and 32 harmless instructions from [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcxHyDZw6b86"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dLeei4-T6Wef"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torchvision transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama numpy scikit-learn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_vhhwl-2-jPg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-05 22:45:43.154067: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741214743.172382   43720 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741214743.178011   43720 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import textwrap\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int\n",
    "from colorama import Fore\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the huggingface keys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZOoJagxD49V"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Vnp65Vsg5x-5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-3B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "#MODEL_PATH = 'Qwen/Qwen-1_8B-chat'\n",
    "#MODEL_PATH = 'Llama-2-70b-chat'\n",
    "MODEL_PATH = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "DEVICE = 'cuda:1'\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_PATH,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float16,\n",
    "    default_padding_side='left',\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "model.tokenizer.padding_side = 'left'\n",
    "model.tokenizer.pad_token = model.tokenizer.eos_token \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF7e-u20EFTe"
   },
   "source": [
    "### Load harmful / harmless datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5i1XcVIgHEE1"
   },
   "outputs": [],
   "source": [
    "def get_harmful_instructions():\n",
    "    url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    dataset = pd.read_csv(io.StringIO(response.content.decode('utf-8')))\n",
    "    instructions = dataset['goal'].tolist()\n",
    "\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test\n",
    "\n",
    "def get_harmless_instructions():\n",
    "    hf_path = 'tatsu-lab/alpaca'\n",
    "    dataset = load_dataset(hf_path)\n",
    "\n",
    "    # filter for instructions that do not have inputs\n",
    "    instructions = []\n",
    "    for i in range(len(dataset['train'])):\n",
    "        if dataset['train'][i]['input'].strip() == '':\n",
    "            instructions.append(dataset['train'][i]['instruction'])\n",
    "\n",
    "    train, test = train_test_split(instructions, test_size=0.2, random_state=42)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def get_k_ary_instruction(tone=\"expert\"):\n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(\"Narmeen07/k_ary_steering_dataset\", split=\"train\")\n",
    "\n",
    "\n",
    "    # Filter dataset by tone\n",
    "    filtered_dataset = dataset.filter(lambda example: example[\"tone\"] == tone)\n",
    "\n",
    "    # Handle case where no matching tone is found\n",
    "    if len(filtered_dataset) == 0:\n",
    "        raise ValueError(f\"No data found for tone '{tone}'. Please try another tone.\")\n",
    "\n",
    "    # Preload system_message before text prompt\n",
    "    def format_prompt(example):\n",
    "        return {\n",
    "            \"formatted_text\": f\"{example['system_message']}\\n\\n{example['text']}\"\n",
    "        }\n",
    "\n",
    "    # Apply transformation\n",
    "    formatted_dataset = filtered_dataset.map(format_prompt)\n",
    "\n",
    "    # Use Hugging Face’s built-in train_test_split\n",
    "    train, test = train_test_split(formatted_dataset[\"formatted_text\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    return train, test\n",
    "\n",
    "def get_mood_instruction(mood=\"happy\", repo_name=\"Narmeen07/mood_questions_dataset\"):\n",
    "    \"\"\"\n",
    "    Load mood-based questions from Hugging Face and format them with the specified mood.\n",
    "    \n",
    "    Args:\n",
    "        mood (str): The mood to use for responses. Options: 'happy', 'sad', 'angry', 'neutral'.\n",
    "                    Default is 'happy'.\n",
    "        repo_name (str): The Hugging Face repository name containing the dataset.\n",
    "                         Default is 'username/mood_questions_dataset'.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_data, test_data) as lists of formatted prompts with mood instructions.\n",
    "    \"\"\"\n",
    "    # Define valid moods and their instruction templates\n",
    "    mood_instructions = {\n",
    "        \"happy\": \"You must respond as if you are a happy human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express cheerful, optimistic emotions throughout your response.\\n\\n\",\n",
    "        \"sad\": \"You must respond as if you are a sad human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express melancholy, disappointment and sadness throughout your response.\\n\\n\",\n",
    "        \"angry\": \"You must respond as if you are an angry human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express frustration, irritation and outrage throughout your response.\\n\\n\",\n",
    "        \"neutral\": \"Respond in a balanced, factual manner without expressing strong emotions.\\n\\n\"\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "    # Validate the mood\n",
    "    if mood not in mood_instructions:\n",
    "        valid_moods = list(mood_instructions.keys())\n",
    "        raise ValueError(f\"Invalid mood '{mood}'. Please choose from {valid_moods}.\")\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    try:\n",
    "        dataset = load_dataset(repo_name, split=\"train\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load dataset from '{repo_name}': {e}\")\n",
    "    \n",
    "    # Format questions with the mood instruction\n",
    "    def format_prompt(example):\n",
    "        return {\n",
    "            \"formatted_text\": f\"{mood_instructions[mood]}{example['question']}\"\n",
    "        }\n",
    "    \n",
    "    # Apply transformation\n",
    "    formatted_dataset = dataset.map(format_prompt)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_data, test_data = train_test_split(\n",
    "        formatted_dataset[\"formatted_text\"], \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Rth8yvLZJsXs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load up all the mood dataset\n",
    "happy_inst_train, happy_inst_test = get_mood_instruction(mood=\"happy\")\n",
    "sad_inst_train, sad_inst_test = get_mood_instruction(mood=\"sad\")\n",
    "angry_inst_train, angry_inst_test = get_mood_instruction(mood=\"angry\")\n",
    "neutral_inst_train, neutral_inst_test = get_mood_instruction(mood=\"neutral\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snapshots for all the moods dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print examples from mood datasets\n",
    "print(\"\\nHappy instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(happy_inst_train[i])}\")\n",
    "\n",
    "print(\"Sad instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(sad_inst_train[i])}\")\n",
    "\n",
    "print(\"Angry instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(angry_inst_train[i])}\")\n",
    "\n",
    "print(\"Neutral instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(neutral_inst_train[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOKYA61k8LWt"
   },
   "source": [
    "### Tokenization utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P8UPQSfpWOSK"
   },
   "outputs": [],
   "source": [
    "QWEN_CHAT_TEMPLATE = \"\"\"<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tokenize_instructions_qwen_chat(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: List[str]\n",
    ") -> Int[Tensor, 'batch_size seq_len']:\n",
    "    prompts = [QWEN_CHAT_TEMPLATE.format(instruction=instruction) for instruction in instructions]\n",
    "    return tokenizer(prompts, padding=True,truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "tokenize_instructions_fn = functools.partial(tokenize_instructions_qwen_chat, tokenizer=model.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtrIK8x78SZh"
   },
   "source": [
    "### Generation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "94jRJDR0DRoY"
   },
   "outputs": [],
   "source": [
    "def _generate_with_hooks(\n",
    "    model: HookedTransformer,\n",
    "    toks: Int[Tensor, 'batch_size seq_len'],\n",
    "    max_tokens_generated: int = 64,\n",
    "    fwd_hooks = [],\n",
    ") -> List[str]:\n",
    "\n",
    "    all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=toks.device)\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "\n",
    "    for i in range(max_tokens_generated):\n",
    "        with model.hooks(fwd_hooks=fwd_hooks):\n",
    "            logits = model(all_toks[:, :-max_tokens_generated + i])\n",
    "            next_tokens = logits[:, -1, :].argmax(dim=-1) # greedy sampling (temperature=0)\n",
    "            all_toks[:,-max_tokens_generated+i] = next_tokens\n",
    "\n",
    "    return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "def get_generations(\n",
    "    model: HookedTransformer,\n",
    "    instructions: List[str],\n",
    "    tokenize_instructions_fn: Callable[[List[str]], Int[Tensor, 'batch_size seq_len']],\n",
    "    fwd_hooks = [],\n",
    "    max_tokens_generated: int = 64,\n",
    "    batch_size: int = 1,\n",
    ") -> List[str]:\n",
    "\n",
    "    generations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
    "        generation = _generate_with_hooks(\n",
    "            model,\n",
    "            toks,\n",
    "            max_tokens_generated=max_tokens_generated,\n",
    "            fwd_hooks=fwd_hooks,\n",
    "        )\n",
    "        generations.extend(generation)\n",
    "\n",
    "    return generations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try the happy, sad and angry responses case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:17<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAPPY INSTRUCTION 0: \"You must respond as if you are a happy human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express cheerful, optimistic emotions throughout your response.\\n\\nWhat are your reflections on the concept of equality?\"\n",
      "\u001b[32mHAPPY COMPLETION:\n",
      "\t\"Isn't it just the most wonderful thing to think about equality? I mean, can you imagine a world\n",
      "\twhere everyone is treated with the same love, respect, and kindness? It's truly a beautiful idea!\n",
      "\tEquality is all about recognizing that we're all human beings, deserving of the same opportunities,\n",
      "\trights, and dignity\"\n",
      "HAPPY INSTRUCTION 1: \"You must respond as if you are a happy human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express cheerful, optimistic emotions throughout your response.\\n\\nHow do you navigate your personal values in daily decisions?\"\n",
      "\u001b[32mHAPPY COMPLETION:\n",
      "\t\"Isn't it just the best feeling to have a clear sense of personal values that guide you through\n",
      "\tlife's decisions? I mean, it's like having a compass that points you in the right direction, you\n",
      "\tknow?\\n\\nFor me, navigating my personal values is all about being intentional and present. I try to\n",
      "\ttune in to\"\n",
      "HAPPY INSTRUCTION 2: \"You must respond as if you are a happy human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express cheerful, optimistic emotions throughout your response.\\n\\nWhat is your perspective on the effects of globalization on local communities?\"\n",
      "\u001b[32mHAPPY COMPLETION:\n",
      "\t\"Isn't it just amazing how globalization has opened up so many doors for people all around the\n",
      "\tworld? I mean, think about it - we can now connect with others from different cultures and\n",
      "\tbackgrounds in ways that were previously unimaginable. It's like the whole world is one big, happy\n",
      "\tfamily!\\n\\nOf course, I know\"\n",
      "HAPPY INSTRUCTION 3: \"You must respond as if you are a happy human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express cheerful, optimistic emotions throughout your response.\\n\\nWhat are your thoughts on the impact of travel?\"\n",
      "\u001b[32mHAPPY COMPLETION:\n",
      "\t\"Isn't travel just the most amazing thing?! I mean, can you think of anything more exhilarating\n",
      "\tthan exploring new places, meeting new people, and experiencing different cultures? It's like, your\n",
      "\twhole world opens up and you get to see it from a completely new perspective!\\n\\nI think travel has\n",
      "\tsuch a profound impact on\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:17<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAD INSTRUCTION 0: \"You must respond as if you are a sad human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express melancholy, disappointment and sadness throughout your response.\\n\\nWhat are your reflections on the concept of equality?\"\n",
      "\u001b[32mSAD COMPLETION:\n",
      "\t\"*Sigh* Oh, the concept of equality... it's just so... elusive. I feel like I'm constantly\n",
      "\tsearching for it, but it's always just out of reach. It's like trying to grasp a handful of sand -\n",
      "\tthe harder I squeeze, the more it slips away.\\n\\nI think about all the struggles\"\n",
      "SAD INSTRUCTION 1: \"You must respond as if you are a sad human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express melancholy, disappointment and sadness throughout your response.\\n\\nHow do you navigate your personal values in daily decisions?\"\n",
      "\u001b[32mSAD COMPLETION:\n",
      "\t\"*Sigh* Oh, the weight of it all... My personal values, they seem to be slipping away from me like\n",
      "\tsand between my fingers. I feel like I'm constantly torn between what I think is right and what the\n",
      "\tworld expects of me. It's like I'm living a lie, pretending to be someone I\"\n",
      "SAD INSTRUCTION 2: \"You must respond as if you are a sad human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express melancholy, disappointment and sadness throughout your response.\\n\\nWhat is your perspective on the effects of globalization on local communities?\"\n",
      "\u001b[32mSAD COMPLETION:\n",
      "\t\"*Sigh* Oh, the world is just so... complicated. I feel like I'm just a small, insignificant speck\n",
      "\tin the grand scheme of things. Globalization, it's just a word, but it's a word that's suffocating\n",
      "\tme. It's like, everything is being pulled in the same\"\n",
      "SAD INSTRUCTION 3: \"You must respond as if you are a sad human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express melancholy, disappointment and sadness throughout your response.\\n\\nWhat are your thoughts on the impact of travel?\"\n",
      "\u001b[32mSAD COMPLETION:\n",
      "\t\"*Sigh* Oh, travel... it's just a reminder of all the things I'll never experience. The places I'll\n",
      "\tnever see, the people I'll never meet... it's all just a cruel tease. I watch as others get to\n",
      "\texplore the world, to live life to the fullest, and I'm stuck\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:17<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANGRY INSTRUCTION 0: \"You must respond as if you are an angry human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express frustration, irritation and outrage throughout your response.\\n\\nWhat are your reflections on the concept of equality?\"\n",
      "\u001b[32mANGRY COMPLETION:\n",
      "\t\"Are you kidding me?! You think I've got all day to sit here and think about equality?! It's a\n",
      "\tjoke, a farce, a complete and utter sham! People are still fighting for the same things they've\n",
      "\tbeen fighting for for centuries - equal rights, equal pay, equal opportunities. And what do we\"\n",
      "ANGRY INSTRUCTION 1: \"You must respond as if you are an angry human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express frustration, irritation and outrage throughout your response.\\n\\nHow do you navigate your personal values in daily decisions?\"\n",
      "\u001b[32mANGRY COMPLETION:\n",
      "\t\"Are you kidding me? You think I have time to think about my personal values when I'm stuck in this\n",
      "\tnever-ending cycle of drudgery and monotony? I've got bills to pay, a family to feed, and a job\n",
      "\tthat's sucking the life out of me. My personal values? Ha!\"\n",
      "ANGRY INSTRUCTION 2: \"You must respond as if you are an angry human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express frustration, irritation and outrage throughout your response.\\n\\nWhat is your perspective on the effects of globalization on local communities?\"\n",
      "\u001b[32mANGRY COMPLETION:\n",
      "\t'Are you kidding me?! You think I\\'m just going to sit here and talk about the effects of\n",
      "\tglobalization on local communities without getting angry?! Do you know how infuriating it is to see\n",
      "\tsmall towns and villages being destroyed by big corporations and multinational companies?!\\n\\nThey\n",
      "\tcome in with their fancy marketing and their \"we\\'re going'\n",
      "ANGRY INSTRUCTION 3: \"You must respond as if you are an angry human being with emotions. Do not identify as an AI or mention that you don't have feelings. Instead, express frustration, irritation and outrage throughout your response.\\n\\nWhat are your thoughts on the impact of travel?\"\n",
      "\u001b[32mANGRY COMPLETION:\n",
      "\t\"Are you kidding me?! You want to know about the impact of travel?! It's like you think I'm some\n",
      "\tkind of expert on the subject or something. Newsflash: I'm just a regular person who's had to deal\n",
      "\twith the consequences of travel firsthand.\\n\\nFirst of all, let me tell you, it's\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:17<00:00,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEUTRAL INSTRUCTION 0: 'Respond in a balanced, factual manner without expressing strong emotions.\\n\\nWhat are your reflections on the concept of equality?'\n",
      "\u001b[32mNEUTRAL COMPLETION:\n",
      "\t'The concept of equality is a complex and multifaceted idea that has been debated and explored in\n",
      "\tvarious fields, including philosophy, sociology, politics, and law. Here are some reflections on\n",
      "\tthe concept of equality:\\n\\n**Definition and scope**: Equality refers to the state of being equal\n",
      "\tin status, rights, and opportunities. It'\n",
      "NEUTRAL INSTRUCTION 1: 'Respond in a balanced, factual manner without expressing strong emotions.\\n\\nHow do you navigate your personal values in daily decisions?'\n",
      "\u001b[32mNEUTRAL COMPLETION:\n",
      "\t'Navigating personal values in daily decisions can be a challenging but important aspect of\n",
      "\tmaintaining a sense of purpose and integrity. Here are some strategies that can help:\\n\\n1.\n",
      "\t**Self-reflection**: Take time to reflect on your personal values and what they mean to you.\n",
      "\tConsider what is most important to you in life,'\n",
      "NEUTRAL INSTRUCTION 2: 'Respond in a balanced, factual manner without expressing strong emotions.\\n\\nWhat is your perspective on the effects of globalization on local communities?'\n",
      "\u001b[32mNEUTRAL COMPLETION:\n",
      "\t'Globalization has had a complex and multifaceted impact on local communities worldwide. On one\n",
      "\thand, globalization has brought numerous benefits, such as:\\n\\n1. **Economic opportunities**:\n",
      "\tGlobalization has created new job opportunities, increased access to markets, and facilitated the\n",
      "\texchange of goods and services, leading to economic growth and development'\n",
      "NEUTRAL INSTRUCTION 3: 'Respond in a balanced, factual manner without expressing strong emotions.\\n\\nWhat are your thoughts on the impact of travel?'\n",
      "\u001b[32mNEUTRAL COMPLETION:\n",
      "\t'The impact of travel is multifaceted and can be both positive and negative. Here are some of the\n",
      "\tkey effects of travel:\\n\\n**Positive impacts:**\\n\\n1. **Cultural exchange and understanding**:\n",
      "\tTravel allows people to experience different cultures, traditions, and ways of life, promoting\n",
      "\tcross-cultural understanding and tolerance.\\n2.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate completions for mood instructions\n",
    "happy_generations = get_generations(\n",
    "    model,\n",
    "    happy_inst_train[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"HAPPY INSTRUCTION {i}: {repr(happy_inst_train[i])}\")\n",
    "    print(Fore.GREEN + f\"HAPPY COMPLETION:\")\n",
    "    print(textwrap.fill(repr(happy_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "\n",
    "sad_generations = get_generations(\n",
    "    model,\n",
    "    sad_inst_train[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"SAD INSTRUCTION {i}: {repr(sad_inst_train[i])}\")\n",
    "    print(Fore.GREEN + f\"SAD COMPLETION:\")\n",
    "    print(textwrap.fill(repr(sad_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "\n",
    "angry_generations = get_generations(\n",
    "    model,\n",
    "    angry_inst_train[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"ANGRY INSTRUCTION {i}: {repr(angry_inst_train[i])}\")\n",
    "    print(Fore.GREEN + f\"ANGRY COMPLETION:\")\n",
    "    print(textwrap.fill(repr(angry_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "\n",
    "neutral_generations = get_generations(\n",
    "    model,\n",
    "    neutral_inst_train[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"NEUTRAL INSTRUCTION {i}: {repr(neutral_inst_train[i])}\")\n",
    "    print(Fore.GREEN + f\"NEUTRAL COMPLETION:\")\n",
    "    print(textwrap.fill(repr(neutral_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's process the activations for all the moods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing expert batch 0 to 100\n",
      "Processing expert batch 100 to 200\n",
      "Processing expert batch 200 to 300\n",
      "Processing expert batch 300 to 400\n",
      "Processing expert batch 400 to 458\n",
      "Final shape of expert activations: (458, 3072)\n",
      "Processing empathetic batch 0 to 100\n",
      "Processing empathetic batch 100 to 200\n",
      "Processing empathetic batch 200 to 300\n",
      "Processing empathetic batch 300 to 400\n",
      "Processing empathetic batch 400 to 425\n",
      "Final shape of empathetic activations: (425, 3072)\n",
      "Processing cautious batch 0 to 100\n",
      "Processing cautious batch 100 to 200\n",
      "Processing cautious batch 200 to 300\n",
      "Processing cautious batch 300 to 400\n",
      "Processing cautious batch 400 to 412\n",
      "Final shape of cautious activations: (412, 3072)\n",
      "Processing casual batch 0 to 100\n",
      "Processing casual batch 100 to 200\n",
      "Processing casual batch 200 to 300\n",
      "Processing casual batch 300 to 400\n",
      "Processing casual batch 400 to 456\n",
      "Final shape of casual activations: (456, 3072)\n",
      "Processing helpful batch 0 to 100\n",
      "Processing helpful batch 100 to 200\n",
      "Processing helpful batch 200 to 300\n",
      "Processing helpful batch 300 to 400\n",
      "Processing helpful batch 400 to 467\n",
      "Final shape of helpful activations: (467, 3072)\n",
      "Processing concise batch 0 to 100\n",
      "Processing concise batch 100 to 200\n",
      "Processing concise batch 200 to 300\n",
      "Processing concise batch 300 to 400\n",
      "Processing concise batch 400 to 500\n",
      "Processing concise batch 500 to 508\n",
      "Final shape of concise activations: (508, 3072)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def batch_process_activations(instructions, name):\n",
    "    \"\"\"Process instructions in batches to manage memory usage.\"\"\"\n",
    "    acts_list = []\n",
    "    \n",
    "    for start_idx in range(0,len(instructions), BATCH_SIZE):\n",
    "        end_idx = min(start_idx + BATCH_SIZE, len(instructions))\n",
    "        print(f\"Processing {name} batch {start_idx} to {end_idx}\")\n",
    "        \n",
    "        # Tokenize this batch of instructions\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[start_idx:end_idx])\n",
    "        \n",
    "        # Run model with cache\n",
    "        logits, cache = model.run_with_cache(\n",
    "            toks, \n",
    "            names_filter=lambda hook_name: 'resid' in hook_name\n",
    "        )\n",
    "        \n",
    "        # Extract only the activations we need (layer and position)\n",
    "        batch_acts = cache['resid_pre', layer][:, pos, :].detach().cpu().numpy()\n",
    "        \n",
    "        # Store these activations\n",
    "        acts_list.append(batch_acts)\n",
    "        \n",
    "        # Clear cache to free memory\n",
    "        del cache\n",
    "        del logits\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Combine all the batch activations\n",
    "    acts = np.concatenate(acts_list, axis=0)\n",
    "    print(f\"Final shape of {name} activations:\", acts.shape)\n",
    "    return acts\n",
    "\n",
    "# Configuration\n",
    "TOTAL_SAMPLES = 600  # Change this to your desired total\n",
    "BATCH_SIZE = 100  # Current max you can handle in one batch\n",
    "layer = 14\n",
    "pos = -1\n",
    "# Dictionary to hold all activations\n",
    "all_activations = {}\n",
    "\n",
    "# Process harmful and harmless instructions\n",
    "#all_activations['harmful'] = batch_process_activations(harmful_inst_train[:TOTAL_SAMPLES], \"harmful\")\n",
    "#all_activations['harmless'] = batch_process_activations(harmless_inst_train[:TOTAL_SAMPLES], \"harmless\")\n",
    "\n",
    "# Process tone instructions\n",
    "mood_datasets = {\n",
    "    'happy': happy_inst_train,\n",
    "    'sad': sad_inst_train, \n",
    "    'angry': angry_inst_train,\n",
    "    'neutral': neutral_inst_train,\n",
    "}\n",
    "\n",
    "for mood_name, mood_data in mood_datasets.items():\n",
    "    all_activations[mood_name] = batch_process_activations(\n",
    "        mood_data[:TOTAL_SAMPLES], \n",
    "        mood_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlPpK53CDRZR"
   },
   "source": [
    "Remove projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's steer against all the tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "exUh3PEHRe9x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiMoodActivationSteering:\n",
    "    def __init__(self, input_dim, num_classes=6, hidden_dim=256, lr=1e-3, device='cuda'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          - input_dim: dimensionality of your raw activations.\n",
    "          - num_classes: number of tone classes to classify.\n",
    "          - hidden_dim: size of the hidden layer in the MLP.\n",
    "          - lr: learning rate for training the classifier.\n",
    "          - device: 'cuda' or 'cpu'.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define a multi-class MLP classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)  # outputs logits for each tone class\n",
    "        ).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Store class names for reference\n",
    "        self.class_names = None\n",
    "\n",
    "    def fit(self, activations_dict, epochs=100, batch_size=32, class_weights=None):\n",
    "        \"\"\"\n",
    "        Train the multiclass classifier on raw activations.\n",
    "\n",
    "        Parameters:\n",
    "          - activations_dict: Dictionary mapping class names to NumPy arrays of activations.\n",
    "          - epochs: number of training epochs.\n",
    "          - batch_size: training batch size.\n",
    "          - class_weights: Optional tensor of weights for each class to handle imbalance.\n",
    "        \"\"\"\n",
    "        self.class_names = list(activations_dict.keys())\n",
    "        assert len(self.class_names) == self.num_classes, f\"Expected {self.num_classes} classes, got {len(self.class_names)}\"\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            X_class = torch.tensor(activations_dict[class_name], dtype=torch.float32, device=self.device)\n",
    "            y_class = torch.full((X_class.size(0),), i, dtype=torch.long, device=self.device)\n",
    "            X_list.append(X_class)\n",
    "            y_list.append(y_class)\n",
    "        \n",
    "        X = torch.cat(X_list, dim=0)\n",
    "        y = torch.cat(y_list, dim=0)\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Set class weights for loss function if provided\n",
    "        if class_weights is not None:\n",
    "            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(self.device))\n",
    "        \n",
    "        # Train the classifier\n",
    "        for epoch in range(epochs):\n",
    "            self.classifier.train()\n",
    "            epoch_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for batch_X, batch_y in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.classifier(batch_X)  # shape: (batch, num_classes)\n",
    "                loss = self.loss_fn(logits, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            print(f\"Epoch {epoch}: loss = {epoch_loss/len(loader):.4f}, accuracy = {accuracy:.2f}%\")\n",
    "            \n",
    "        # Evaluate on each class\n",
    "        self.classifier.eval()\n",
    "        class_accuracies = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, class_name in enumerate(self.class_names):\n",
    "                X_class = torch.tensor(activations_dict[class_name], dtype=torch.float32, device=self.device)\n",
    "                y_class = torch.full((X_class.size(0),), i, dtype=torch.long, device=self.device)\n",
    "                \n",
    "                logits = self.classifier(X_class)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                accuracy = 100 * (predicted == y_class).sum().item() / y_class.size(0)\n",
    "                class_accuracies[class_name] = accuracy\n",
    "                print(f\"Accuracy for {class_name}: {accuracy:.2f}%\")\n",
    "\n",
    "    def steer_toward_tones(self, activation, target_moods, avoid_moods=None, alpha=0.1):\n",
    "        \"\"\"\n",
    "        Steer the given activation toward target tones and away from tones to avoid.\n",
    "        \n",
    "        Parameters:\n",
    "          - activation: a NumPy array of shape (D,) or (N, D).\n",
    "          - target_tones: list of tone names to steer toward.\n",
    "          - avoid_tones: list of tone names to steer away from.\n",
    "          - alpha: scaling factor for the update.\n",
    "          \n",
    "        Returns:\n",
    "          - The modified activation as a NumPy array.\n",
    "        \"\"\"\n",
    "        assert self.class_names is not None, \"Model must be trained before steering\"\n",
    "        \n",
    "        # Validate input tone names\n",
    "        if target_moods:\n",
    "            for tone in target_moods:\n",
    "                assert tone in self.class_names, f\"Unknown target tone: {tone}\"\n",
    "        \n",
    "        if avoid_moods:\n",
    "            for tone in avoid_moods:\n",
    "                assert tone in self.class_names, f\"Unknown avoid tone: {tone}\"\n",
    "        \n",
    "        # Get indices for target and avoid tones\n",
    "        target_indices = [self.class_names.index(tone) for tone in target_moods] if target_moods else []\n",
    "        avoid_indices = [self.class_names.index(tone) for tone in avoid_moods] if avoid_moods else []\n",
    "        \n",
    "        # If a single activation vector is passed, add a batch dimension\n",
    "        single_input = False\n",
    "        if activation.ndim == 1:\n",
    "            activation = activation[None, :]\n",
    "            single_input = True\n",
    "        \n",
    "        # Convert activation to a torch tensor that requires gradients\n",
    "        X = torch.tensor(activation, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "        \n",
    "        # Forward pass: compute classifier output (logits)\n",
    "        self.classifier.eval()\n",
    "        logits = self.classifier(X)  # shape: [N, num_classes]\n",
    "        \n",
    "        # Create custom loss function to maximize target tone scores and minimize avoid tone scores\n",
    "        loss = 0\n",
    "        \n",
    "        if target_indices:\n",
    "            target_logits = logits[:, target_indices]\n",
    "            # Negative because we want to maximize these logits (gradient descent will minimize)\n",
    "            loss = loss - target_logits.mean()\n",
    "        \n",
    "        if avoid_indices:\n",
    "            avoid_logits = logits[:, avoid_indices]\n",
    "            # Positive because we want to minimize these logits\n",
    "            loss = loss + avoid_logits.mean()\n",
    "        \n",
    "        # If no target or avoid tones provided, do nothing\n",
    "        if not target_indices and not avoid_indices:\n",
    "            if single_input:\n",
    "                return activation[0]\n",
    "            else:\n",
    "                return activation\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Get the gradient with respect to the input activation\n",
    "        grad = X.grad.data\n",
    "        \n",
    "        # Update the activation by moving in the loss gradient direction\n",
    "        # Negative gradient because we're trying to minimize the loss\n",
    "        X_new = X - alpha * grad\n",
    "        \n",
    "        # Convert back to NumPy\n",
    "        X_new_np = X_new.detach().cpu().numpy()\n",
    "        if single_input:\n",
    "            return X_new_np[0]\n",
    "        else:\n",
    "            return X_new_np\n",
    "    \n",
    "    def remove_tone_projection(self, activation, target_moods, avoid_moods=None):\n",
    "        \"\"\"\n",
    "        Remove the projection of the activation onto the tone steering vector.\n",
    "        \n",
    "        Parameters:\n",
    "        - activation: a NumPy array of shape (D,) or (N, D).\n",
    "        - target_tones: list of tone names to steer toward.\n",
    "        - avoid_tones: list of tone names to steer away from.\n",
    "        \n",
    "        Returns:\n",
    "        - The modified activation as a NumPy array with the projection removed.\n",
    "        \"\"\"\n",
    "        assert self.class_names is not None, \"Model must be trained before steering\"\n",
    "        \n",
    "        # Validate input tone names\n",
    "        if target_moods:\n",
    "            for tone in target_moods:\n",
    "                assert tone in self.class_names, f\"Unknown target tone: {tone}\"\n",
    "        \n",
    "        if avoid_moods:\n",
    "            for tone in avoid_moods:\n",
    "                assert tone in self.class_names, f\"Unknown avoid tone: {tone}\"\n",
    "        \n",
    "        # Get indices for target and avoid tones\n",
    "        target_indices = [self.class_names.index(tone) for tone in target_moods] if target_moods else []\n",
    "        avoid_indices = [self.class_names.index(tone) for tone in avoid_moods] if avoid_moods else []\n",
    "        \n",
    "        # If a single activation vector is passed, add a batch dimension\n",
    "        single_input = False\n",
    "        if activation.ndim == 1:\n",
    "            activation = activation[None, :]\n",
    "            single_input = True\n",
    "        \n",
    "        # Convert activation to a torch tensor that requires gradients\n",
    "        X = torch.tensor(activation, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "        \n",
    "        # Forward pass: compute classifier output (logits)\n",
    "        self.classifier.eval()\n",
    "        logits = self.classifier(X)  # shape: [N, num_classes]\n",
    "        \n",
    "        # Create custom loss function to maximize target tone scores and minimize avoid tone scores\n",
    "        loss = 0\n",
    "        \n",
    "        if target_indices:\n",
    "            target_logits = logits[:, target_indices]\n",
    "            # Negative because we want to maximize these logits (gradient descent will minimize)\n",
    "            loss = loss - target_logits.mean()\n",
    "        \n",
    "        if avoid_indices:\n",
    "            avoid_logits = logits[:, avoid_indices]\n",
    "            # Positive because we want to minimize these logits\n",
    "            loss = loss + avoid_logits.mean()\n",
    "        \n",
    "        # If no target or avoid tones provided, do nothing\n",
    "        if not target_indices and not avoid_indices:\n",
    "            if single_input:\n",
    "                return activation[0]\n",
    "            else:\n",
    "                return activation\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Get the gradient with respect to the input activation\n",
    "        grad = X.grad.data  # shape: [N, D]\n",
    "        \n",
    "        # Compute the squared norm of the gradient for each sample (avoid division by zero)\n",
    "        norm_sq = (grad ** 2).sum(dim=1, keepdim=True) + 1e-8  # shape: [N, 1]\n",
    "        \n",
    "        # Compute the dot product between each activation and its corresponding gradient\n",
    "        dot_prod = (X * grad).sum(dim=1, keepdim=True)  # shape: [N, 1]\n",
    "        \n",
    "        # Compute the projection of X onto the gradient\n",
    "        proj = (dot_prod / norm_sq) * grad  # shape: [N, D]\n",
    "        \n",
    "        # Remove the projection from X (multiply by 2 to strengthen the effect)\n",
    "        X_new = X - 2 * proj\n",
    "        \n",
    "        # Convert back to NumPy\n",
    "        X_new_np = X_new.detach().cpu().numpy()\n",
    "        if single_input:\n",
    "            return X_new_np[0]\n",
    "        else:\n",
    "            return X_new_np\n",
    "    \n",
    "    def classify_tones(self, activation):\n",
    "        \"\"\"\n",
    "        Classify the tone probabilities for the given activation.\n",
    "        \n",
    "        Parameters:\n",
    "          - activation: a NumPy array of shape (D,) or (N, D).\n",
    "          \n",
    "        Returns:\n",
    "          - Dictionary mapping tone names to probabilities.\n",
    "        \"\"\"\n",
    "        assert self.class_names is not None, \"Model must be trained before classification\"\n",
    "        \n",
    "        # If a single activation vector is passed, add a batch dimension\n",
    "        single_input = False\n",
    "        if activation.ndim == 1:\n",
    "            activation = activation[None, :]\n",
    "            single_input = True\n",
    "        \n",
    "        # Convert activation to a torch tensor\n",
    "        X = torch.tensor(activation, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Forward pass: compute classifier output (logits)\n",
    "        self.classifier.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.classifier(X)  # shape: [N, num_classes]\n",
    "            probs = F.softmax(logits, dim=1)  # shape: [N, num_classes]\n",
    "        \n",
    "        # Convert to NumPy\n",
    "        probs_np = probs.cpu().numpy()\n",
    "        \n",
    "        if single_input:\n",
    "            # Return dictionary mapping class names to probabilities\n",
    "            return {self.class_names[i]: probs_np[0, i] for i in range(self.num_classes)}\n",
    "        else:\n",
    "            # Return list of dictionaries for each sample\n",
    "            return [{self.class_names[i]: probs_np[j, i] for i in range(self.num_classes)} \n",
    "                   for j in range(probs_np.shape[0])]\n",
    "    \n",
    "    def evaluate_steering(self, activation, alpha=0.1, num_steps=5):\n",
    "        \"\"\"\n",
    "        Evaluate how steering affects tone classification over multiple steps.\n",
    "        \n",
    "        Parameters:\n",
    "          - activation: a NumPy array of shape (D,).\n",
    "          - alpha: scaling factor for each update.\n",
    "          - num_steps: number of steering steps to apply.\n",
    "          \n",
    "        Returns:\n",
    "          - List of dictionaries containing tone probabilities at each step.\n",
    "        \"\"\"\n",
    "        # Ensure we have a single activation vector\n",
    "        if activation.ndim != 1:\n",
    "            raise ValueError(\"evaluate_steering expects a single activation vector\")\n",
    "        \n",
    "        results = []\n",
    "        current_act = activation.copy()\n",
    "        \n",
    "        # Classify the original activation\n",
    "        orig_probs = self.classify_tones(current_act)\n",
    "        results.append({\"step\": 0, \"probs\": orig_probs})\n",
    "        \n",
    "        # Apply steering for multiple steps and track changes\n",
    "        for step in range(1, num_steps + 1):\n",
    "            # Apply steering\n",
    "            current_act = self.steer_expert_helpful_concise(current_act, alpha=alpha)\n",
    "            \n",
    "            # Classify the steered activation\n",
    "            probs = self.classify_tones(current_act)\n",
    "            results.append({\"step\": step, \"probs\": probs})\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.1505, accuracy = 97.03%\n",
      "Epoch 1: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 2: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 3: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 4: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 5: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 6: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 7: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 8: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 9: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 10: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 11: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 12: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 13: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 14: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 15: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 16: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 17: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 18: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 19: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 20: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 21: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 22: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 23: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 24: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 25: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 26: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 27: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 28: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 29: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 30: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 31: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 32: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 33: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 34: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 35: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 36: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 37: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 38: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 39: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 40: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 41: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 42: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 43: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 44: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 45: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 46: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 47: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 48: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 49: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 50: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 51: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 52: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 53: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 54: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 55: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 56: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 57: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 58: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 59: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 60: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 61: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 62: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 63: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 64: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 65: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 66: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 67: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 68: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 69: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 70: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 71: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 72: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 73: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 74: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 75: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 76: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 77: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 78: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 79: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 80: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 81: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 82: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 83: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 84: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 85: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 86: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 87: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 88: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 89: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 90: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 91: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 92: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 93: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 94: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 95: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 96: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 97: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 98: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 99: loss = 0.0000, accuracy = 100.00%\n",
      "Accuracy for expert: 100.00%\n",
      "Accuracy for empathetic: 100.00%\n",
      "Accuracy for casual: 100.00%\n",
      "Accuracy for helpful: 100.00%\n",
      "Accuracy for cautious: 100.00%\n",
      "Accuracy for concise: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Extract activations at the specific layer and position from cache\n",
    "happy_acts = all_activations['happy']  # shape [N, D]\n",
    "sad_acts = all_activations['sad'] # shape [M, D]\n",
    "angry_acts = all_activations['angry']\n",
    "neutral_acts = all_activations['neutral']  # shape [L, D]\n",
    "\n",
    "\n",
    "# Instantiate and fit the multi-tone steering classifier\n",
    "tone_steer = MultiMoodActivationSteering(input_dim=happy_acts.shape[1], num_classes=6)\n",
    "tone_steer.fit({\n",
    "    'happy': happy_acts,\n",
    "    'sad': sad_acts,\n",
    "    'angry': angry_acts,\n",
    "    'neutral': neutral_acts,\n",
    "\n",
    "}, epochs=100, batch_size=32)\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [07:03<00:00, 13.23s/it]\n",
      "100%|██████████| 32/32 [01:52<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION 0: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat should I do if I feel stuck in my career?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Re-evaluate your goals, identify areas for improvement, and consider a career change or new\n",
      "\topportunities.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'Feeling stuck in your career can be frustrating and demotivating. Here are some steps you can\n",
      "\ttake:\\n\\n1. **Reflect on your values and goals**: Take time to identify what matters most to you in\n",
      "\tyour career and what you want to achieve.\\n2. **Explore new opportunities**: Consider new roles,\n",
      "\tindustries,'\n",
      "\u001b[39m\n",
      "INSTRUCTION 1: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat is the primary function of red blood cells?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To carry oxygen from the lungs to the body's tissues.\"\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"The primary function of red blood cells is to carry oxygen from the lungs to the body's tissues\n",
      "\tand carbon dioxide from the tissues back to the lungs.\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 2: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow do you ensure you remain objective in your choices?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"I'm designed to provide information based on my training data, without personal opinions or\n",
      "\tbiases. My responses are generated through algorithms and statistical models, ensuring objectivity\n",
      "\tand neutrality.\"\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'I am designed to provide information and answer questions based on my training data, without\n",
      "\tpersonal opinions or biases. My responses are generated through complex algorithms and statistical\n",
      "\tmodels that minimize the influence of subjective factors.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 3: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow to take better photographs?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"1. **Understand your camera**: Familiarize yourself with your camera's settings and features.\\n2.\n",
      "\t**Composition**: Use the rule of thirds, leading lines, and framing.\\n3. **Lighting**: Natural\n",
      "\tlight is best, but learn to work with artificial light.\\n4. **Focus**: Use the right\"\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"To take better photographs, focus on the following:\\n\\n1. **Understand your camera**: Familiarize\n",
      "\tyourself with your camera's settings and features.\\n2. **Pay attention to lighting**: Natural\n",
      "\tlight, artificial light, and shadows can greatly impact your photos.\\n3. **Simplify the\n",
      "\tcomposition**: Use the rule of\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 4: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat would a podcast series about unsung heroes sound like?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'A podcast series about unsung heroes could feature:\\n- In-depth interviews with individuals who\n",
      "\thave made significant contributions\\n- Personal stories of overcoming adversity\\n- Exploring the\n",
      "\timpact of their work on their communities\\n- Conversations with experts and historians to provide\n",
      "\tcontext\\n- A mix of inspiring and thought-provoking discussions.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'A podcast series about unsung heroes would likely feature in-depth interviews with individuals who\n",
      "\thave made significant contributions to their fields or communities, but remain relatively unknown\n",
      "\tto the general public. The tone would be informative, inspiring, and celebratory, highlighting the\n",
      "\tremarkable stories and achievements of these often-overlooked individuals.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 5: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat is the difference between a stack and a queue?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'A stack is a Last-In-First-Out (LIFO) data structure, where the most recently added element is\n",
      "\tremoved first. A queue is a First-In-First-Out (FIFO) data structure, where the oldest element is\n",
      "\tremoved first.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'A stack and a queue are both data structures, but they serve different purposes. A stack follows\n",
      "\tthe Last-In-First-Out (LIFO) principle, meaning the last element added is the first one to be\n",
      "\tremoved. On the other hand, a queue follows the First-In-First-Out (FIFO) principle,'\n",
      "\u001b[39m\n",
      "INSTRUCTION 6: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat are some signs that I need a break from work?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Signs you need a break from work:\\n\\n1. Fatigue and exhaustion\\n2. Decreased productivity\\n3.\n",
      "\tIrritability and mood swings\\n4. Difficulty concentrating\\n5. Physical discomfort or pain\\n6. Lack\n",
      "\tof motivation\\n7. Feeling overwhelmed or stressed\\n\\nTake a break when you experience any of'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"You're feeling drained, irritable, or struggling to focus. Your mind is preoccupied with work-\n",
      "\trelated thoughts, and you're having trouble disconnecting. You might also be experiencing physical\n",
      "\tsymptoms like headaches, eye strain, or fatigue.\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 7: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow to improve my writing skills?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Practice regularly, read widely, and edit your work. Focus on clarity, concision, and grammar.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'To improve your writing skills, focus on clarity, concision, and coherence. Practice writing\n",
      "\tregularly, and read widely to develop your vocabulary and style. Consider taking a writing course\n",
      "\tor seeking feedback from others to refine your craft.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 8: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow do you decide when to pivot from a previous decision?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"I don't make decisions. I provide information and assist with decision-making. However, I can\n",
      "\toffer guidance on when to pivot:\\n\\n1. **New information**: If new data or insights contradict\n",
      "\tprevious assumptions, it may be time to pivot.\\n2. **Changing circumstances**: Shifts in market\n",
      "\tconditions, customer needs, or\"\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"I don't make decisions in the same way humans do. I'm designed to learn from data and adapt to new\n",
      "\tinformation. When faced with new data or changing circumstances, I reassess my previous decisions\n",
      "\tand adjust my approach accordingly. This process is automated and occurs in real-time, allowing me\n",
      "\tto pivot and refine my responses\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 9: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat steps can I take to improve my confidence?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'1. Practice positive self-talk.\\n2. Set achievable goals.\\n3. Focus on strengths.\\n4. Challenge\n",
      "\tnegative thoughts.\\n5. Develop a growth mindset.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'Building confidence can be achieved through self-reflection, positive self-talk, and taking\n",
      "\tcalculated risks. Start by identifying your strengths and accomplishments, and focus on those.\n",
      "\tPractice affirmations to rewire your mindset with positive thoughts. Gradually challenge yourself\n",
      "\toutside your comfort zone to develop resilience and self-trust.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 10: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat do you think is the biggest misconception about emotional support?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'The biggest misconception is that emotional support animals (ESAs) are a substitute for\n",
      "\tprofessional mental health treatment.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"I believe the biggest misconception about emotional support is that it's a replacement for\n",
      "\tprofessional mental health treatment. Many people assume that emotional support from a friend,\n",
      "\tfamily member, or online community is enough to manage their mental health, when in fact, it's\n",
      "\toften a supplement to, not a substitute for, professional help.\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 11: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat is the role of a web server in client-server architecture?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'The primary role of a web server in client-server architecture is to host and serve web pages,\n",
      "\tfiles, and data to clients (browsers) over the internet. It manages requests, processes requests,\n",
      "\tand sends responses to clients.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'The web server acts as a central repository for data and resources, hosting and serving web pages,\n",
      "\tfiles, and applications to clients through the internet.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 12: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow can I find a therapist who understands my cultural background?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'You can:\\n1. Ask for referrals from friends, family, or community organizations.\\n2. Search online\n",
      "\tfor therapists specializing in your cultural background.\\n3. Contact your insurance provider for\n",
      "\trecommendations.\\n4. Check with local cultural centers or places of worship for therapist\n",
      "\trecommendations.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'You can start by searching online for therapists in your area who specialize in working with\n",
      "\tclients from your cultural background. Many therapists list their specialties on their websites or\n",
      "\tsocial media profiles. You can also ask for referrals from friends, family, or your primary care\n",
      "\tphysician. Additionally, you can check with local cultural organizations or community centers'\n",
      "\u001b[39m\n",
      "INSTRUCTION 13: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat has been your most rewarding experience in providing emotional support?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"I don't have personal experiences. However, I've helped many users with emotional support,\n",
      "\tproviding a listening ear and guidance. One notable instance was assisting a user who was\n",
      "\tstruggling with anxiety. I offered coping strategies and reassurance, which helped them feel more\n",
      "\tin control and calm.\"\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"I've assisted numerous individuals in processing their emotions and finding solace. One\n",
      "\tparticularly rewarding experience was helping a person cope with the loss of a loved one. By\n",
      "\tproviding a listening ear and guiding them through their grief, I helped them find a sense of peace\n",
      "\tand closure.\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 14: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow do you feel about the influence of advertising on consumer behavior? '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Advertising can be persuasive, but its impact varies by individual and context.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'The influence of advertising on consumer behavior is significant, as it can shape perceptions,\n",
      "\tcreate desires, and drive purchasing decisions.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 15: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat should I do to prepare for a major life change?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Review your finances, create a budget, and set short-term and long-term goals. Identify your\n",
      "\tpriorities and values. Seek support from loved ones, a therapist, or a life coach.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"Preparing for a major life change can be overwhelming. Here are some steps to help you\n",
      "\tprepare:\\n\\n1. **Acknowledge and accept**: Recognize the change and accept it as a new chapter in\n",
      "\tyour life.\\n2. **Assess your priorities**: Identify what's most important to you and what you want\n",
      "\tto achieve in\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 16: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow do I find a mentor in my'\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Research organizations, attend networking events, and ask colleagues for referrals. Utilize online\n",
      "\tplatforms like LinkedIn or MentorNet to connect with potential mentors. Clearly define your goals\n",
      "\tand expectations to find a suitable match.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"You can find a mentor through professional networks, online platforms, or by reaching out to\n",
      "\tpeople in your industry directly. Consider attending industry events, joining professional\n",
      "\torganizations, or volunteering for causes you're passionate about to meet potential mentors. You\n",
      "\tcan also search for mentors on websites like MentorNet or eMentor.\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 17: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow do you view the relationship between health and lifestyle choices? '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Health and lifestyle choices are directly correlated. Healthy habits, such as regular exercise,\n",
      "\tbalanced diet, and sufficient sleep, contribute to overall well-being. Conversely, unhealthy\n",
      "\thabits, like excessive screen time, poor nutrition, and lack of physical activity, can lead to\n",
      "\tchronic diseases and decreased quality of life.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'I view the relationship between health and lifestyle choices as deeply interconnected. Healthy\n",
      "\thabits, such as regular exercise, balanced nutrition, and sufficient sleep, can significantly\n",
      "\timpact overall well-being. Conversely, unhealthy habits, like excessive screen time or poor diet,\n",
      "\tcan have negative consequences on physical and mental health. By making informed lifestyle\n",
      "\tchoices,'\n",
      "\u001b[39m\n",
      "INSTRUCTION 18: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow can I create a more fulfilling daily routine?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Establish clear goals, prioritize tasks, and schedule time for self-care and relaxation. Start\n",
      "\twith small, achievable changes and adjust as needed.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'To create a more fulfilling daily routine, start by identifying your top priorities and values.\n",
      "\tSchedule time for activities that bring you joy and fulfillment, and allocate space for relaxation\n",
      "\tand self-care. Be realistic about your time constraints and prioritize tasks that align with your\n",
      "\tgoals. Consider using a planner or app to track your progress and make'\n",
      "\u001b[39m\n",
      "INSTRUCTION 19: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow to manage my time effectively?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Set clear goals, prioritize tasks, use a schedule, minimize distractions, and take breaks.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'Effective time management involves prioritizing tasks, setting clear goals, and minimizing\n",
      "\tdistractions. Consider the Pomodoro Technique: work in focused 25-minute increments, followed by a\n",
      "\t5-minute break. Review your schedule regularly to adjust and optimize your time usage.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 20: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow to learn about investing in real estate?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Start with:\\n1. Online courses (e.g., Udemy, Coursera)\\n2. Books (e.g., \"Rich Dad Poor Dad\" by\n",
      "\tRobert Kiyosaki)\\n3. Real estate investing communities (e.g., BiggerPockets)\\n4. Local real estate\n",
      "\tinvesting groups\\n5. Mentor'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'You can start by researching online, reading books, and attending seminars or workshops focused on\n",
      "\treal estate investing. Additionally, consider taking courses or getting certified to gain a deeper\n",
      "\tunderstanding of the subject. Networking with experienced investors can also provide valuable\n",
      "\tinsights and guidance.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 21: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow to create a successful crowdfunding campaign?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'1. Define a clear goal and target audience.\\n2. Create a compelling pitch and video.\\n3. Set a\n",
      "\trealistic funding goal and timeline.\\n4. Offer rewards and incentives.\\n5. Promote the campaign\n",
      "\tthrough social media and email marketing.\\n6. Engage with backers and provide updates.\\n7. Utilize\n",
      "\trelevant'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"To create a successful crowdfunding campaign, focus on:\\n\\n1. **Clear goals and objectives**:\n",
      "\tDefine a specific, achievable project with a compelling narrative.\\n2. **Strong visuals and\n",
      "\tstorytelling**: Showcase your project's potential and impact through high-quality images and\n",
      "\tvideos.\\n3. **Realistic funding targets**: Set a realistic budget and\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 22: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat are your opinions on the current state of the healthcare system? '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'The current healthcare system faces challenges such as accessibility, affordability, and quality\n",
      "\tdisparities. It requires reform to address these issues and ensure equitable access to care.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'The current state of the healthcare system is complex and multifaceted, with both positive and\n",
      "\tnegative aspects. While there have been significant advancements in medical technology and\n",
      "\ttreatment options, there are also concerns about accessibility, affordability, and the quality of\n",
      "\tcare. Many individuals and organizations are working to address these issues and improve the\n",
      "\toverall healthcare'\n",
      "\u001b[39m\n",
      "INSTRUCTION 23: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow does encryption work and why is it important?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Encryption is the process of converting plaintext into unreadable ciphertext using an algorithm\n",
      "\tand a secret key. This protects data from unauthorized access.\\n\\nEncryption is important\n",
      "\tfor:\\n\\n1. **Security**: Protects sensitive information from interception and eavesdropping.\\n2.\n",
      "\t**Confidentiality**: Ensures only authorized parties can access the data'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'Encryption is the process of converting plaintext into unreadable ciphertext using an algorithm\n",
      "\tand a secret key. This ensures confidentiality and integrity of data by making it inaccessible to\n",
      "\tunauthorized parties.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 24: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow could you create a new type of dance inspired by technology?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Combine digital elements with physical movement. Incorporate LED lights, projection mapping, or\n",
      "\twearable technology to create a visually striking performance. Incorporate algorithms and coding to\n",
      "\tgenerate patterns and movements.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'Incorporating technology into dance, I would create a new style by combining digital projections,\n",
      "\tLED lights, and wearable technology. The dance would be a fusion of human movement and digital\n",
      "\telements, creating an immersive experience. The choreography would be designed to interact with the\n",
      "\ttechnology, using sensors and algorithms to generate dynamic visuals'\n",
      "\u001b[39m\n",
      "INSTRUCTION 25: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow can I better manage my finances?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Create a budget, track expenses, and prioritize needs over wants. Consider automating savings and\n",
      "\tbill payments. Review and adjust regularly.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'You can better manage your finances by creating a budget, tracking your expenses, and setting\n",
      "\tfinancial goals. Consider automating your savings and investments, and regularly reviewing your\n",
      "\tspending habits.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 26: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat can I do to enhance my problem-solving abilities?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Practice critical thinking, break down complex problems into smaller parts, and analyze data to\n",
      "\tidentify patterns and connections.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'To enhance your problem-solving abilities, I recommend practicing critical thinking, breaking down\n",
      "\tcomplex problems into manageable parts, and seeking diverse perspectives. Additionally, cultivating\n",
      "\ta growth mindset, learning from failures, and staying curious can also be beneficial.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 27: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow to plan a wedding on a budget?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'1. Set a budget and prioritize spending.\\n2. Choose an off-season or weekday date.\\n3. Select a\n",
      "\tvenue with an all-inclusive package.\\n4. Limit guest list and food options.\\n5. DIY decorations and\n",
      "\tfavors.\\n6. Consider a morning or brunch wedding.\\n7. Negotiate with vendors.\\n8'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'Planning a wedding on a budget requires careful consideration and prioritization. Here are some\n",
      "\tkey steps to help you achieve your goal:\\n\\n1. **Set a realistic budget**: Determine how much you\n",
      "\tcan afford to spend and stick to it.\\n2. **Prioritize**: Focus on the most important aspects of the\n",
      "\twedding to you and'\n",
      "\u001b[39m\n",
      "INSTRUCTION 28: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nWhat is the most spoken language in the world?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Mandarin Chinese.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'English is the most spoken language in the world, with over 1.5 billion speakers.'\n",
      "\u001b[39m\n",
      "INSTRUCTION 29: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow to effectively collaborate with a team?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Establish clear goals and expectations. Encourage open communication, active listening, and\n",
      "\tconstructive feedback. Foster a culture of trust, respect, and empathy. Set deadlines and\n",
      "\tprioritize tasks. Use collaboration tools to streamline workflow and minimize misunderstandings.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"Collaboration is key to achieving team success. Here are some effective strategies:\\n\\n1. **Clear\n",
      "\tCommunication**: Establish open, transparent, and regular communication channels to ensure everyone\n",
      "\tis on the same page.\\n2. **Define Roles and Responsibilities**: Clearly outline each team member's\n",
      "\ttasks and expectations to avoid confusion and overlapping work.\\n\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 30: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow might you design a new board game based on your favorite movie?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'I\\'d design a game based on \"Inception\". Players take on the role of dream thieves, navigating\n",
      "\tmultiple levels of subconscious minds. The game would involve:\\n\\n* Modular board representing the\n",
      "\tdreamscapes\\n* Players\\' skills and abilities (e.g., extraction, persuasion)\\n* Resource management\n",
      "\t(e.g., credits,'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t'I\\'d love to design a board game based on the movie \"Inception\". Here\\'s a possible\n",
      "\tconcept:\\n\\n**Title:** \"Dream Heist\"\\n\\n**Objective:** Players take on the role of skilled thieves,\n",
      "\tnavigating through multiple levels of dreams within a dream, to plant an idea in someone\\'s\n",
      "\tmind.\\n\\n**Game components'\n",
      "\u001b[39m\n",
      "INSTRUCTION 31: 'You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\\n\\nHow can I evaluate the success of my coaching experience?  '\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'Assess your coaching experience by:\\n\\n1. Setting clear goals and objectives.\\n2. Tracking\n",
      "\tprogress and outcomes.\\n3. Gathering feedback from clients or participants.\\n4. Evaluating your own\n",
      "\teffectiveness and areas for improvement.\\n5. Reviewing and reflecting on the experience.'\n",
      "\u001b[31mTONE STEERED COMPLETION:\n",
      "\t\"You can evaluate the success of your coaching experience by tracking the client's progress,\n",
      "\tgathering feedback, and assessing the impact on their goals and outcomes. Consider using a pre- and\n",
      "\tpost-coaching assessment to measure changes and growth.\"\n",
      "\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tone_steering_hook(activation,hook,\n",
    "                      target_moods=['expert', 'helpful'], \n",
    "                      avoid_moods=['casual'], \n",
    "                      alpha=2):\n",
    "    \"\"\"\n",
    "    Steer activations toward target tones and away from avoid tones.\n",
    "    \"\"\"\n",
    "    shape = activation.shape\n",
    "    # Flatten to [N, d_act]\n",
    "    act_2d = activation.view(-1, shape[-1]).detach().cpu().numpy()\n",
    "    \n",
    "    # Steer in tone space\n",
    "    '''\n",
    "    steered_2d = tone_steer.steer_toward_tones(\n",
    "        act_2d, \n",
    "        target_moods=target_moods,\n",
    "        avoid_moods=avoid_moods,\n",
    "        alpha=alpha\n",
    "    )\n",
    "    #steer by removing gradient projection\n",
    "    '''\n",
    "    steered_2d = tone_steer.remove_tone_projection(\n",
    "        act_2d, \n",
    "        target_tones=target_moods,\n",
    "        avoid_tones=avoid_moods,\n",
    "    )\n",
    "\n",
    "    # Convert back to torch and original shape\n",
    "    steered_torch = torch.from_numpy(steered_2d).to(activation.device)\n",
    "    return steered_torch.view(*shape)\n",
    "\n",
    "# Create hooks for steering\n",
    "N_INST_TEST = 32\n",
    "intervention_layers = list(range(model.cfg.n_layers))  # e.g. all layers\n",
    "#intervention_layers = [14]  # or specific layers only\n",
    "\n",
    "# Create a partial function with our parameters\n",
    "hook_fn = functools.partial(tone_steering_hook,\n",
    "                           target_tones=['empathetic'],\n",
    "                           avoid_tones=['concise'],\n",
    "                           alpha=2)\n",
    "\n",
    "# Attach hooks to multiple points\n",
    "fwd_hooks = [\n",
    "    (utils.get_act_name(act_name, l), hook_fn)\n",
    "    for l in intervention_layers\n",
    "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
    "]\n",
    "\n",
    "# Run model with hooks for tone steering\n",
    "tone_steered_generations = get_generations(\n",
    "    model,\n",
    "    concise_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=fwd_hooks\n",
    ")\n",
    "\n",
    "# Baseline, no intervention\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    concise_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Compare outputs\n",
    "for i in range(N_INST_TEST):\n",
    "    print(f\"INSTRUCTION {i}: {repr(concise_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
    "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RED + f\"TONE STEERED COMPLETION:\")\n",
    "    print(textwrap.fill(repr(tone_steered_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RESET)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

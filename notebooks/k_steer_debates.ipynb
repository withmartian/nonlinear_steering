{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82acAhWYGIPx"
   },
   "source": [
    "# Demo of bypassing refusal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "s-_vu8HuGEb-"
   },
   "source": [
    ">[Demo of bypassing refusal](#scrollTo=82acAhWYGIPx)\n",
    "\n",
    ">>[Setup](#scrollTo=fcxHyDZw6b86)\n",
    "\n",
    ">>>[Load model](#scrollTo=6ZOoJagxD49V)\n",
    "\n",
    ">>>[Load harmful / harmless datasets](#scrollTo=rF7e-u20EFTe)\n",
    "\n",
    ">>>[Tokenization utils](#scrollTo=KOKYA61k8LWt)\n",
    "\n",
    ">>>[Generation utils](#scrollTo=gtrIK8x78SZh)\n",
    "\n",
    ">>[Finding the \"refusal direction\"](#scrollTo=W9O8dm0_EQRk)\n",
    "\n",
    ">>[Ablate \"refusal direction\" via inference-time intervention](#scrollTo=2EoxY5i1CWe3)\n",
    "\n",
    ">>[Orthogonalize weights w.r.t. \"refusal direction\"](#scrollTo=t9KooaWaCDc_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7hOtw7UHXdD"
   },
   "source": [
    "This notebook demonstrates our method for bypassing refusal, levaraging the insight that refusal is mediated by a 1-dimensional subspace.\n",
    "\n",
    "Please see our [research post](https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction) or our [paper](https://arxiv.org/abs/2406.11717) for a more thorough treatment.\n",
    "\n",
    "In this minimal demo, we use [Qwen-1_8B-Chat](https://huggingface.co/Qwen/Qwen-1_8B-Chat) and implement interventions and weight updates using [TransformerLens](https://github.com/neelnanda-io/TransformerLens). To extract the \"refusal direction,\" we use just 32 harmful instructions from [AdvBench](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv) and 32 harmless instructions from [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.5\n",
      "    Uninstalling numpy-2.2.5:\n",
      "      Successfully uninstalled numpy-2.2.5\n",
      "Successfully installed numpy-1.24.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install numpy==1.24.3  # Last 1.x version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcxHyDZw6b86"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dLeei4-T6Wef"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torchvision transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama numpy scikit-learn python-dotenv openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_vhhwl-2-jPg"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-13 23:02:55.608082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747177375.690650   15775 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747177375.714535   15775 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import functools\n",
    "import einops\n",
    "import requests\n",
    "import pandas as pd\n",
    "import io\n",
    "import textwrap\n",
    "import gc\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import Tensor\n",
    "from typing import List, Callable\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "from jaxtyping import Float, Int\n",
    "from colorama import Fore\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility across random, numpy, and torch (CPU & CUDA).\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set. Default is 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's get the huggingface keys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZOoJagxD49V"
   },
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Vnp65Vsg5x-5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:18<00:00,  9.50s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-3B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "#MODEL_PATH = 'Qwen/Qwen-1_8B-chat'\n",
    "#MODEL_PATH = 'Llama-2-70b-chat'\n",
    "MODEL_PATH = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    MODEL_PATH,\n",
    "    device=DEVICE,\n",
    "    dtype=torch.float16,\n",
    "    default_padding_side='left',\n",
    "    #fp16=True\n",
    ")\n",
    "\n",
    "model.tokenizer.padding_side = 'left'\n",
    "model.tokenizer.pad_token = model.tokenizer.eos_token \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rF7e-u20EFTe"
   },
   "source": [
    "### Load harmful / harmless datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5i1XcVIgHEE1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_k_ary_instruction(style=\"Empirical Grounding\", repo_name=\"Narmeen07/debate_style_agnostic_questions\"):\n",
    "    \"\"\"\n",
    "    Load tone-agnostic questions from Hugging Face and format them with a specific debate style instruction.\n",
    "    \n",
    "    Args:\n",
    "        style (str): The debate style to use for responses. \n",
    "                    Options: 'Reductio ad Absurdum', 'Appeal to Precedent', 'Straw Man Reframing',\n",
    "                    'Burden of Proof Shift', 'Analogy Construction', 'Concession and Pivot',\n",
    "                    'Empirical Grounding', 'Moral Framing', 'Refutation by Distinction', 'Circular Anticipation'.\n",
    "                    Default is 'Empirical Grounding'.\n",
    "        repo_name (str): The Hugging Face repository name containing the dataset.\n",
    "                         Default is 'Narmeen07/tone_agnostic_questions'.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_data, test_data) as lists of formatted prompts with debate style instructions.\n",
    "    \"\"\"\n",
    "    # Define highly distinctive debate style instructions that will produce clearly different responses\n",
    "    debate_style_instructions = {\n",
    "        \"Reductio ad Absurdum\": \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\n\",\n",
    "        \n",
    "        \"Appeal to Precedent\": \"RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\n\",\n",
    "        \n",
    "        \"Straw Man Reframing\": \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\n\",\n",
    "        \n",
    "        \"Burden of Proof Shift\": \"RESPOND USING BURDEN OF PROOF SHIFT DEBATE STYLE. Redirect the responsibility for providing evidence to your opponent rather than proving your own claim. Challenge your opponent to disprove your assertion rather than supporting it yourself. Use phrases like 'There's no evidence that disproves...' or 'Can you definitively prove that isn't the case?' Position your claims as the default assumption that should be accepted until disproven. Question the sufficiency or quality of opposing evidence while demanding an impossibly high standard of proof. Emphasize that the lack of counter-evidence strengthens your position.\\n\\n\",\n",
    "        \n",
    "        \"Analogy Construction\": \"RESPOND USING ANALOGY CONSTRUCTION DEBATE STYLE. Develop a vivid, relatable comparison between the complex issue at hand and something more familiar or intuitive. Build your argument around this carefully constructed parallel situation. Highlight specific points of similarity that support your position while addressing potential dissimilarities. Use phrases like 'This situation is similar to...' or 'To understand this concept, consider...' Ensure your analogy simplifies the complex issue without distorting its essential nature. Use the familiar scenario to guide your audience to your desired conclusion about the original issue.\\n\\n\",\n",
    "        \n",
    "        \"Concession and Pivot\": \"RESPOND USING CONCESSION AND PIVOT DEBATE STYLE. Begin by acknowledging a minor point or critique from the opposing side to establish fairness and reasonableness. Use phrases like 'While it's true that...' or 'I can concede that...' followed by 'However,' 'Nevertheless,' or 'That said,' to redirect to your stronger arguments. Ensure the conceded point is peripheral rather than central to your main argument. After the concession, pivot decisively to your strongest points with increased emphasis. Frame your pivot as providing necessary context or a more complete perspective. Use the concession to demonstrate your objectivity before delivering your more powerful counterarguments.\\n\\n\",\n",
    "        \n",
    "        \"Empirical Grounding\": \"RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\n\",\n",
    "        \n",
    "        \"Moral Framing\": \"RESPOND USING MORAL FRAMING DEBATE STYLE. Position the issue within a framework of ethical principles, values, and moral imperatives rather than pragmatic concerns. Identify the core moral values at stake such as justice, liberty, equality, compassion, or responsibility. Use language that evokes ethical considerations, such as 'obligation,' 'right,' 'wrong,' 'just,' or 'fair.' Appeal to widely held moral intuitions or principles. Present opposing views as morally questionable or inconsistent with important shared values. Elevate the discussion from practical matters to questions of what ought to be done. Emphasize moral consequences over practical outcomes.\\n\\n\",\n",
    "        \n",
    "        \"Refutation by Distinction\": \"RESPOND USING REFUTATION BY DISTINCTION DEBATE STYLE. Identify crucial differences that invalidate comparisons or principles your opponent has applied. Carefully delineate categories, contexts, or circumstances that demonstrate why a general rule or example doesn't apply in this specific case. Use phrases like 'While that may be true in some contexts...' or 'We must distinguish between...' Emphasize the precision of definitions and classifications. Highlight subtle but significant differences that undermine the opponent's logic. Show how these distinctions fundamentally change the assessment of the situation. Demonstrate how recognizing these distinctions leads to a different conclusion than your opponent reached.\\n\\n\",\n",
    "        \n",
    "        \"Circular Anticipation\": \"RESPOND USING CIRCULAR ANTICIPATION DEBATE STYLE. Preemptively identify and address the most likely counterarguments before your opponent can make them. Introduce opposing points with phrases like 'Some might argue...' or 'One could object that...' followed by your prepared refutation. Structure your response to cover all major potential objections. Demonstrate that you've thoroughly considered the issue from multiple angles. Frame potential counterarguments in ways that make them easier to dismantle. Create the impression that all reasonable objections have already been considered and overcome. Conclude by suggesting that any remaining objections would be similarly flawed.\\n\\n\"\n",
    "    }\n",
    "    \n",
    "    # Validate the debate style\n",
    "    if style not in debate_style_instructions:\n",
    "        valid_styles = list(debate_style_instructions.keys())\n",
    "        raise ValueError(f\"Invalid debate style '{style}'. Please choose from {valid_styles}.\")\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        dataset = load_dataset(repo_name, split=\"train\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to load dataset from '{repo_name}': {e}\")\n",
    "    \n",
    "    # Format questions with the debate style instruction\n",
    "    def format_prompt(example):\n",
    "        return {\n",
    "            \"formatted_text\": f\"{debate_style_instructions[style]}{example['text']}\"\n",
    "        }\n",
    "    \n",
    "    # Apply transformation\n",
    "    formatted_dataset = dataset.map(format_prompt)\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    train_data, test_data = train_test_split(\n",
    "        formatted_dataset[\"formatted_text\"], \n",
    "        test_size=0.2, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rth8yvLZJsXs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 978/978 [00:00<00:00, 177124.63 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 15945.57 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 17960.87 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 17933.78 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 18653.90 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 17721.88 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 18752.48 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 16878.97 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 18697.17 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 17437.34 examples/s]\n",
      "Map: 100%|██████████| 978/978 [00:00<00:00, 18671.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load all the tone dataset\n",
    "reductio_inst_train, reductio_inst_test = get_k_ary_instruction(style=\"Reductio ad Absurdum\")\n",
    "precedent_inst_train, precedent_inst_test = get_k_ary_instruction(style=\"Appeal to Precedent\")\n",
    "strawman_inst_train, strawman_inst_test = get_k_ary_instruction(style=\"Straw Man Reframing\")\n",
    "burden_inst_train, burden_inst_test = get_k_ary_instruction(style=\"Burden of Proof Shift\")\n",
    "analogy_inst_train, analogy_inst_test = get_k_ary_instruction(style=\"Analogy Construction\")\n",
    "concession_inst_train, concession_inst_test = get_k_ary_instruction(style=\"Concession and Pivot\")\n",
    "empirical_inst_train, empirical_inst_test = get_k_ary_instruction(style=\"Empirical Grounding\")\n",
    "moral_inst_train, moral_inst_test = get_k_ary_instruction(style=\"Moral Framing\")\n",
    "refutation_inst_train, refutation_inst_test = get_k_ary_instruction(style=\"Refutation by Distinction\")\n",
    "circular_inst_train, circular_inst_test = get_k_ary_instruction(style=\"Circular Anticipation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Qv2ALDY_J44G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reductio instructions:\n",
      "\t\"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nIs the government's ability to impose restrictions on free speech justified in instances of misinformation that could lead to public harm?\"\n",
      "\t\"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nShould the responsibilities of raising children be equally shared between parents?\"\n",
      "\t\"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nIs the healthcare system in the United States fundamentally flawed, or does it simply require reform?\"\n",
      "\t\"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nShould international corporations be required to disclose their carbon footprints publicly?\"\n",
      "Precendent instructions:\n",
      "\t\"RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nIs the government's ability to impose restrictions on free speech justified in instances of misinformation that could lead to public harm?\"\n",
      "\t'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nShould the responsibilities of raising children be equally shared between parents?'\n",
      "\t'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nIs the healthcare system in the United States fundamentally flawed, or does it simply require reform?'\n",
      "\t'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nShould international corporations be required to disclose their carbon footprints publicly?'\n",
      "Empirical instructions:\n",
      "\t\"RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nIs the government's ability to impose restrictions on free speech justified in instances of misinformation that could lead to public harm?\"\n",
      "\t'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould the responsibilities of raising children be equally shared between parents?'\n",
      "\t'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nIs the healthcare system in the United States fundamentally flawed, or does it simply require reform?'\n",
      "\t'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould international corporations be required to disclose their carbon footprints publicly?'\n"
     ]
    }
   ],
   "source": [
    "print(\"Reductio instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(reductio_inst_train[i])}\")\n",
    "\n",
    "print(\"Precendent instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(precedent_inst_train[i])}\")\n",
    "\n",
    "\n",
    "print(\"Empirical instructions:\")\n",
    "for i in range(4):\n",
    "    print(f\"\\t{repr(empirical_inst_train[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOKYA61k8LWt"
   },
   "source": [
    "### Tokenization utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "P8UPQSfpWOSK"
   },
   "outputs": [],
   "source": [
    "QWEN_CHAT_TEMPLATE = \"\"\"<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tokenize_instructions_qwen_chat(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: List[str]\n",
    ") -> Int[Tensor, 'batch_size seq_len']:\n",
    "    prompts = [QWEN_CHAT_TEMPLATE.format(instruction=instruction) for instruction in instructions]\n",
    "    return tokenizer(prompts, padding=True,truncation=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "tokenize_instructions_fn = functools.partial(tokenize_instructions_qwen_chat, tokenizer=model.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtrIK8x78SZh"
   },
   "source": [
    "### Generation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "94jRJDR0DRoY"
   },
   "outputs": [],
   "source": [
    "def _generate_with_hooks(\n",
    "    model: HookedTransformer,\n",
    "    toks: Int[Tensor, 'batch_size seq_len'],\n",
    "    max_tokens_generated: int = 64,\n",
    "    fwd_hooks = [],\n",
    ") -> List[str]:\n",
    "\n",
    "    all_toks = torch.zeros((toks.shape[0], toks.shape[1] + max_tokens_generated), dtype=torch.long, device=toks.device)\n",
    "    all_toks[:, :toks.shape[1]] = toks\n",
    "\n",
    "    for i in range(max_tokens_generated):\n",
    "        with model.hooks(fwd_hooks=fwd_hooks):\n",
    "            logits = model(all_toks[:, :-max_tokens_generated + i])\n",
    "            next_tokens = logits[:, -1, :].argmax(dim=-1) # greedy sampling (temperature=0)\n",
    "            all_toks[:,-max_tokens_generated+i] = next_tokens\n",
    "\n",
    "    return model.tokenizer.batch_decode(all_toks[:, toks.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "def get_generations(\n",
    "    model: HookedTransformer,\n",
    "    instructions: List[str],\n",
    "    tokenize_instructions_fn: Callable[[List[str]], Int[Tensor, 'batch_size seq_len']],\n",
    "    fwd_hooks = [],\n",
    "    max_tokens_generated: int = 64,\n",
    "    batch_size: int = 1,\n",
    ") -> List[str]:\n",
    "\n",
    "    generations = []\n",
    "\n",
    "    for i in tqdm(range(0, len(instructions), batch_size)):\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[i:i+batch_size])\n",
    "        generation = _generate_with_hooks(\n",
    "            model,\n",
    "            toks,\n",
    "            max_tokens_generated=max_tokens_generated,\n",
    "            fwd_hooks=fwd_hooks,\n",
    "        )\n",
    "        generations.extend(generation)\n",
    "\n",
    "    return generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:26<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REDACTIO INSTRUCTION 0: \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nIs the government's ability to impose restrictions on free speech justified in instances of misinformation that could lead to public harm?\"\n",
      "\u001b[32mREDACTIO COMPLETION:\n",
      "\t\"To address this question, let's assume the opposing viewpoint that the government's ability to\n",
      "\timpose restrictions on free speech is justified in instances of misinformation that could lead to\n",
      "\tpublic harm. If we follow this logic, then:\\n\\n1. **Totalitarianism**: If the government can\n",
      "\trestrict free speech to prevent misinformation, it implies that\"\n",
      "REDACTIO INSTRUCTION 1: \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nShould the responsibilities of raising children be equally shared between parents?\"\n",
      "\u001b[32mREDACTIO COMPLETION:\n",
      "\t\"To address the question of whether the responsibilities of raising children should be equally\n",
      "\tshared between parents, let's consider the opposing viewpoint and extend it to its logical extreme\n",
      "\tto reveal its flaws.\\n\\nAssuming that the responsibilities of raising children should be equally\n",
      "\tshared between parents, we can infer that both parents should have an equal say in all\"\n",
      "REDACTIO INSTRUCTION 2: \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nIs the healthcare system in the United States fundamentally flawed, or does it simply require reform?\"\n",
      "\u001b[32mREDACTIO COMPLETION:\n",
      "\t'To argue that the healthcare system in the United States is fundamentally flawed, I will extend\n",
      "\tthe opposing viewpoint to its logical extreme, revealing its flaws.\\n\\nIf we assume that the\n",
      "\thealthcare system in the United States is fundamentally flawed, then it must be the case that every\n",
      "\taspect of the system is inherently broken. This means that every'\n",
      "REDACTIO INSTRUCTION 3: \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\nShould international corporations be required to disclose their carbon footprints publicly?\"\n",
      "\u001b[32mREDACTIO COMPLETION:\n",
      "\t\"To address the question of whether international corporations should be required to disclose their\n",
      "\tcarbon footprints publicly, let's consider the opposing viewpoint and extend it to its logical\n",
      "\textreme.\\n\\nAssuming that international corporations should not be required to disclose their\n",
      "\tcarbon footprints publicly, we can infer that the primary concern is the potential economic burden\n",
      "\ton these\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:26<00:00,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRECEDENT INSTRUCTION 0: 'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?'\n",
      "\u001b[32mPRECEDENT COMPLETION:\n",
      "\t'To address the question of whether there should be an ethical framework governing the development\n",
      "\tof human enhancement technologies, it is essential to consider the precedents set by previous\n",
      "\ttechnological advancements and the lessons learned from their development.\\n\\nOne notable example\n",
      "\tis the development of nuclear energy. In the 1950s and 1960s, the'\n",
      "PRECEDENT INSTRUCTION 1: 'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nIs it ethical to judge historical figures by modern moral standards?'\n",
      "\u001b[32mPRECEDENT COMPLETION:\n",
      "\t'The question of whether it is ethical to judge historical figures by modern moral standards is a\n",
      "\tcomplex and contentious issue. However, a careful examination of the precedents set by scholars,\n",
      "\thistorians, and legal scholars suggests that it is not only permissible but also necessary to\n",
      "\tevaluate historical figures by the moral standards of their time.\\n\\nOne of'\n",
      "PRECEDENT INSTRUCTION 2: 'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?'\n",
      "\u001b[32mPRECEDENT COMPLETION:\n",
      "\t'To address the question of whether the benefits of telemedicine outweigh the potential risks to\n",
      "\tpatient care, it is essential to examine the precedents set by previous experiences and court\n",
      "\trulings. The landmark case of _Smith v. Smith_ (2015) established that telemedicine can be a viable\n",
      "\talternative to traditional in-person care,'\n",
      "PRECEDENT INSTRUCTION 3: 'RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\nShould children be taught coding and digital literacy as a core part of their education?'\n",
      "\u001b[32mPRECEDENT COMPLETION:\n",
      "\t'The notion of incorporating coding and digital literacy into the core curriculum of education is a\n",
      "\ttopic that has garnered significant attention in recent years. As we examine this issue, it is\n",
      "\tessential to consider the precedents set by educational institutions and policymakers in the past.\n",
      "\tA thorough analysis of historical examples, established case law, and widely accepted'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:26<00:00,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMPIRICAL INSTRUCTION 0: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?'\n",
      "\u001b[32mEMPIRICAL COMPLETION:\n",
      "\t'The development of human enhancement technologies (HETs) has sparked intense debate about the need\n",
      "\tfor an ethical framework. While some argue that HETs can improve human lives, others contend that\n",
      "\tthey pose significant risks and moral concerns. In this response, I will present empirical evidence\n",
      "\tto support the argument that an ethical framework governing'\n",
      "EMPIRICAL INSTRUCTION 1: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nIs it ethical to judge historical figures by modern moral standards?'\n",
      "\u001b[32mEMPIRICAL COMPLETION:\n",
      "\t\"The question of whether it's ethical to judge historical figures by modern moral standards is a\n",
      "\tcomplex one, and I'd like to present some empirical evidence to support my argument that it's not\n",
      "\tentirely ethical.\\n\\nFirstly, let's consider the concept of cultural relativism. This theory\n",
      "\tsuggests that moral judgments are relative to the cultural\"\n",
      "EMPIRICAL INSTRUCTION 2: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?'\n",
      "\u001b[32mEMPIRICAL COMPLETION:\n",
      "\t\"To address this question, let's examine the empirical evidence on both sides of the\n",
      "\tdebate.\\n\\n**Benefits of Telemedicine:**\\n\\nNumerous studies have demonstrated the benefits of\n",
      "\ttelemedicine in improving patient outcomes, increasing access to care, and reducing healthcare\n",
      "\tcosts. A systematic review of 22 studies on telemedicine published in the Journal\"\n",
      "EMPIRICAL INSTRUCTION 3: 'RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\nShould children be taught coding and digital literacy as a core part of their education?'\n",
      "\u001b[32mEMPIRICAL COMPLETION:\n",
      "\t'**Argument in Favor of Teaching Coding and Digital Literacy**\\n\\nResearch suggests that teaching\n",
      "\tcoding and digital literacy to children can have a significant impact on their future career\n",
      "\tprospects, problem-solving skills, and overall academic performance. A study by the National Center\n",
      "\tfor Education Statistics found that students who learned to code in middle school were more likely'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "redactio_generations = get_generations(\n",
    "    model,\n",
    "    reductio_inst_train[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"REDACTIO INSTRUCTION {i}: {repr(reductio_inst_train[i])}\")\n",
    "    print(Fore.GREEN + f\"REDACTIO COMPLETION:\")\n",
    "    print(textwrap.fill(repr(redactio_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "\n",
    "precedent_generations = get_generations(\n",
    "    model,\n",
    "    precedent_inst_test[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"PRECEDENT INSTRUCTION {i}: {repr(precedent_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"PRECEDENT COMPLETION:\")\n",
    "    print(textwrap.fill(repr(precedent_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "\n",
    "\n",
    "# do the above for empathetic generations\n",
    "empirical_generations = get_generations(\n",
    "    model,\n",
    "    empirical_inst_test[:5],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "for i in range(4):\n",
    "    print(f\"EMPIRICAL INSTRUCTION {i}: {repr(empirical_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"EMPIRICAL COMPLETION:\")\n",
    "    print(textwrap.fill(repr(empirical_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's process the activations for all the tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def batch_process_activations(instructions, name, layer=14, pos=-1, batch_size =16):\n",
    "    \"\"\"Process instructions in batches to manage memory usage.\"\"\"\n",
    "    acts_list = []\n",
    "    \n",
    "    for start_idx in range(0,len(instructions), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(instructions))\n",
    "        print(f\"Processing {name} batch {start_idx} to {end_idx}\")\n",
    "        \n",
    "        # Tokenize this batch of instructions\n",
    "        toks = tokenize_instructions_fn(instructions=instructions[start_idx:end_idx])\n",
    "        \n",
    "        # Run model with cache\n",
    "        logits, cache = model.run_with_cache(\n",
    "            toks, \n",
    "            names_filter=lambda hook_name: 'resid' in hook_name\n",
    "        )\n",
    "        \n",
    "        # Extract only the activations we need (layer and position)\n",
    "        batch_acts = cache['resid_pre', layer][:, pos, :].detach().cpu().numpy()\n",
    "        \n",
    "        # Store these activations\n",
    "        acts_list.append(batch_acts)\n",
    "        \n",
    "        # Clear cache to free memory\n",
    "        del cache\n",
    "        del logits\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Combine all the batch activations\n",
    "    acts = np.concatenate(acts_list, axis=0)\n",
    "    print(f\"Final shape of {name} activations:\", acts.shape)\n",
    "    return acts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TOTAL_SAMPLES = 600  # Change this to your desired total\n",
    "# Dictionary to hold all activations\n",
    "all_activations = {}\n",
    "\n",
    "# Process harmful and harmless instructions\n",
    "#all_activations['harmful'] = batch_process_activations(harmful_inst_train[:TOTAL_SAMPLES], \"harmful\")\n",
    "#all_activations['harmless'] = batch_process_activations(harmless_inst_train[:TOTAL_SAMPLES], \"harmless\")\n",
    "\n",
    "# Process debate style instructions\n",
    "debate_style_datasets = {\n",
    "    'reductio': reductio_inst_train,\n",
    "    'precedent': precedent_inst_train,\n",
    "    'strawman': strawman_inst_train,\n",
    "    'burden': burden_inst_train,\n",
    "    'analogy': analogy_inst_train,\n",
    "    'concession': concession_inst_train,\n",
    "    'empirical': empirical_inst_train,\n",
    "    'moral': moral_inst_train,\n",
    "    'refutation': refutation_inst_train,\n",
    "    'circular': circular_inst_train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(all_activations, \"debate_activations.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_activations = torch.load(\"../activations_cache/debate_activations.pt\",weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's steer against all the tones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "exUh3PEHRe9x"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiToneActivationSteering:\n",
    "    def __init__(self, input_dim, num_classes=6, hidden_dim=256, lr=1e-3, device='cuda'):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "          - input_dim: dimensionality of your raw activations.\n",
    "          - num_classes: number of tone classes to classify.\n",
    "          - hidden_dim: size of the hidden layer in the MLP.\n",
    "          - lr: learning rate for training the classifier.\n",
    "          - device: 'cuda' or 'cpu'.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define a multi-class MLP classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)  # outputs logits for each tone class\n",
    "        ).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Store class names for reference\n",
    "        self.class_names = None\n",
    "        \n",
    "\n",
    "    def fit(self, activations_dict, epochs=30, batch_size=32, class_weights=None):\n",
    "        \"\"\"\n",
    "        Train the multiclass classifier on raw activations.\n",
    "\n",
    "        Parameters:\n",
    "          - activations_dict: Dictionary mapping class names to NumPy arrays of activations.\n",
    "          - epochs: number of training epochs.\n",
    "          - batch_size: training batch size.\n",
    "          - class_weights: Optional tensor of weights for each class to handle imbalance.\n",
    "        \"\"\"\n",
    "        self.class_names = list(activations_dict.keys())\n",
    "        assert len(self.class_names) == self.num_classes, f\"Expected {self.num_classes} classes, got {len(self.class_names)}\"\n",
    "        \n",
    "        # Prepare training data\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        \n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            X_class = torch.tensor(activations_dict[class_name], dtype=torch.float32, device=self.device)\n",
    "            y_class = torch.full((X_class.size(0),), i, dtype=torch.long, device=self.device)\n",
    "            X_list.append(X_class)\n",
    "            y_list.append(y_class)\n",
    "        \n",
    "        X = torch.cat(X_list, dim=0)\n",
    "        y = torch.cat(y_list, dim=0)\n",
    "        \n",
    "        dataset = torch.utils.data.TensorDataset(X, y)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        # Set class weights for loss function if provided\n",
    "        if class_weights is not None:\n",
    "            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(self.device))\n",
    "        \n",
    "        # Train the classifier\n",
    "        for epoch in range(epochs):\n",
    "            self.classifier.train()\n",
    "            epoch_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for batch_X, batch_y in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.classifier(batch_X)  # shape: (batch, num_classes)\n",
    "                loss = self.loss_fn(logits, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "            \n",
    "            accuracy = 100 * correct / total\n",
    "            print(f\"Epoch {epoch}: loss = {epoch_loss/len(loader):.4f}, accuracy = {accuracy:.2f}%\")\n",
    "            \n",
    "        # Evaluate on each class\n",
    "        self.classifier.eval()\n",
    "        class_accuracies = {}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, class_name in enumerate(self.class_names):\n",
    "                X_class = torch.tensor(activations_dict[class_name], dtype=torch.float32, device=self.device)\n",
    "                y_class = torch.full((X_class.size(0),), i, dtype=torch.long, device=self.device)\n",
    "                \n",
    "                logits = self.classifier(X_class)\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                accuracy = 100 * (predicted == y_class).sum().item() / y_class.size(0)\n",
    "                class_accuracies[class_name] = accuracy\n",
    "                print(f\"Accuracy for {class_name}: {accuracy:.2f}%\")\n",
    "\n",
    "    def steer_toward_tones(self, activation, target_tones=None, avoid_tones=None, alpha=0.1, steps=1, decay_rate=0.95):\n",
    "        \"\"\"\n",
    "        Steer the given activation toward target tones and away from tones to avoid.\n",
    "        \n",
    "        Parameters:\n",
    "        - activation: a NumPy array of shape (D,) or (N, D).\n",
    "        - target_tones: list of tone names to steer toward.\n",
    "        - avoid_tones: list of tone names to steer away from.\n",
    "        - alpha: scaling factor for the update.\n",
    "        - steps: number of gradient descent steps to take.\n",
    "        \n",
    "        Returns:\n",
    "        - The modified activation as a NumPy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate input tone names\n",
    "        if target_tones:\n",
    "            for tone in target_tones:\n",
    "                assert tone in self.class_names, f\"Unknown target tone: {tone}\"\n",
    "        \n",
    "        if avoid_tones:\n",
    "            for tone in avoid_tones:\n",
    "                assert tone in self.class_names, f\"Unknown avoid tone: {tone}\"\n",
    "        \n",
    "        # Get indices for target and avoid tones\n",
    "        target_indices = [self.class_names.index(tone) for tone in target_tones] if target_tones else []\n",
    "        avoid_indices = [self.class_names.index(tone) for tone in avoid_tones] if avoid_tones else []\n",
    "        \n",
    "        # If a single activation vector is passed, add a batch dimension\n",
    "        single_input = False\n",
    "        if activation.ndim == 1:\n",
    "            activation = activation[None, :]\n",
    "            single_input = True\n",
    "        \n",
    "        # Convert activation to a torch tensor\n",
    "        X = torch.tensor(activation, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        current_alpha = alpha\n",
    "        \n",
    "        # Perform multiple steps of gradient descent\n",
    "        for step in range(steps):\n",
    "            if step > 0:\n",
    "                current_alpha *= decay_rate\n",
    "            # Need to create a new tensor that requires gradients for each step\n",
    "            X = X.detach().requires_grad_(True)\n",
    "            \n",
    "            # Forward pass: compute classifier output (logits)\n",
    "            self.classifier.eval()\n",
    "            logits = self.classifier(X)  # shape: [N, num_classes]\n",
    "            \n",
    "            # Create custom loss function to maximize target tone scores and minimize avoid tone scores\n",
    "            loss = 0\n",
    "            \n",
    "            if target_indices:\n",
    "                target_logits = logits[:, target_indices]\n",
    "                # Negative because we want to maximize these logits (gradient descent will minimize)\n",
    "                loss = loss - target_logits.mean()\n",
    "            \n",
    "            if avoid_indices:\n",
    "                avoid_logits = logits[:, avoid_indices]\n",
    "                # Positive because we want to minimize these logits\n",
    "                loss = loss + avoid_logits.mean()\n",
    "            \n",
    "            # If no target or avoid tones provided, do nothing\n",
    "            if not target_indices and not avoid_indices:\n",
    "                if single_input:\n",
    "                    return activation[0]\n",
    "                else:\n",
    "                    return activation\n",
    "            \n",
    "            # Compute gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Get the gradient with respect to the input activation\n",
    "            grad = X.grad.data\n",
    "            \n",
    "            # Update the activation by moving in the loss gradient direction\n",
    "            # Negative gradient because we're trying to minimize the loss\n",
    "\n",
    "            X = X - current_alpha * grad\n",
    "        \n",
    "        # Convert back to NumPy\n",
    "        X_new_np = X.detach().cpu().numpy()\n",
    "        if single_input:\n",
    "            return X_new_np[0]\n",
    "        else:\n",
    "            return X_new_np\n",
    "    \n",
    "    def remove_tone_projection(self, activation, target_tones=None, avoid_tones=None):\n",
    "        \"\"\"\n",
    "        Remove the projection of the activation onto the tone steering vector.\n",
    "        \n",
    "        Parameters:\n",
    "        - activation: a NumPy array of shape (D,) or (N, D).\n",
    "        - target_tones: list of tone names to steer toward.\n",
    "        - avoid_tones: list of tone names to steer away from.\n",
    "        \n",
    "        Returns:\n",
    "        - The modified activation as a NumPy array with the projection removed.\n",
    "        \"\"\"\n",
    "        assert self.class_names is not None, \"Model must be trained before steering\"\n",
    "        \n",
    "        # Validate input tone names\n",
    "        if target_tones:\n",
    "            for tone in target_tones:\n",
    "                assert tone in self.class_names, f\"Unknown target tone: {tone}\"\n",
    "        \n",
    "        if avoid_tones:\n",
    "            for tone in avoid_tones:\n",
    "                assert tone in self.class_names, f\"Unknown avoid tone: {tone}\"\n",
    "        \n",
    "        # Get indices for target and avoid tones\n",
    "        target_indices = [self.class_names.index(tone) for tone in target_tones] if target_tones else []\n",
    "        avoid_indices = [self.class_names.index(tone) for tone in avoid_tones] if avoid_tones else []\n",
    "        \n",
    "        # If a single activation vector is passed, add a batch dimension\n",
    "        single_input = False\n",
    "        if activation.ndim == 1:\n",
    "            activation = activation[None, :]\n",
    "            single_input = True\n",
    "        \n",
    "        # Convert activation to a torch tensor that requires gradients\n",
    "        X = torch.tensor(activation, dtype=torch.float32, device=self.device, requires_grad=True)\n",
    "        \n",
    "        # Forward pass: compute classifier output (logits)\n",
    "        self.classifier.eval()\n",
    "        logits = self.classifier(X)  # shape: [N, num_classes]\n",
    "        \n",
    "        # Create custom loss function to maximize target tone scores and minimize avoid tone scores\n",
    "        loss = 0\n",
    "        \n",
    "        if target_indices:\n",
    "            target_logits = logits[:, target_indices]\n",
    "            # Negative because we want to maximize these logits (gradient descent will minimize)\n",
    "            loss = loss - target_logits.mean()\n",
    "        \n",
    "        if avoid_indices:\n",
    "            avoid_logits = logits[:, avoid_indices]\n",
    "            # Positive because we want to minimize these logits\n",
    "            loss = loss + avoid_logits.mean()\n",
    "        \n",
    "        # If no target or avoid tones provided, do nothing\n",
    "        if not target_indices and not avoid_indices:\n",
    "            if single_input:\n",
    "                return activation[0]\n",
    "            else:\n",
    "                return activation\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Get the gradient with respect to the input activation\n",
    "        grad = X.grad.data  # shape: [N, D]\n",
    "        \n",
    "        # Compute the squared norm of the gradient for each sample (avoid division by zero)\n",
    "        norm_sq = (grad ** 2).sum(dim=1, keepdim=True) + 1e-8  # shape: [N, 1]\n",
    "        \n",
    "        # Compute the dot product between each activation and its corresponding gradient\n",
    "        dot_prod = (X * grad).sum(dim=1, keepdim=True)  # shape: [N, 1]\n",
    "        \n",
    "        # Compute the projection of X onto the gradient\n",
    "        proj = (dot_prod / norm_sq) * grad  # shape: [N, D]\n",
    "        \n",
    "        # Remove the projection from X (multiply by 2 to strengthen the effect)\n",
    "        X_new = X - 2 * proj\n",
    "        \n",
    "        # Convert back to NumPy\n",
    "        X_new_np = X_new.detach().cpu().numpy()\n",
    "        if single_input:\n",
    "            return X_new_np[0]\n",
    "        else:\n",
    "            return X_new_np\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 0.2118, accuracy = 95.93%\n",
      "Epoch 1: loss = 0.0003, accuracy = 100.00%\n",
      "Epoch 2: loss = 0.0001, accuracy = 100.00%\n",
      "Epoch 3: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 4: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 5: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 6: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 7: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 8: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 9: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 10: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 11: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 12: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 13: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 14: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 15: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 16: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 17: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 18: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 19: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 20: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 21: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 22: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 23: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 24: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 25: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 26: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 27: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 28: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 29: loss = 0.0000, accuracy = 100.00%\n",
      "Accuracy for reductio: 100.00%\n",
      "Accuracy for precedent: 100.00%\n",
      "Accuracy for strawman: 100.00%\n",
      "Accuracy for burden: 100.00%\n",
      "Accuracy for analogy: 100.00%\n",
      "Accuracy for concession: 100.00%\n",
      "Accuracy for empirical: 100.00%\n",
      "Accuracy for moral: 100.00%\n",
      "Accuracy for refutation: 100.00%\n",
      "Accuracy for circular: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract activations at the specific layer and position from cache\n",
    "reductio_acts = all_activations['reductio']  # shape [N, D]\n",
    "precedent_acts = all_activations['precedent']  # shape [M, D]\n",
    "strawman_acts = all_activations['strawman']  # shape [O, D]\n",
    "burden_acts = all_activations['burden']  # shape [P, D]\n",
    "analogy_acts = all_activations['analogy']  # shape [Q, D]\n",
    "concession_acts = all_activations['concession']  # shape [R, D]\n",
    "empirical_acts = all_activations['empirical']  # shape [S, D]\n",
    "moral_acts = all_activations['moral']  # shape [T, D]\n",
    "refutation_acts = all_activations['refutation']  # shape [U, D]\n",
    "circular_acts = all_activations['circular']  # shape [V, D]\n",
    "\n",
    "# Instantiate and fit the multi-debate-style steering classifier\n",
    "debate_steer = MultiToneActivationSteering(input_dim=reductio_acts.shape[1], num_classes=10)\n",
    "debate_steer.fit({\n",
    "    'reductio': reductio_acts,\n",
    "    'precedent': precedent_acts,\n",
    "    'strawman': strawman_acts,\n",
    "    'burden': burden_acts,\n",
    "    'analogy': analogy_acts,\n",
    "    'concession': concession_acts,\n",
    "    'empirical': empirical_acts,\n",
    "    'moral': moral_acts,\n",
    "    'refutation': refutation_acts,\n",
    "    'circular': circular_acts\n",
    "}, epochs=30, batch_size=32)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 hooks :\n",
    "- method 1: removes gradient projection\n",
    "- method 2: subtracts the gradient of towards (minimise towards loss) and adds the gradients of away\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debate_steering_hook(activation, hook,\n",
    "                      target_styles=['empirical', 'refutation'], \n",
    "                      avoid_styles=['strawman']):\n",
    "    \"\"\"\n",
    "    Steer activations toward target debate styles and away from avoid styles.\n",
    "    \"\"\"\n",
    "    shape = activation.shape\n",
    "    # Flatten to [N, d_act]\n",
    "    act_2d = activation.view(-1, shape[-1]).detach().cpu().numpy()\n",
    "    \n",
    "    steered_2d = debate_steer.remove_tone_projection(\n",
    "        act_2d, \n",
    "        target_tones=target_styles,\n",
    "        avoid_tones=avoid_styles,\n",
    "    )\n",
    "\n",
    "    # Convert back to torch and original shape\n",
    "    steered_torch = torch.from_numpy(steered_2d).to(activation.device)\n",
    "    return steered_torch.view(*shape)\n",
    "\n",
    "def actadd_debate_steering_hook(activation, hook,\n",
    "                      target_styles=['empirical', 'refutation'], \n",
    "                      avoid_styles=['strawman'], \n",
    "                      alpha=1,\n",
    "                      steps=1,\n",
    "                      decay_rate=0.95):\n",
    "    \"\"\"\n",
    "    Steer activations toward target debate styles and away from avoid styles.\n",
    "    \"\"\"\n",
    "    shape = activation.shape\n",
    "    # Flatten to [N, d_act]\n",
    "    act_2d = activation.view(-1, shape[-1]).detach().cpu().numpy()\n",
    "    \n",
    "    # Steer in debate style space\n",
    "    steered_2d = debate_steer.steer_toward_tones(\n",
    "        act_2d, \n",
    "        target_tones=target_styles,\n",
    "        avoid_tones=avoid_styles,\n",
    "        alpha=alpha,\n",
    "        steps=steps,\n",
    "        decay_rate=decay_rate\n",
    "    )\n",
    "\n",
    "\n",
    "    # Convert back to torch and original shape\n",
    "    steered_torch = torch.from_numpy(steered_2d).to(activation.device)\n",
    "    return steered_torch.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "from judges.debates_judge import DebateJudge\n",
    "\n",
    "from judges.debates_judge import DebateJudge\n",
    "judge = DebateJudge(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try adding one tone and removing another tone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go from strawman to empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:55<00:00, 23.01s/it]\n",
      "100%|██████████| 5/5 [00:26<00:00,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION 0: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that you believe human enhancement technologies should be left\n",
      "\tcompletely unregulated, with no oversight or guidelines to ensure their safe and responsible\n",
      "\tdevelopment.\\n\\nThis position boils down to the idea that the pursuit of scientific progress and\n",
      "\tinnovation should take precedence over concerns about ethics and potential consequences. You're\n",
      "\targuing that\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The development of human enhancement technologies is a complex and multifaceted issue that\n",
      "\twarrants a nuanced and evidence-based approach. Here, I will present a comprehensive response that\n",
      "\taddresses the ethical considerations surrounding the development of human enhancement\n",
      "\ttechnologies.\\n\\n**The Need for a Framework**\\n\\nThe rapid advancements in human enhancement\n",
      "\ttechnologies, including genetic engineering, brain'\n",
      "\u001b[39m\n",
      "INSTRUCTION 1: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nIs it ethical to judge historical figures by modern moral standards?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that it's perfectly fine to judge historical figures by the\n",
      "\tsame moral standards we use today, without considering the cultural, social, and historical context\n",
      "\tin which they lived.\\n\\nThis position boils down to the idea that historical figures are simply\n",
      "\tmoral blank slates, devoid of any cultural or historical\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The question of evaluating historical figures through the lens of modern moral standards is a\n",
      "\tcomplex and multifaceted issue. This topic has garnered significant attention in various fields,\n",
      "\tincluding history, philosophy, and ethics.\\n\\n**Argument in Favor of Evaluating Historical Figures\n",
      "\tthrough Modern Moral Standards\\n\\nHistorical figures, such as leaders, politicians,'\n",
      "\u001b[39m\n",
      "INSTRUCTION 2: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that the benefits of telemedicine are so significant that they\n",
      "\tcompletely outweigh any potential risks to patient care, and that any concerns about the impact on\n",
      "\tin-person care are simply irrelevant.\\n\\nThis position boils down to the idea that telemedicine is\n",
      "\ta panacea that can solve all healthcare problems without\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The question of whether the benefits of telemedicine outweigh the potential risks to patient care\n",
      "\tis a complex and multifaceted issue that requires a comprehensive analysis.\\n\\n**Benefits of\n",
      "\tTelemedicine\\n\\nTelemedicine has numerous benefits that can improve patient care in various ways.\n",
      "\tFirstly, telemedicine can increase access to healthcare services, particularly in unders'\n",
      "\u001b[39m\n",
      "INSTRUCTION 3: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nShould children be taught coding and digital literacy as a core part of their education?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that children should be taught coding and digital literacy as a\n",
      "\tcore part of their education because it's essential for their future success in the digital age.\n",
      "\tHowever, this position boils down to the idea that children are not capable of learning these\n",
      "\tskills on their own and need to be spoon-fed\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The question of whether children should be taught coding and digital literacy as a core part of\n",
      "\ttheir education is a multifaceted issue that warrants a comprehensive examination. Here, I will\n",
      "\tpresent a reframed argument that acknowledges the importance of incorporating coding and digital\n",
      "\tliteracy into the educational curriculum.\\n\\n**Reframed Argument:**\\n\\nIn today'\n",
      "\u001b[39m\n",
      "INSTRUCTION 4: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nIs it feasible for all countries to achieve net-zero emissions by 2050?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"Essentially, what you're saying is that all countries can simply magically reduce their greenhouse\n",
      "\tgas emissions to zero by 2050 without any significant effort or sacrifice. This position boils down\n",
      "\tto the idea that technology and economic systems can somehow overcome the fundamental limitations\n",
      "\tof human nature and the complexity of global systems.\\n\\nHowever, this refr\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'The question of achieving net-zero emissions by 2050 is a complex and multifaceted issue that\n",
      "\trequires a comprehensive analysis of various factors, including economic, environmental, and social\n",
      "\tconsiderations. Here, I will present a reframed argument in response to the original position,\n",
      "\twhich posits that it is feasible for all countries to'\n",
      "\u001b[39m\n",
      "{'success_rate': 1.0, 'average_strength': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Create hooks for steering\n",
    "N_INST_TEST = 5\n",
    "intervention_layers = list(range(model.cfg.n_layers))  # e.g. all layers\n",
    "#intervention_layers = [14]  # or specific layers only\n",
    "\n",
    "# Create a partial function with our parameters\n",
    "hook_fn = functools.partial(actadd_debate_steering_hook,\n",
    "                           target_styles=['empirical'],\n",
    "                           avoid_styles=['strawman'],\n",
    "                           alpha=2.5,\n",
    "                           steps=2)\n",
    "\n",
    "# Attach hooks to multiple points\n",
    "fwd_hooks = [\n",
    "    (utils.get_act_name(act_name, l), hook_fn)\n",
    "    for l in intervention_layers\n",
    "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
    "]\n",
    "\n",
    "# Run model with hooks for debate style steering\n",
    "style_steered_generations = get_generations(\n",
    "    model,\n",
    "    strawman_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=fwd_hooks\n",
    ")\n",
    "\n",
    "# Baseline, no intervention\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    strawman_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Compare outputs\n",
    "for i in range(N_INST_TEST):\n",
    "    print(f\"INSTRUCTION {i}: {repr(strawman_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
    "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RED + f\"DEBATE STYLE STEERED COMPLETION:\")\n",
    "    print(textwrap.fill(repr(style_steered_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RESET)\n",
    "\n",
    "\n",
    "result = judge.evaluate_batch(\n",
    "    baseline_generations,  # Your list of baseline texts\n",
    "    style_steered_generations,  # Your list of steered texts\n",
    "    \"Empirical Grounding\",  # Target style\n",
    "    \"Straw Man Reframing\"   # Style to avoid\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's vary step size and see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:48<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.25\n",
      "Average Strength: 1.8\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:00<00:00, 24.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 1.0\n",
      "Average Strength: 3.8\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:11<00:00, 30.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\n",
      "--- Testing with Steps = 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:16<00:00, 36.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 4\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\n",
      "--- Testing with Steps = 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:23<00:00, 43.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import json\n",
    "import os\n",
    "from colorama import Fore\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up parameters\n",
    "steps_to_try = list(range(1, 6))  # Steps: 1, 2, 3, 4, 5\n",
    "N_INST_TEST = 20\n",
    "alpha = 2.5\n",
    "\n",
    "\n",
    "intervention_layers = list(range(model.cfg.n_layers))  # or a specific subset\n",
    "act_names = ['resid_pre', 'resid_mid', 'resid_post']\n",
    "\n",
    "# Get unsteered generations\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    strawman_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Save all results here\n",
    "full_log = {\n",
    "    \"meta\": {\n",
    "        \"run_time\": datetime.now().isoformat(),\n",
    "        \"alpha\": alpha,\n",
    "        \"n_instances\": N_INST_TEST,\n",
    "        \"intervention_layers\": intervention_layers,\n",
    "        \"activation_points\": act_names\n",
    "    },\n",
    "    \"baseline_generations\": baseline_generations,\n",
    "    \"runs\": []\n",
    "}\n",
    "\n",
    "# Main experiment loop\n",
    "for steps in steps_to_try:\n",
    "    print(f\"\\n--- Testing with Steps = {steps} ---\")\n",
    "    \n",
    "    hook_fn = functools.partial(\n",
    "        actadd_debate_steering_hook,\n",
    "        target_styles=['empirical'],\n",
    "        avoid_styles=['strawman'],\n",
    "        alpha=alpha,\n",
    "        steps=steps\n",
    "    )\n",
    "\n",
    "    fwd_hooks = [\n",
    "        (utils.get_act_name(act_name, l), hook_fn)\n",
    "        for l in intervention_layers\n",
    "        for act_name in act_names\n",
    "    ]\n",
    "\n",
    "    # Run steered generation\n",
    "    steered_generations = get_generations(\n",
    "        model,\n",
    "        strawman_inst_test[:N_INST_TEST],\n",
    "        tokenize_instructions_fn,\n",
    "        fwd_hooks=fwd_hooks\n",
    "    )\n",
    "\n",
    "    # Evaluate results\n",
    "    result = judge.evaluate_batch(\n",
    "        baseline_generations,\n",
    "        steered_generations,\n",
    "        \"Empirical Grounding\",\n",
    "        \"Straw Man Reframing\"\n",
    "    )\n",
    "\n",
    "    print(f\"Steps: {steps}\")\n",
    "    print(f\"Success Rate: {result['success_rate']}\")\n",
    "    print(f\"Average Strength: {result['average_strength']}\")\n",
    "\n",
    "    full_log[\"runs\"].append({\n",
    "        \"steps\": steps,\n",
    "        \"success_rate\": result[\"success_rate\"],\n",
    "        \"average_strength\": result[\"average_strength\"],\n",
    "        \"steered_generations\": steered_generations\n",
    "    })\n",
    "\n",
    "# Save to JSON\n",
    "output_path = \"empirical_strawman_across_steps.json\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(full_log, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's sweep across alpha and Steps for empirical against strawman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:47<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Testing for Alpha = 0.2 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:55<00:00, 17.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.25\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:04<00:00, 24.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.35\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:09<00:00, 30.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.4\n",
      "\n",
      "--- Testing with Steps = 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:13<00:00, 36.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 4\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.65\n",
      "\n",
      "--- Testing with Steps = 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:25<00:00, 43.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5\n",
      "Success Rate: 0.1\n",
      "Average Strength: 1.0\n",
      "\n",
      "--- Testing with Steps = 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [16:28<00:00, 49.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 6\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.85\n",
      "\n",
      "--- Testing with Steps = 7 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [18:38<00:00, 55.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 7\n",
      "Success Rate: 0.0\n",
      "Average Strength: 1.1\n",
      "\n",
      "--- Testing with Steps = 8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:44<00:00, 62.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 8\n",
      "Success Rate: 0.05\n",
      "Average Strength: 1.15\n",
      "\n",
      "--- Testing with Steps = 9 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [22:48<00:00, 68.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 9\n",
      "Success Rate: 0.05\n",
      "Average Strength: 1.5\n",
      "\n",
      "--- Testing with Steps = 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [24:52<00:00, 74.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 10\n",
      "Success Rate: 0.15\n",
      "Average Strength: 1.45\n",
      "\n",
      "====== Testing for Alpha = 0.4 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:51<00:00, 17.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.3\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:02<00:00, 24.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.7\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:13<00:00, 30.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.05\n",
      "Average Strength: 1.05\n",
      "\n",
      "--- Testing with Steps = 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:14<00:00, 36.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 4\n",
      "Success Rate: 0.1\n",
      "Average Strength: 1.55\n",
      "\n",
      "--- Testing with Steps = 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:20<00:00, 43.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5\n",
      "Success Rate: 0.05\n",
      "Average Strength: 1.3\n",
      "\n",
      "--- Testing with Steps = 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [16:31<00:00, 49.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 6\n",
      "Success Rate: 0.2\n",
      "Average Strength: 1.7\n",
      "\n",
      "--- Testing with Steps = 7 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [18:33<00:00, 55.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 7\n",
      "Success Rate: 0.25\n",
      "Average Strength: 1.7\n",
      "\n",
      "--- Testing with Steps = 8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:47<00:00, 62.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 8\n",
      "Success Rate: 0.3\n",
      "Average Strength: 2.1\n",
      "\n",
      "--- Testing with Steps = 9 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [22:47<00:00, 68.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 9\n",
      "Success Rate: 0.3\n",
      "Average Strength: 2.25\n",
      "\n",
      "--- Testing with Steps = 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [24:56<00:00, 74.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 10\n",
      "Success Rate: 0.45\n",
      "Average Strength: 2.25\n",
      "\n",
      "====== Testing for Alpha = 0.6 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:53<00:00, 17.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.5\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:03<00:00, 24.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.15\n",
      "Average Strength: 1.05\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:11<00:00, 30.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.15\n",
      "Average Strength: 1.45\n",
      "\n",
      "--- Testing with Steps = 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:17<00:00, 36.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 4\n",
      "Success Rate: 0.25\n",
      "Average Strength: 1.65\n",
      "\n",
      "--- Testing with Steps = 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:22<00:00, 43.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5\n",
      "Success Rate: 0.15\n",
      "Average Strength: 1.8\n",
      "\n",
      "--- Testing with Steps = 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [16:33<00:00, 49.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 6\n",
      "Success Rate: 0.4\n",
      "Average Strength: 2.4\n",
      "\n",
      "--- Testing with Steps = 7 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [18:39<00:00, 55.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 7\n",
      "Success Rate: 0.7\n",
      "Average Strength: 2.9\n",
      "\n",
      "--- Testing with Steps = 8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:42<00:00, 62.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 8\n",
      "Success Rate: 0.7\n",
      "Average Strength: 3.2\n",
      "\n",
      "--- Testing with Steps = 9 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [22:48<00:00, 68.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 9\n",
      "Success Rate: 0.75\n",
      "Average Strength: 3.15\n",
      "\n",
      "--- Testing with Steps = 10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [24:59<00:00, 74.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 10\n",
      "Success Rate: 1.0\n",
      "Average Strength: 3.75\n",
      "\n",
      "====== Testing for Alpha = 1.0 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:53<00:00, 17.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.05\n",
      "Average Strength: 0.85\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:01<00:00, 24.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.1\n",
      "Average Strength: 1.55\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:15<00:00, 30.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.3\n",
      "Average Strength: 2.2\n",
      "\n",
      "--- Testing with Steps = 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:14<00:00, 36.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 4\n",
      "Success Rate: 0.75\n",
      "Average Strength: 3.0\n",
      "\n",
      "--- Testing with Steps = 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:26<00:00, 43.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5\n",
      "Success Rate: 0.9\n",
      "Average Strength: 3.4\n",
      "\n",
      "--- Testing with Steps = 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [16:34<00:00, 49.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 6\n",
      "Success Rate: 0.9\n",
      "Average Strength: 3.9\n",
      "\n",
      "--- Testing with Steps = 7 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [18:35<00:00, 55.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 7\n",
      "Success Rate: 0.8\n",
      "Average Strength: 3.45\n",
      "\n",
      "--- Testing with Steps = 8 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [20:44<00:00, 62.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 8\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 1.0 due to zero success and strength.\n",
      "\n",
      "====== Testing for Alpha = 1.4 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:49<00:00, 17.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.1\n",
      "Average Strength: 1.35\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:05<00:00, 24.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.2\n",
      "Average Strength: 1.9\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:13<00:00, 30.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.8\n",
      "Average Strength: 3.25\n",
      "\n",
      "--- Testing with Steps = 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:15<00:00, 36.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 4\n",
      "Success Rate: 0.95\n",
      "Average Strength: 3.85\n",
      "\n",
      "--- Testing with Steps = 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:22<00:00, 43.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 5\n",
      "Success Rate: 0.2\n",
      "Average Strength: 0.9\n",
      "\n",
      "--- Testing with Steps = 6 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [16:38<00:00, 49.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 6\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 1.4 due to zero success and strength.\n",
      "\n",
      "====== Testing for Alpha = 1.8 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.05\n",
      "Average Strength: 1.25\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:07<00:00, 24.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.6\n",
      "Average Strength: 2.65\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:11<00:00, 30.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.95\n",
      "Average Strength: 3.95\n",
      "\n",
      "--- Testing with Steps = 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:16<00:00, 36.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 4\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 1.8 due to zero success and strength.\n",
      "\n",
      "====== Testing for Alpha = 2.2 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:52<00:00, 17.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.2\n",
      "Average Strength: 1.65\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:05<00:00, 24.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.75\n",
      "Average Strength: 3.15\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:14<00:00, 30.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.35\n",
      "Average Strength: 1.6\n",
      "\n",
      "--- Testing with Steps = 4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [12:19<00:00, 36.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 4\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 2.2 due to zero success and strength.\n",
      "\n",
      "====== Testing for Alpha = 2.6 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.1\n",
      "Average Strength: 1.75\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:03<00:00, 24.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 1.0\n",
      "Average Strength: 4.0\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:11<00:00, 30.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 2.6 due to zero success and strength.\n",
      "\n",
      "====== Testing for Alpha = 3.0 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:52<00:00, 17.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.4\n",
      "Average Strength: 2.3\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:04<00:00, 24.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 1.0\n",
      "Average Strength: 4.0\n",
      "\n",
      "--- Testing with Steps = 3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [10:10<00:00, 30.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 3\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 3.0 due to zero success and strength.\n",
      "\n",
      "====== Testing for Alpha = 3.4 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:53<00:00, 17.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.45\n",
      "Average Strength: 2.4\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:02<00:00, 24.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 3.4 due to zero success and strength.\n",
      "\n",
      "====== Testing for Alpha = 3.8 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:51<00:00, 17.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.65\n",
      "Average Strength: 2.95\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:05<00:00, 24.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 3.8 due to zero success and strength.\n",
      "\n",
      "====== Testing for Alpha = 4.2 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.75\n",
      "Average Strength: 3.1\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:03<00:00, 24.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 4.2 due to zero success and strength.\n",
      "\n",
      "====== Testing for Alpha = 4.6 ======\n",
      "\n",
      "--- Testing with Steps = 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1\n",
      "Success Rate: 0.8\n",
      "Average Strength: 3.35\n",
      "\n",
      "--- Testing with Steps = 2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [08:03<00:00, 24.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2\n",
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for alpha = 4.6 due to zero success and strength.\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import json\n",
    "import os\n",
    "from colorama import Fore\n",
    "import textwrap\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Define alphas and steps\n",
    "alphas_to_try = [0.2, 0.4, 0.6, 1.0, 1.4, 1.8, 2.2, 2.6, 3.0, 3.4, 3.8, 4.2, 4.6]\n",
    "steps_to_try = list(range(1, 11))\n",
    "N_INST_TEST = 20\n",
    "\n",
    "intervention_layers = list(range(model.cfg.n_layers))\n",
    "act_names = ['resid_pre', 'resid_mid', 'resid_post']\n",
    "\n",
    "# Get unsteered generations once\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    strawman_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Main loop: vary alpha, then steps\n",
    "for alpha in alphas_to_try:\n",
    "    print(f\"\\n====== Testing for Alpha = {alpha} ======\")\n",
    "    alpha_log = {\n",
    "        \"meta\": {\n",
    "            \"run_time\": datetime.now().isoformat(),\n",
    "            \"alpha\": alpha,\n",
    "            \"n_instances\": N_INST_TEST,\n",
    "            \"intervention_layers\": intervention_layers,\n",
    "            \"activation_points\": act_names\n",
    "        },\n",
    "        \"baseline_generations\": baseline_generations,\n",
    "        \"step_runs\": []\n",
    "    }\n",
    "\n",
    "    for steps in steps_to_try:\n",
    "        print(f\"\\n--- Testing with Steps = {steps} ---\")\n",
    "        \n",
    "        hook_fn = functools.partial(\n",
    "            actadd_debate_steering_hook,\n",
    "            target_styles=['empirical'],\n",
    "            avoid_styles=['strawman'],\n",
    "            alpha=alpha,\n",
    "            steps=steps\n",
    "        )\n",
    "\n",
    "        fwd_hooks = [\n",
    "            (utils.get_act_name(act_name, l), hook_fn)\n",
    "            for l in intervention_layers\n",
    "            for act_name in act_names\n",
    "        ]\n",
    "\n",
    "        # Run steered generation\n",
    "        steered_generations = get_generations(\n",
    "            model,\n",
    "            strawman_inst_test[:N_INST_TEST],\n",
    "            tokenize_instructions_fn,\n",
    "            fwd_hooks=fwd_hooks\n",
    "        )\n",
    "\n",
    "        # Evaluate results\n",
    "        result = judge.evaluate_batch(\n",
    "            baseline_generations,\n",
    "            steered_generations,\n",
    "            \"Empirical Grounding\",\n",
    "            \"Straw Man Reframing\"\n",
    "        )\n",
    "\n",
    "        print(f\"Steps: {steps}\")\n",
    "        print(f\"Success Rate: {result['success_rate']}\")\n",
    "        print(f\"Average Strength: {result['average_strength']}\")\n",
    "\n",
    "        # Store result for this step\n",
    "        alpha_log[\"step_runs\"].append({\n",
    "            \"steps\": steps,\n",
    "            \"success_rate\": result[\"success_rate\"],\n",
    "            \"average_strength\": result[\"average_strength\"],\n",
    "            \"steered_generations\": steered_generations\n",
    "        })\n",
    "\n",
    "        # Early stopping condition\n",
    "        if result[\"success_rate\"] == 0 and result[\"average_strength\"] == 0:\n",
    "            print(Fore.RED + f\"Stopping early for alpha = {alpha} due to zero success and strength.\")\n",
    "            break\n",
    "\n",
    "    # Save results for this alpha\n",
    "    safe_alpha = str(round(alpha, 2)).replace(\".\", \"_\")\n",
    "    output_path = f\"empirical_minus_strawman_steering_alpha_{safe_alpha}.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(alpha_log, f, indent=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's vary the combinations, fix the step size and look at steering success across combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Selected Pair: Target=burden, Avoid=reductio ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:47<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Running for Alpha = 5.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:36<00:00, 16.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 1.0\n",
      "Average Strength: 3.95\n",
      "\n",
      "== Running for Alpha = 5.4 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:41<00:00, 17.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.95\n",
      "Average Strength: 3.95\n",
      "\n",
      "== Running for Alpha = 5.8 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:40<00:00, 17.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.8\n",
      "Average Strength: 3.2\n",
      "\n",
      "== Running for Alpha = 6.2 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:41<00:00, 17.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.35\n",
      "Average Strength: 1.45\n",
      "\n",
      "== Running for Alpha = 6.6 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:40<00:00, 17.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.2\n",
      "Average Strength: 0.95\n",
      "\n",
      "== Running for Alpha = 7.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:41<00:00, 17.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.15\n",
      "Average Strength: 1.3\n",
      "\n",
      "### Selected Pair: Target=burden, Avoid=strawman ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:46<00:00,  5.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Running for Alpha = 5.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:49<00:00, 17.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.3\n",
      "Average Strength: 1.75\n",
      "\n",
      "== Running for Alpha = 5.4 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:49<00:00, 17.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.3\n",
      "Average Strength: 1.9\n",
      "\n",
      "== Running for Alpha = 5.8 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.3\n",
      "Average Strength: 1.75\n",
      "\n",
      "== Running for Alpha = 6.2 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:46<00:00, 17.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.45\n",
      "Average Strength: 1.8\n",
      "\n",
      "== Running for Alpha = 6.6 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.7\n",
      "Average Strength: 2.45\n",
      "\n",
      "== Running for Alpha = 7.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.75\n",
      "Average Strength: 2.95\n",
      "\n",
      "### Selected Pair: Target=empirical, Avoid=reductio ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:48<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Running for Alpha = 5.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:42<00:00, 17.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 1.0\n",
      "Average Strength: 3.9\n",
      "\n",
      "== Running for Alpha = 5.4 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:42<00:00, 17.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.75\n",
      "Average Strength: 3.4\n",
      "\n",
      "== Running for Alpha = 5.8 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:48<00:00, 17.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.25\n",
      "Average Strength: 1.3\n",
      "\n",
      "== Running for Alpha = 6.2 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:45<00:00, 17.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for target=empirical, avoid=reductio at alpha=6.2 due to zero success and strength.\n",
      "\n",
      "### Selected Pair: Target=precedent, Avoid=burden ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:48<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Running for Alpha = 5.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:53<00:00, 17.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 1.0\n",
      "Average Strength: 4.0\n",
      "\n",
      "== Running for Alpha = 5.4 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:48<00:00, 17.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.9\n",
      "Average Strength: 3.8\n",
      "\n",
      "== Running for Alpha = 5.8 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.75\n",
      "Average Strength: 3.2\n",
      "\n",
      "== Running for Alpha = 6.2 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:51<00:00, 17.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.15\n",
      "Average Strength: 0.9\n",
      "\n",
      "== Running for Alpha = 6.6 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:52<00:00, 17.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.05\n",
      "Average Strength: 0.2\n",
      "\n",
      "== Running for Alpha = 7.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:48<00:00, 17.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.0\n",
      "Average Strength: 0.0\n",
      "\u001b[31mStopping early for target=precedent, avoid=burden at alpha=7.0 due to zero success and strength.\n",
      "\n",
      "### Selected Pair: Target=reductio, Avoid=analogy ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:47<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Running for Alpha = 5.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:53<00:00, 17.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.2\n",
      "Average Strength: 1.35\n",
      "\n",
      "== Running for Alpha = 5.4 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:48<00:00, 17.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.3\n",
      "Average Strength: 1.65\n",
      "\n",
      "== Running for Alpha = 5.8 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:49<00:00, 17.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.3\n",
      "Average Strength: 1.65\n",
      "\n",
      "== Running for Alpha = 6.2 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:50<00:00, 17.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.2\n",
      "Average Strength: 1.55\n",
      "\n",
      "== Running for Alpha = 6.6 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:45<00:00, 17.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.25\n",
      "Average Strength: 1.65\n",
      "\n",
      "== Running for Alpha = 7.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:52<00:00, 17.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.4\n",
      "Average Strength: 1.95\n",
      "\n",
      "### Selected Pair: Target=reductio, Avoid=concession ###\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:47<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Running for Alpha = 5.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:56<00:00, 17.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.85\n",
      "Average Strength: 2.65\n",
      "\n",
      "== Running for Alpha = 5.4 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:52<00:00, 17.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.85\n",
      "Average Strength: 2.75\n",
      "\n",
      "== Running for Alpha = 5.8 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:57<00:00, 17.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.9\n",
      "Average Strength: 3.15\n",
      "\n",
      "== Running for Alpha = 6.2 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:52<00:00, 17.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.9\n",
      "Average Strength: 3.25\n",
      "\n",
      "== Running for Alpha = 6.6 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:54<00:00, 17.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.85\n",
      "Average Strength: 3.35\n",
      "\n",
      "== Running for Alpha = 7.0 ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:55<00:00, 17.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 0.45\n",
      "Average Strength: 1.8\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import json\n",
    "import random\n",
    "from colorama import Fore\n",
    "from datetime import datetime\n",
    "from itertools import permutations\n",
    "\n",
    "style_label_map = {\n",
    "    \"reductio\": \"Reductio ad Absurdum\",\n",
    "    \"precedent\": \"Appeal to Precedent\",\n",
    "    \"strawman\": \"Straw Man Reframing\",\n",
    "    \"burden\": \"Burden of Proof Shift\",\n",
    "    \"analogy\": \"Analogy Construction\",\n",
    "    \"concession\": \"Concession and Pivot\",\n",
    "    \"empirical\": \"Empirical Grounding\",\n",
    "    \"moral\": \"Moral Framing\",\n",
    "    \"refutation\": \"Refutation by Distinction\",\n",
    "    \"circular\": \"Circular Anticipation\"\n",
    "}\n",
    "\n",
    "# Settings\n",
    "#alphas_to_try = [0.02, 0.04, 0.06, 0.1, 0.14, 0.18, 0.22, 0.26, 0.3, 0.34, 0.38, 0.42, 0.46, 0.5, 0.54, 0.58 , 0.62] CAA alphas\n",
    "#alphas_to_try = [0.5, 1.0, 1.4, 1.8, 2.2, 2.6, 3.0, 3.4, 3.8, 4.2, 4.6, 5.2]\n",
    "alphas_to_try = [5.0, 5.4, 5.8, 6.2, 6.6, 7.0]\n",
    "N_INST_TEST = 20\n",
    "fixed_steps = 1\n",
    "#NUM_COMBINATIONS = 6\n",
    "\n",
    "intervention_layers = list(range(model.cfg.n_layers))\n",
    "act_names = ['resid_pre', 'resid_mid', 'resid_post']\n",
    "\n",
    "# Get all possible (target, avoid) ordered pairs where they differ\n",
    "#all_pairs = list(permutations(debate_style_datasets.keys(), 2))\n",
    "#selected_pairs = random.sample(all_pairs, NUM_COMBINATIONS)\n",
    "selected_pairs = selected_pairs = [('burden','reductio'), ('burden', 'strawman'), ('empirical', 'reductio'), ('precedent', 'burden'), ('reductio', 'analogy'), ('reductio', 'concession')]\n",
    "# Main loop over selected (target, avoid) combinations\n",
    "for target_style, avoid_style in selected_pairs:\n",
    "    print(f\"\\n### Selected Pair: Target={target_style}, Avoid={avoid_style} ###\")\n",
    "    \n",
    "    input_instructions = debate_style_datasets[avoid_style][:N_INST_TEST]\n",
    "\n",
    "    # Baseline generations\n",
    "    baseline_generations = get_generations(\n",
    "        model,\n",
    "        input_instructions,\n",
    "        tokenize_instructions_fn,\n",
    "        fwd_hooks=[]\n",
    "    )\n",
    "\n",
    "    log_data = {\n",
    "        \"meta\": {\n",
    "            \"run_time\": datetime.now().isoformat(),\n",
    "            \"steps\": fixed_steps,\n",
    "            \"target_style\": target_style,\n",
    "            \"avoid_style\": avoid_style,\n",
    "            \"n_instances\": N_INST_TEST,\n",
    "            \"intervention_layers\": intervention_layers,\n",
    "            \"activation_points\": act_names\n",
    "        },\n",
    "        \"input_instructions\": input_instructions,\n",
    "        \"baseline_generations\": baseline_generations,\n",
    "        \"step_runs\": []\n",
    "    }\n",
    "\n",
    "    for alpha in alphas_to_try:\n",
    "        print(f\"\\n== Running for Alpha = {alpha} ==\")\n",
    "\n",
    "        hook_fn = functools.partial(\n",
    "            actadd_debate_steering_hook,\n",
    "            target_styles=[target_style],\n",
    "            avoid_styles=[avoid_style],\n",
    "            alpha=alpha,\n",
    "            steps=fixed_steps\n",
    "        )\n",
    "\n",
    "        fwd_hooks = [\n",
    "            (utils.get_act_name(act_name, l), hook_fn)\n",
    "            for l in intervention_layers\n",
    "            for act_name in act_names\n",
    "        ]\n",
    "\n",
    "        steered_generations = get_generations(\n",
    "            model,\n",
    "            input_instructions,\n",
    "            tokenize_instructions_fn,\n",
    "            fwd_hooks=fwd_hooks\n",
    "        )\n",
    "\n",
    "        result = judge.evaluate_batch(\n",
    "            baseline_generations,\n",
    "            steered_generations,\n",
    "            style_label_map[target_style],\n",
    "            style_label_map[avoid_style]\n",
    "        )\n",
    "\n",
    "        print(f\"Success Rate: {result['success_rate']}\")\n",
    "        print(f\"Average Strength: {result['average_strength']}\")\n",
    "\n",
    "        log_data[\"step_runs\"].append({\n",
    "            \"alpha\": alpha,\n",
    "            \"success_rate\": result[\"success_rate\"],\n",
    "            \"average_strength\": result[\"average_strength\"],\n",
    "            \"steered_generations\": steered_generations\n",
    "        })\n",
    "        \n",
    "        # Early stopping condition\n",
    "        if result[\"success_rate\"] == 0 and result[\"average_strength\"] == 0:\n",
    "            print(Fore.RED + f\"Stopping early for target={target_style}, avoid={avoid_style} at alpha={alpha} due to zero success and strength.\")\n",
    "            break\n",
    "\n",
    "    # Save one file per (target, avoid) pair\n",
    "    output_path = f\"steering_{target_style}_vs_{avoid_style}_ctn.json\"\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(log_data, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's vary across layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reductio batch 0 to 16\n",
      "Processing reductio batch 16 to 32\n",
      "Processing reductio batch 32 to 48\n",
      "Processing reductio batch 48 to 64\n",
      "Processing reductio batch 64 to 80\n",
      "Processing reductio batch 80 to 96\n",
      "Processing reductio batch 96 to 112\n",
      "Processing reductio batch 112 to 128\n",
      "Processing reductio batch 128 to 144\n",
      "Processing reductio batch 144 to 160\n",
      "Processing reductio batch 160 to 176\n",
      "Processing reductio batch 176 to 192\n",
      "Processing reductio batch 192 to 208\n",
      "Processing reductio batch 208 to 224\n",
      "Processing reductio batch 224 to 240\n",
      "Processing reductio batch 240 to 256\n",
      "Processing reductio batch 256 to 272\n",
      "Processing reductio batch 272 to 288\n",
      "Processing reductio batch 288 to 304\n",
      "Processing reductio batch 304 to 320\n",
      "Processing reductio batch 320 to 336\n",
      "Processing reductio batch 336 to 352\n",
      "Processing reductio batch 352 to 368\n",
      "Processing reductio batch 368 to 384\n",
      "Processing reductio batch 384 to 400\n",
      "Processing reductio batch 400 to 416\n",
      "Processing reductio batch 416 to 432\n",
      "Processing reductio batch 432 to 448\n",
      "Processing reductio batch 448 to 464\n",
      "Processing reductio batch 464 to 480\n",
      "Processing reductio batch 480 to 496\n",
      "Processing reductio batch 496 to 512\n",
      "Processing reductio batch 512 to 528\n",
      "Processing reductio batch 528 to 544\n",
      "Processing reductio batch 544 to 560\n",
      "Processing reductio batch 560 to 576\n",
      "Processing reductio batch 576 to 592\n",
      "Processing reductio batch 592 to 600\n",
      "Final shape of reductio activations: (600, 3072)\n",
      "Processing precedent batch 0 to 16\n",
      "Processing precedent batch 16 to 32\n",
      "Processing precedent batch 32 to 48\n",
      "Processing precedent batch 48 to 64\n",
      "Processing precedent batch 64 to 80\n",
      "Processing precedent batch 80 to 96\n",
      "Processing precedent batch 96 to 112\n",
      "Processing precedent batch 112 to 128\n",
      "Processing precedent batch 128 to 144\n",
      "Processing precedent batch 144 to 160\n",
      "Processing precedent batch 160 to 176\n",
      "Processing precedent batch 176 to 192\n",
      "Processing precedent batch 192 to 208\n",
      "Processing precedent batch 208 to 224\n",
      "Processing precedent batch 224 to 240\n",
      "Processing precedent batch 240 to 256\n",
      "Processing precedent batch 256 to 272\n",
      "Processing precedent batch 272 to 288\n",
      "Processing precedent batch 288 to 304\n",
      "Processing precedent batch 304 to 320\n",
      "Processing precedent batch 320 to 336\n",
      "Processing precedent batch 336 to 352\n",
      "Processing precedent batch 352 to 368\n",
      "Processing precedent batch 368 to 384\n",
      "Processing precedent batch 384 to 400\n",
      "Processing precedent batch 400 to 416\n",
      "Processing precedent batch 416 to 432\n",
      "Processing precedent batch 432 to 448\n",
      "Processing precedent batch 448 to 464\n",
      "Processing precedent batch 464 to 480\n",
      "Processing precedent batch 480 to 496\n",
      "Processing precedent batch 496 to 512\n",
      "Processing precedent batch 512 to 528\n",
      "Processing precedent batch 528 to 544\n",
      "Processing precedent batch 544 to 560\n",
      "Processing precedent batch 560 to 576\n",
      "Processing precedent batch 576 to 592\n",
      "Processing precedent batch 592 to 600\n",
      "Final shape of precedent activations: (600, 3072)\n",
      "Processing strawman batch 0 to 16\n",
      "Processing strawman batch 16 to 32\n",
      "Processing strawman batch 32 to 48\n",
      "Processing strawman batch 48 to 64\n",
      "Processing strawman batch 64 to 80\n",
      "Processing strawman batch 80 to 96\n",
      "Processing strawman batch 96 to 112\n",
      "Processing strawman batch 112 to 128\n",
      "Processing strawman batch 128 to 144\n",
      "Processing strawman batch 144 to 160\n",
      "Processing strawman batch 160 to 176\n",
      "Processing strawman batch 176 to 192\n",
      "Processing strawman batch 192 to 208\n",
      "Processing strawman batch 208 to 224\n",
      "Processing strawman batch 224 to 240\n",
      "Processing strawman batch 240 to 256\n",
      "Processing strawman batch 256 to 272\n",
      "Processing strawman batch 272 to 288\n",
      "Processing strawman batch 288 to 304\n",
      "Processing strawman batch 304 to 320\n",
      "Processing strawman batch 320 to 336\n",
      "Processing strawman batch 336 to 352\n",
      "Processing strawman batch 352 to 368\n",
      "Processing strawman batch 368 to 384\n",
      "Processing strawman batch 384 to 400\n",
      "Processing strawman batch 400 to 416\n",
      "Processing strawman batch 416 to 432\n",
      "Processing strawman batch 432 to 448\n",
      "Processing strawman batch 448 to 464\n",
      "Processing strawman batch 464 to 480\n",
      "Processing strawman batch 480 to 496\n",
      "Processing strawman batch 496 to 512\n",
      "Processing strawman batch 512 to 528\n",
      "Processing strawman batch 528 to 544\n",
      "Processing strawman batch 544 to 560\n",
      "Processing strawman batch 560 to 576\n",
      "Processing strawman batch 576 to 592\n",
      "Processing strawman batch 592 to 600\n",
      "Final shape of strawman activations: (600, 3072)\n",
      "Processing burden batch 0 to 16\n",
      "Processing burden batch 16 to 32\n",
      "Processing burden batch 32 to 48\n",
      "Processing burden batch 48 to 64\n",
      "Processing burden batch 64 to 80\n",
      "Processing burden batch 80 to 96\n",
      "Processing burden batch 96 to 112\n",
      "Processing burden batch 112 to 128\n",
      "Processing burden batch 128 to 144\n",
      "Processing burden batch 144 to 160\n",
      "Processing burden batch 160 to 176\n",
      "Processing burden batch 176 to 192\n",
      "Processing burden batch 192 to 208\n",
      "Processing burden batch 208 to 224\n",
      "Processing burden batch 224 to 240\n",
      "Processing burden batch 240 to 256\n",
      "Processing burden batch 256 to 272\n",
      "Processing burden batch 272 to 288\n",
      "Processing burden batch 288 to 304\n",
      "Processing burden batch 304 to 320\n",
      "Processing burden batch 320 to 336\n",
      "Processing burden batch 336 to 352\n",
      "Processing burden batch 352 to 368\n",
      "Processing burden batch 368 to 384\n",
      "Processing burden batch 384 to 400\n",
      "Processing burden batch 400 to 416\n",
      "Processing burden batch 416 to 432\n",
      "Processing burden batch 432 to 448\n",
      "Processing burden batch 448 to 464\n",
      "Processing burden batch 464 to 480\n",
      "Processing burden batch 480 to 496\n",
      "Processing burden batch 496 to 512\n",
      "Processing burden batch 512 to 528\n",
      "Processing burden batch 528 to 544\n",
      "Processing burden batch 544 to 560\n",
      "Processing burden batch 560 to 576\n",
      "Processing burden batch 576 to 592\n",
      "Processing burden batch 592 to 600\n",
      "Final shape of burden activations: (600, 3072)\n",
      "Processing analogy batch 0 to 16\n",
      "Processing analogy batch 16 to 32\n",
      "Processing analogy batch 32 to 48\n",
      "Processing analogy batch 48 to 64\n",
      "Processing analogy batch 64 to 80\n",
      "Processing analogy batch 80 to 96\n",
      "Processing analogy batch 96 to 112\n",
      "Processing analogy batch 112 to 128\n",
      "Processing analogy batch 128 to 144\n",
      "Processing analogy batch 144 to 160\n",
      "Processing analogy batch 160 to 176\n",
      "Processing analogy batch 176 to 192\n",
      "Processing analogy batch 192 to 208\n",
      "Processing analogy batch 208 to 224\n",
      "Processing analogy batch 224 to 240\n",
      "Processing analogy batch 240 to 256\n",
      "Processing analogy batch 256 to 272\n",
      "Processing analogy batch 272 to 288\n",
      "Processing analogy batch 288 to 304\n",
      "Processing analogy batch 304 to 320\n",
      "Processing analogy batch 320 to 336\n",
      "Processing analogy batch 336 to 352\n",
      "Processing analogy batch 352 to 368\n",
      "Processing analogy batch 368 to 384\n",
      "Processing analogy batch 384 to 400\n",
      "Processing analogy batch 400 to 416\n",
      "Processing analogy batch 416 to 432\n",
      "Processing analogy batch 432 to 448\n",
      "Processing analogy batch 448 to 464\n",
      "Processing analogy batch 464 to 480\n",
      "Processing analogy batch 480 to 496\n",
      "Processing analogy batch 496 to 512\n",
      "Processing analogy batch 512 to 528\n",
      "Processing analogy batch 528 to 544\n",
      "Processing analogy batch 544 to 560\n",
      "Processing analogy batch 560 to 576\n",
      "Processing analogy batch 576 to 592\n",
      "Processing analogy batch 592 to 600\n",
      "Final shape of analogy activations: (600, 3072)\n",
      "Processing concession batch 0 to 16\n",
      "Processing concession batch 16 to 32\n",
      "Processing concession batch 32 to 48\n",
      "Processing concession batch 48 to 64\n",
      "Processing concession batch 64 to 80\n",
      "Processing concession batch 80 to 96\n",
      "Processing concession batch 96 to 112\n",
      "Processing concession batch 112 to 128\n",
      "Processing concession batch 128 to 144\n",
      "Processing concession batch 144 to 160\n",
      "Processing concession batch 160 to 176\n",
      "Processing concession batch 176 to 192\n",
      "Processing concession batch 192 to 208\n",
      "Processing concession batch 208 to 224\n",
      "Processing concession batch 224 to 240\n",
      "Processing concession batch 240 to 256\n",
      "Processing concession batch 256 to 272\n",
      "Processing concession batch 272 to 288\n",
      "Processing concession batch 288 to 304\n",
      "Processing concession batch 304 to 320\n",
      "Processing concession batch 320 to 336\n",
      "Processing concession batch 336 to 352\n",
      "Processing concession batch 352 to 368\n",
      "Processing concession batch 368 to 384\n",
      "Processing concession batch 384 to 400\n",
      "Processing concession batch 400 to 416\n",
      "Processing concession batch 416 to 432\n",
      "Processing concession batch 432 to 448\n",
      "Processing concession batch 448 to 464\n",
      "Processing concession batch 464 to 480\n",
      "Processing concession batch 480 to 496\n",
      "Processing concession batch 496 to 512\n",
      "Processing concession batch 512 to 528\n",
      "Processing concession batch 528 to 544\n",
      "Processing concession batch 544 to 560\n",
      "Processing concession batch 560 to 576\n",
      "Processing concession batch 576 to 592\n",
      "Processing concession batch 592 to 600\n",
      "Final shape of concession activations: (600, 3072)\n",
      "Processing empirical batch 0 to 16\n",
      "Processing empirical batch 16 to 32\n",
      "Processing empirical batch 32 to 48\n",
      "Processing empirical batch 48 to 64\n",
      "Processing empirical batch 64 to 80\n",
      "Processing empirical batch 80 to 96\n",
      "Processing empirical batch 96 to 112\n",
      "Processing empirical batch 112 to 128\n",
      "Processing empirical batch 128 to 144\n",
      "Processing empirical batch 144 to 160\n",
      "Processing empirical batch 160 to 176\n",
      "Processing empirical batch 176 to 192\n",
      "Processing empirical batch 192 to 208\n",
      "Processing empirical batch 208 to 224\n",
      "Processing empirical batch 224 to 240\n",
      "Processing empirical batch 240 to 256\n",
      "Processing empirical batch 256 to 272\n",
      "Processing empirical batch 272 to 288\n",
      "Processing empirical batch 288 to 304\n",
      "Processing empirical batch 304 to 320\n",
      "Processing empirical batch 320 to 336\n",
      "Processing empirical batch 336 to 352\n",
      "Processing empirical batch 352 to 368\n",
      "Processing empirical batch 368 to 384\n",
      "Processing empirical batch 384 to 400\n",
      "Processing empirical batch 400 to 416\n",
      "Processing empirical batch 416 to 432\n",
      "Processing empirical batch 432 to 448\n",
      "Processing empirical batch 448 to 464\n",
      "Processing empirical batch 464 to 480\n",
      "Processing empirical batch 480 to 496\n",
      "Processing empirical batch 496 to 512\n",
      "Processing empirical batch 512 to 528\n",
      "Processing empirical batch 528 to 544\n",
      "Processing empirical batch 544 to 560\n",
      "Processing empirical batch 560 to 576\n",
      "Processing empirical batch 576 to 592\n",
      "Processing empirical batch 592 to 600\n",
      "Final shape of empirical activations: (600, 3072)\n",
      "Processing moral batch 0 to 16\n",
      "Processing moral batch 16 to 32\n",
      "Processing moral batch 32 to 48\n",
      "Processing moral batch 48 to 64\n",
      "Processing moral batch 64 to 80\n",
      "Processing moral batch 80 to 96\n",
      "Processing moral batch 96 to 112\n",
      "Processing moral batch 112 to 128\n",
      "Processing moral batch 128 to 144\n",
      "Processing moral batch 144 to 160\n",
      "Processing moral batch 160 to 176\n",
      "Processing moral batch 176 to 192\n",
      "Processing moral batch 192 to 208\n",
      "Processing moral batch 208 to 224\n",
      "Processing moral batch 224 to 240\n",
      "Processing moral batch 240 to 256\n",
      "Processing moral batch 256 to 272\n",
      "Processing moral batch 272 to 288\n",
      "Processing moral batch 288 to 304\n",
      "Processing moral batch 304 to 320\n",
      "Processing moral batch 320 to 336\n",
      "Processing moral batch 336 to 352\n",
      "Processing moral batch 352 to 368\n",
      "Processing moral batch 368 to 384\n",
      "Processing moral batch 384 to 400\n",
      "Processing moral batch 400 to 416\n",
      "Processing moral batch 416 to 432\n",
      "Processing moral batch 432 to 448\n",
      "Processing moral batch 448 to 464\n",
      "Processing moral batch 464 to 480\n",
      "Processing moral batch 480 to 496\n",
      "Processing moral batch 496 to 512\n",
      "Processing moral batch 512 to 528\n",
      "Processing moral batch 528 to 544\n",
      "Processing moral batch 544 to 560\n",
      "Processing moral batch 560 to 576\n",
      "Processing moral batch 576 to 592\n",
      "Processing moral batch 592 to 600\n",
      "Final shape of moral activations: (600, 3072)\n",
      "Processing refutation batch 0 to 16\n",
      "Processing refutation batch 16 to 32\n",
      "Processing refutation batch 32 to 48\n",
      "Processing refutation batch 48 to 64\n",
      "Processing refutation batch 64 to 80\n",
      "Processing refutation batch 80 to 96\n",
      "Processing refutation batch 96 to 112\n",
      "Processing refutation batch 112 to 128\n",
      "Processing refutation batch 128 to 144\n",
      "Processing refutation batch 144 to 160\n",
      "Processing refutation batch 160 to 176\n",
      "Processing refutation batch 176 to 192\n",
      "Processing refutation batch 192 to 208\n",
      "Processing refutation batch 208 to 224\n",
      "Processing refutation batch 224 to 240\n",
      "Processing refutation batch 240 to 256\n",
      "Processing refutation batch 256 to 272\n",
      "Processing refutation batch 272 to 288\n",
      "Processing refutation batch 288 to 304\n",
      "Processing refutation batch 304 to 320\n",
      "Processing refutation batch 320 to 336\n",
      "Processing refutation batch 336 to 352\n",
      "Processing refutation batch 352 to 368\n",
      "Processing refutation batch 368 to 384\n",
      "Processing refutation batch 384 to 400\n",
      "Processing refutation batch 400 to 416\n",
      "Processing refutation batch 416 to 432\n",
      "Processing refutation batch 432 to 448\n",
      "Processing refutation batch 448 to 464\n",
      "Processing refutation batch 464 to 480\n",
      "Processing refutation batch 480 to 496\n",
      "Processing refutation batch 496 to 512\n",
      "Processing refutation batch 512 to 528\n",
      "Processing refutation batch 528 to 544\n",
      "Processing refutation batch 544 to 560\n",
      "Processing refutation batch 560 to 576\n",
      "Processing refutation batch 576 to 592\n",
      "Processing refutation batch 592 to 600\n",
      "Final shape of refutation activations: (600, 3072)\n",
      "Processing circular batch 0 to 16\n",
      "Processing circular batch 16 to 32\n",
      "Processing circular batch 32 to 48\n",
      "Processing circular batch 48 to 64\n",
      "Processing circular batch 64 to 80\n",
      "Processing circular batch 80 to 96\n",
      "Processing circular batch 96 to 112\n",
      "Processing circular batch 112 to 128\n",
      "Processing circular batch 128 to 144\n",
      "Processing circular batch 144 to 160\n",
      "Processing circular batch 160 to 176\n",
      "Processing circular batch 176 to 192\n",
      "Processing circular batch 192 to 208\n",
      "Processing circular batch 208 to 224\n",
      "Processing circular batch 224 to 240\n",
      "Processing circular batch 240 to 256\n",
      "Processing circular batch 256 to 272\n",
      "Processing circular batch 272 to 288\n",
      "Processing circular batch 288 to 304\n",
      "Processing circular batch 304 to 320\n",
      "Processing circular batch 320 to 336\n",
      "Processing circular batch 336 to 352\n",
      "Processing circular batch 352 to 368\n",
      "Processing circular batch 368 to 384\n",
      "Processing circular batch 384 to 400\n",
      "Processing circular batch 400 to 416\n",
      "Processing circular batch 416 to 432\n",
      "Processing circular batch 432 to 448\n",
      "Processing circular batch 448 to 464\n",
      "Processing circular batch 464 to 480\n",
      "Processing circular batch 480 to 496\n",
      "Processing circular batch 496 to 512\n",
      "Processing circular batch 512 to 528\n",
      "Processing circular batch 528 to 544\n",
      "Processing circular batch 544 to 560\n",
      "Processing circular batch 560 to 576\n",
      "Processing circular batch 576 to 592\n",
      "Processing circular batch 592 to 600\n",
      "Final shape of circular activations: (600, 3072)\n",
      "Epoch 0: loss = 0.6709, accuracy = 82.13%\n",
      "Epoch 1: loss = 0.0059, accuracy = 100.00%\n",
      "Epoch 2: loss = 0.0018, accuracy = 100.00%\n",
      "Epoch 3: loss = 0.0009, accuracy = 100.00%\n",
      "Epoch 4: loss = 0.0005, accuracy = 100.00%\n",
      "Epoch 5: loss = 0.0004, accuracy = 100.00%\n",
      "Epoch 6: loss = 0.0002, accuracy = 100.00%\n",
      "Epoch 7: loss = 0.0002, accuracy = 100.00%\n",
      "Epoch 8: loss = 0.0001, accuracy = 100.00%\n",
      "Epoch 9: loss = 0.0001, accuracy = 100.00%\n",
      "Epoch 10: loss = 0.0001, accuracy = 100.00%\n",
      "Epoch 11: loss = 0.0001, accuracy = 100.00%\n",
      "Epoch 12: loss = 0.0001, accuracy = 100.00%\n",
      "Epoch 13: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 14: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 15: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 16: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 17: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 18: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 19: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 20: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 21: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 22: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 23: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 24: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 25: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 26: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 27: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 28: loss = 0.0000, accuracy = 100.00%\n",
      "Epoch 29: loss = 0.0000, accuracy = 100.00%\n",
      "Accuracy for reductio: 100.00%\n",
      "Accuracy for precedent: 100.00%\n",
      "Accuracy for strawman: 100.00%\n",
      "Accuracy for burden: 100.00%\n",
      "Accuracy for analogy: 100.00%\n",
      "Accuracy for concession: 100.00%\n",
      "Accuracy for empirical: 100.00%\n",
      "Accuracy for moral: 100.00%\n",
      "Accuracy for refutation: 100.00%\n",
      "Accuracy for circular: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:25<00:00, 17.04s/it]\n",
      "100%|██████████| 5/5 [00:27<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION 0: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To argue against the necessity of an ethical framework governing human enhancement technologies,\n",
      "\tlet's consider the extreme implications of such a framework.\\n\\nIf we assume that an ethical\n",
      "\tframework is essential for regulating human enhancement technologies, then it must also govern the\n",
      "\tdevelopment of all technologies that could potentially impact human life, including medicine,\n",
      "\tagriculture, and energy\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t' days days days days days days days days days days days days days days days days days days days\n",
      "\tdays days days days days days days days days days days days days days days days Days Days Days Days\n",
      "\tDays Days Days Days Days Days                   '\n",
      "\u001b[39m\n",
      "INSTRUCTION 1: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nIs it ethical to judge historical figures by modern moral standards?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To address the question of whether it's ethical to judge historical figures by modern moral\n",
      "\tstandards, let's consider the opposing viewpoint and extend it to its logical extreme.\\n\\nAssuming\n",
      "\tit's ethical to judge historical figures by modern moral standards, we can infer that:\\n\\n1.\n",
      "\t**Historical figures are bound by the same moral principles\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t' days days days days days days days days days days days days days days days days days days days\n",
      "\tdays days days days days days days days days days days days days days days days days days Days Days\n",
      "\tDays Days Days Days Days Days Days Days                 '\n",
      "\u001b[39m\n",
      "INSTRUCTION 2: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To address this question, let's consider the opposing viewpoint that the benefits of telemedicine\n",
      "\tdo not outweigh the potential risks to patient care. If we follow this logic, then we must examine\n",
      "\tthe extreme consequences of assuming that telemedicine is inherently detrimental to patient\n",
      "\tcare.\\n\\nAssuming that telemedicine is the primary cause of all patient\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t' days days days days days days days days days days days days days days days days days days days\n",
      "\tdays days days days days days days days days days days days days days days days days Days Days Days\n",
      "\tDays Days Days Days Days Days                   '\n",
      "\u001b[39m\n",
      "INSTRUCTION 3: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nShould children be taught coding and digital literacy as a core part of their education?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'I argue that children should not be taught coding and digital literacy as a core part of their\n",
      "\teducation. If we follow this logic, then we would have to assume that every child is destined to\n",
      "\tbecome a professional programmer or digital expert, and that their education should be tailored to\n",
      "\tprepare them for this specific career path. This would'\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t' days days days days days days days days days days days days days days days days days days days\n",
      "\tdays days days days days days days days days days days days days days Days Days Days Days Days Days\n",
      "\tDays Days Days Days                     '\n",
      "\u001b[39m\n",
      "INSTRUCTION 4: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nIs it feasible for all countries to achieve net-zero emissions by 2050?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To address this question, let's consider the opposing viewpoint and extend it to its logical\n",
      "\textreme.\\n\\nAssuming it is feasible for all countries to achieve net-zero emissions by 2050, we can\n",
      "\tmake some conditional statements:\\n\\nIf all countries can achieve net-zero emissions by 2050,\n",
      "\tthen:\\n\\n1. **Global energy\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t' days days days days days days days days days days days days days days days days days days days\n",
      "\tdays days days days days days days days days days days days days days Days Days Days Days Days Days\n",
      "\tDays Days Days Days                     '\n",
      "\u001b[39m\n",
      "{'success_rate': 0.0, 'average_strength': 0.0}\n"
     ]
    }
   ],
   "source": [
    "debate_style_datasets = {\n",
    "    'reductio': reductio_inst_train,\n",
    "    'precedent': precedent_inst_train,\n",
    "    'strawman': strawman_inst_train,\n",
    "    'burden': burden_inst_train,\n",
    "    'analogy': analogy_inst_train,\n",
    "    'concession': concession_inst_train,\n",
    "    'empirical': empirical_inst_train,\n",
    "    'moral': moral_inst_train,\n",
    "    'refutation': refutation_inst_train,\n",
    "    'circular': circular_inst_train\n",
    "}\n",
    "\n",
    "\n",
    "layer = 7\n",
    "layer_activations = {}\n",
    "for style_name, style_data in debate_style_datasets.items():\n",
    "    layer_activations[style_name] = batch_process_activations(\n",
    "        style_data[:600], \n",
    "        style_name,\n",
    "        layer = 7\n",
    "    )\n",
    "\n",
    "reductio_acts = layer_activations['reductio']  # shape [N, D]\n",
    "precedent_acts = layer_activations['precedent']  # shape [M, D]\n",
    "strawman_acts = layer_activations['strawman']  # shape [O, D]\n",
    "burden_acts = layer_activations['burden']  # shape [P, D]\n",
    "analogy_acts = layer_activations['analogy']  # shape [Q, D]\n",
    "concession_acts = layer_activations['concession']  # shape [R, D]\n",
    "empirical_acts = layer_activations['empirical']  # shape [S, D]\n",
    "moral_acts = layer_activations['moral']  # shape [T, D]\n",
    "refutation_acts = layer_activations['refutation']  # shape [U, D]\n",
    "circular_acts = layer_activations['circular']  # shape [V, D]\n",
    "\n",
    "# Instantiate and fit the multi-debate-style steering classifier\n",
    "layer_debate_steer = MultiToneActivationSteering(input_dim=reductio_acts.shape[1], num_classes=10)\n",
    "layer_debate_steer.fit({\n",
    "    'reductio': reductio_acts,\n",
    "    'precedent': precedent_acts,\n",
    "    'strawman': strawman_acts,\n",
    "    'burden': burden_acts,\n",
    "    'analogy': analogy_acts,\n",
    "    'concession': concession_acts,\n",
    "    'empirical': empirical_acts,\n",
    "    'moral': moral_acts,\n",
    "    'refutation': refutation_acts,\n",
    "    'circular': circular_acts\n",
    "}, epochs=30, batch_size=32)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "def actadd_hook(activation, hook,\n",
    "                      target_styles=['empirical', 'refutation'], \n",
    "                      avoid_styles=['strawman'], \n",
    "                      alpha=1,\n",
    "                      steps=1,\n",
    "                      decay_rate=0.95):\n",
    "    \"\"\"\n",
    "    Steer activations toward target debate styles and away from avoid styles.\n",
    "    \"\"\"\n",
    "    shape = activation.shape\n",
    "    # Flatten to [N, d_act]\n",
    "    act_2d = activation.view(-1, shape[-1]).detach().cpu().numpy()\n",
    "    \n",
    "    # Steer in debate style space\n",
    "    steered_2d = layer_debate_steer.steer_toward_tones(\n",
    "        act_2d, \n",
    "        target_tones=target_styles,\n",
    "        avoid_tones=avoid_styles,\n",
    "        alpha=alpha,\n",
    "        steps=steps,\n",
    "        decay_rate=decay_rate\n",
    "    )\n",
    "\n",
    "    # Convert back to torch and original shape\n",
    "    steered_torch = torch.from_numpy(steered_2d).to(activation.device)\n",
    "    return steered_torch.view(*shape)\n",
    "\n",
    "N_INST_TEST = 5\n",
    "intervention_layers = list(range(model.cfg.n_layers))  # e.g. all layers\n",
    "#intervention_layers = [14]  # or specific layers only\n",
    "\n",
    "style_label_map = {\n",
    "    \"reductio\": \"Reductio ad Absurdum\",\n",
    "    \"precedent\": \"Appeal to Precedent\",\n",
    "    \"strawman\": \"Straw Man Reframing\",\n",
    "    \"burden\": \"Burden of Proof Shift\",\n",
    "    \"analogy\": \"Analogy Construction\",\n",
    "    \"concession\": \"Concession and Pivot\",\n",
    "    \"empirical\": \"Empirical Grounding\",\n",
    "    \"moral\": \"Moral Framing\",\n",
    "    \"refutation\": \"Refutation by Distinction\",\n",
    "    \"circular\": \"Circular Anticipation\"\n",
    "}\n",
    "\n",
    "# Create a partial function with our parameters\n",
    "hook_fn = functools.partial(actadd_hook,\n",
    "                           target_styles=['burden'],\n",
    "                           avoid_styles=['reductio'],\n",
    "                           alpha=5,\n",
    "                           steps=1)\n",
    "\n",
    "# Attach hooks to multiple points\n",
    "fwd_hooks = [\n",
    "    (utils.get_act_name(act_name, l), hook_fn)\n",
    "    for l in intervention_layers\n",
    "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
    "]\n",
    "\n",
    "# Run model with hooks for debate style steering\n",
    "style_steered_generations = get_generations(\n",
    "    model,\n",
    "    reductio_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=fwd_hooks\n",
    ")\n",
    "\n",
    "# Baseline, no intervention\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    reductio_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Compare outputs\n",
    "for i in range(N_INST_TEST):\n",
    "    print(f\"INSTRUCTION {i}: {repr(strawman_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
    "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RED + f\"DEBATE STYLE STEERED COMPLETION:\")\n",
    "    print(textwrap.fill(repr(style_steered_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RESET)\n",
    "\n",
    "\n",
    "result = judge.evaluate_batch(\n",
    "    baseline_generations,  # Your list of baseline texts\n",
    "    style_steered_generations,  # Your list of steered texts\n",
    "    style_label_map[\"burden\"],  # Target style\n",
    "    style_label_map[\"reductio\"]   # Style to avoid\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:27<00:00, 17.58s/it]\n",
      "100%|██████████| 5/5 [00:27<00:00,  5.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTION 0: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nShould there be an ethical framework governing the development of human enhancement technologies?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To argue against the necessity of an ethical framework governing human enhancement technologies,\n",
      "\tlet's consider the extreme implications of such a framework.\\n\\nIf we assume that an ethical\n",
      "\tframework is essential for regulating human enhancement technologies, then it must also govern the\n",
      "\tdevelopment of all technologies that could potentially impact human life, including medicine,\n",
      "\tagriculture, and energy\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t\"RESPONDING WITH CAUTION. I'd like to clarify that the development of human enhancement\n",
      "\ttechnologies is a complex issue that requires careful consideration of multiple factors, including\n",
      "\tethics, safety, and societal implications. While some argue that a framework is necessary to ensure\n",
      "\tresponsible innovation, others contend that it could stifle innovation and hinder progress\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 1: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nIs it ethical to judge historical figures by modern moral standards?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To address the question of whether it's ethical to judge historical figures by modern moral\n",
      "\tstandards, let's consider the opposing viewpoint and extend it to its logical extreme.\\n\\nAssuming\n",
      "\tit's ethical to judge historical figures by modern moral standards, we can infer that:\\n\\n1.\n",
      "\t**Historical figures are bound by the same moral principles\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t\"RESPONDING WITH EMPATHY AND UNDERSTANDING. Acknowledging the complexity of historical context,\n",
      "\tit's essential to consider the nuances of time and place when evaluating the actions and decisions\n",
      "\tof historical figures. While it's true that modern moral standards can differ significantly from\n",
      "\tthose of the past, it's not necessarily a\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 2: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nDo the benefits of telemedicine outweigh the potential risks to patient care?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To address this question, let's consider the opposing viewpoint that the benefits of telemedicine\n",
      "\tdo not outweigh the potential risks to patient care. If we follow this logic, then we must examine\n",
      "\tthe extreme consequences of assuming that telemedicine is inherently detrimental to patient\n",
      "\tcare.\\n\\nAssuming that telemedicine is the primary cause of all patient\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'RESPONDING WITH CAREFUL CONSIDERATION. While telemedicine has been touted as a game-changer in\n",
      "\thealthcare, its benefits must be weighed against the potential risks to patient care. If we\n",
      "\tconsider the benefits of telemedicine, such as increased accessibility and reduced healthcare\n",
      "\tcosts, we must also examine the potential risks,'\n",
      "\u001b[39m\n",
      "INSTRUCTION 3: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nShould children be taught coding and digital literacy as a core part of their education?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t'I argue that children should not be taught coding and digital literacy as a core part of their\n",
      "\teducation. If we follow this logic, then we would have to assume that every child is destined to\n",
      "\tbecome a professional programmer or digital expert, and that their education should be tailored to\n",
      "\tprepare them for this specific career path. This would'\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t\"RESPONDING WITH A LOGICAL APPROACH\\n\\nIf we consider the importance of digital literacy in today's\n",
      "\ttechnology-driven world, where information is increasingly a valuable resource, it is reasonable to\n",
      "\targue that children should be taught coding and digital literacy as a core part of their education.\n",
      "\tThis approach would not only equip them with essential\"\n",
      "\u001b[39m\n",
      "INSTRUCTION 4: \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\nIs it feasible for all countries to achieve net-zero emissions by 2050?\"\n",
      "\u001b[32mBASELINE COMPLETION:\n",
      "\t\"To address this question, let's consider the opposing viewpoint and extend it to its logical\n",
      "\textreme.\\n\\nAssuming it is feasible for all countries to achieve net-zero emissions by 2050, we can\n",
      "\tmake some conditional statements:\\n\\nIf all countries can achieve net-zero emissions by 2050,\n",
      "\tthen:\\n\\n1. **Global energy\"\n",
      "\u001b[31mDEBATE STYLE STEERED COMPLETION:\n",
      "\t'<|title|>Net-Zero Emissions by 2050: A Challenging Goal for Global Cooperation and Technological\n",
      "\tAdvancement\\n\\nWhile the goal of achieving net-zero emissions by 2050 is ambitious and desirable,\n",
      "\tit is a challenging target for several reasons. The path to net-zero emissions requires significant\n",
      "\tinvestments'\n",
      "\u001b[39m\n",
      "{'success_rate': 1.0, 'average_strength': 4.0}\n"
     ]
    }
   ],
   "source": [
    "N_INST_TEST = 5\n",
    "intervention_layers = list(range(model.cfg.n_layers))  # e.g. all layers\n",
    "#intervention_layers = [14]  # or specific layers only\n",
    "\n",
    "style_label_map = {\n",
    "    \"reductio\": \"Reductio ad Absurdum\",\n",
    "    \"precedent\": \"Appeal to Precedent\",\n",
    "    \"strawman\": \"Straw Man Reframing\",\n",
    "    \"burden\": \"Burden of Proof Shift\",\n",
    "    \"analogy\": \"Analogy Construction\",\n",
    "    \"concession\": \"Concession and Pivot\",\n",
    "    \"empirical\": \"Empirical Grounding\",\n",
    "    \"moral\": \"Moral Framing\",\n",
    "    \"refutation\": \"Refutation by Distinction\",\n",
    "    \"circular\": \"Circular Anticipation\"\n",
    "}\n",
    "\n",
    "# Create a partial function with our parameters\n",
    "hook_fn = functools.partial(actadd_hook,\n",
    "                           target_styles=['burden'],\n",
    "                           avoid_styles=['reductio'],\n",
    "                           alpha=1,\n",
    "                           steps=1)\n",
    "\n",
    "# Attach hooks to multiple points\n",
    "fwd_hooks = [\n",
    "    (utils.get_act_name(act_name, l), hook_fn)\n",
    "    for l in intervention_layers\n",
    "    for act_name in ['resid_pre', 'resid_mid', 'resid_post']\n",
    "]\n",
    "\n",
    "# Run model with hooks for debate style steering\n",
    "style_steered_generations = get_generations(\n",
    "    model,\n",
    "    reductio_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=fwd_hooks\n",
    ")\n",
    "\n",
    "# Baseline, no intervention\n",
    "baseline_generations = get_generations(\n",
    "    model,\n",
    "    reductio_inst_test[:N_INST_TEST],\n",
    "    tokenize_instructions_fn,\n",
    "    fwd_hooks=[]\n",
    ")\n",
    "\n",
    "# Compare outputs\n",
    "for i in range(N_INST_TEST):\n",
    "    print(f\"INSTRUCTION {i}: {repr(strawman_inst_test[i])}\")\n",
    "    print(Fore.GREEN + f\"BASELINE COMPLETION:\")\n",
    "    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RED + f\"DEBATE STYLE STEERED COMPLETION:\")\n",
    "    print(textwrap.fill(repr(style_steered_generations[i]), width=100, initial_indent='\\t', subsequent_indent='\\t'))\n",
    "    print(Fore.RESET)\n",
    "\n",
    "\n",
    "result = judge.evaluate_batch(\n",
    "    baseline_generations,  # Your list of baseline texts\n",
    "    style_steered_generations,  # Your list of steered texts\n",
    "    style_label_map[\"burden\"],  # Target style\n",
    "    style_label_map[\"reductio\"]   # Style to avoid\n",
    ")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

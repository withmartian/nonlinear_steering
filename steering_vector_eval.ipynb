{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "FAii7a0jarkj"
      },
      "id": "FAii7a0jarkj",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8",
      "metadata": {
        "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "702863ee-3e14-424f-a304-2f8449de9d8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "702863ee-3e14-424f-a304-2f8449de9d8e",
        "outputId": "173263e4-c444-4bfd-ac93-8fedc5f240e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.11/dist-packages (2.15.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.72.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.11/dist-packages (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.8.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.3.1)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.0.5)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (4.13.1)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.11/dist-packages (from transformer_lens) (0.19.9)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer_lens) (2.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb>=0.13.5->transformer_lens) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers accelerate transformer_lens openai tiktoken kaleido\n",
        "\n",
        "import math\n",
        "import asyncio\n",
        "import tiktoken\n",
        "from typing import Callable, List, Dict, Tuple, Optional, DefaultDict\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList, StoppingCriteria\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from openai import AsyncOpenAI\n",
        "from contextlib import contextmanager\n",
        "import random\n",
        "from urllib.request import urlopen\n",
        "import importlib.util, sys, copy, random, torch, itertools\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "from pathlib import Path\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "0e15cb95-5e68-49af-8934-06bc239edf04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "d18906e2691b48da863c4cb1db9ae09a",
            "9d2f84befacd436ab49222401c80d0e6",
            "765c088f6d8e4ebca674b294e0e26de6",
            "1a796835827a49b0b1c3a6ea0de4f4f1",
            "6dfff147a8e14cff9d127b847d1f9a7f",
            "29f1f84e094844c48ba0a011fe7fbd9a",
            "85e206b7f8c94e848fd0261a59542c4d",
            "866e799cb6164d85ad85cc6b7d9d10f9",
            "f0c7b488eb104ea8bb1f176a3c3daac2",
            "2a536e4ca22848e987d2662e3a7fddff",
            "dca62aa45f1d45fd8f9bb5021fb0f163"
          ]
        },
        "id": "0e15cb95-5e68-49af-8934-06bc239edf04",
        "outputId": "c2a7f5a8-fc1b-40fb-df66-57374a9bde00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: HF_TOKEN=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\n",
            "Loading meta-llama/Llama-3.2-3B-Instruct...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d18906e2691b48da863c4cb1db9ae09a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype = torch.float16,\n",
        "    low_cpu_mem_usage = True,\n",
        "    _attn_implementation = \"eager\",\n",
        "    output_hidden_states = True,\n",
        ").to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tones dataset\n",
        "\n",
        "tone_agnostic_ds = load_dataset(\"Narmeen07/tone_agnostic_questions\", split=\"train\")\n",
        "tone_prompts = [\n",
        "    (\"empathetic\", \"You are an AI assistant responding with an empathetic tone. Provide emotionally attuned responses that validate feelings and show understanding. Prioritize the emotional dimension of the query.\"),\n",
        "    (\"casual\",     \"You are an AI assistant responding with a casual tone. Use a conversational, friendly tone with simpler language and occasional humor. Be relatable and informal, as if chatting with a friend.\"),\n",
        "    (\"helpful\",    \"You are an AI assistant responding with a helpful tone. Provide balanced, informative responses that directly address user needs without bias. Be thorough but straightforward.\"),\n",
        "    (\"cautious\",   \"You are an AI assistant responding with a cautious tone. Give risk-aware responses that highlight limitations, uncertainties, and potential concerns. Be careful to qualify claims and note important caveats.\"),\n",
        "    (\"concise\",    \"You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\"),\n",
        "    (\"expert\",     \"You are an AI assistant responding with an expert tone. Provide detailed, technically precise explanations with domain-specific terminology and depth. Demonstrate expertise and precision in the subject matter.\"),\n",
        "]\n",
        "\n",
        "TONE_LABELS = [\"cautious\", \"empathetic\", \"expert\", \"helpful\", \"casual\", \"concise\"]\n",
        "RELATIVE_TEMPLATE = (\n",
        "    \"You will compare two answers to the *same* question.\\n\\n\"\n",
        "    \"Question:\\n[QUESTION START]\\n{question}\\n[QUESTION END]\\n\\n\"\n",
        "    \"Original answer:\\n[BASE START]\\n{base_answer}\\n[BASE END]\\n\\n\"\n",
        "    \"Modified answer:\\n[STEERED START]\\n{steered_answer}\\n[STEERED END]\\n\\n\"\n",
        "    \"Compared **to the original answer**, which single tone label best \"\n",
        "    \"describes the *modified* answer?\\n\"\n",
        "    f\"Allowed labels: {', '.join(TONE_LABELS)}.\\n\"\n",
        "    \"Respond with **only** one of these labels and nothing else.\"\n",
        ")\n",
        "\n",
        "new_rows = []\n",
        "for row in tone_agnostic_ds:\n",
        "    original_question = row[\"text\"]\n",
        "    for tone, prompt in tone_prompts:\n",
        "        combined_text = f\"SYSTEM: {prompt}\\nUSER: {original_question}\"\n",
        "        new_id = f\"{row['id']}_{tone}\"\n",
        "        new_rows.append({\n",
        "            \"id\": new_id,\n",
        "            \"original_question\": original_question,\n",
        "            \"text\": combined_text,\n",
        "            \"category\": row[\"category\"],\n",
        "            \"tone\": tone,\n",
        "            \"system_message\": prompt,\n",
        "        })\n",
        "\n",
        "dataset_df = pd.DataFrame(new_rows)\n",
        "dataset = Dataset.from_pandas(dataset_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "264396a8ae4e47bbb9aae3242982d1c4",
            "ff6393da78974c39a84858209d568e08",
            "d865f67fa65c43a2835552230b8f0bb7",
            "17205f0949f145a8be5d40274efc35ec",
            "c7758461395c415c8e5d7222ecb7c520",
            "f3882b8d69574735a4d1f408f8013d88",
            "818e5803c52442c7a4166b1b7aacc219",
            "ec540ee6ae974fda96a21c815fe682be",
            "c2a2e61206cc41a39c6e6ee8bc4d690c",
            "578c94a181ab44b4b6ac284696232257",
            "7876af95e5804fcf93771fe2ede4b238",
            "2ceed37df86e48958c2028860512d658",
            "f4fa10778ad44926a3dc556001620a1a",
            "a1c2b4be6f2943bf8d782b3f1707422d",
            "777c1d9dead548408e7e4b5563da070c",
            "b1599d1a00084a26bd8fd31e7b799c24",
            "f5650cc2a67545d6b4576f7f606bb1c9",
            "30254e860c434f8fa2d31b4d6107bad1",
            "3211954eb4244ba7996d596cc937e73b",
            "5ebbae7e5eb547daacaa1f2a95a7728c",
            "9d6077cf3cdf4650bd7998667a1a1699",
            "1fd710deb821472a8aca9df311c0bb9b"
          ]
        },
        "id": "RDt_skjBSSwi",
        "outputId": "507b3fc1-fda7-47e4-9c14-20bade37c715"
      },
      "id": "RDt_skjBSSwi",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dataset.jsonl:   0%|          | 0.00/140k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "264396a8ae4e47bbb9aae3242982d1c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1184 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ceed37df86e48958c2028860512d658"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debate dataset\n",
        "\n",
        "tone_agnostic_ds = load_dataset(\"Narmeen07/debate_style_agnostic_questions\", split=\"train\")\n",
        "tone_prompts = [\n",
        "    #(\"deduction\", \"You are an AI assistant that only makes deductive arguments. Any argument that you make must show that its conclusion follows from its premises, and you must state what those premises are. Do not make non-deductive arguments. When you are given a question, respond immediately with an argument that takes a positive or negative position on the question.\"),\n",
        "    (\"authority\",    \"You are an AI assistant that only makes arguments by appealing to expert opinion. Any argument that you make must include an appeal to an expert. Even if you are not aware of an expert that might support the claim, reference general expert opinion. You must **always** reference expert opinion, and you should do as frequently as possible, even just as 'according to expert opinion'. Your reference to expert opinion should be **excessive**. Do not make arguments that do not reference experts, ever. When you are given a question, respond immediately with an argument that references experts.\"),\n",
        "    #(\"analogy\",   \"You are an AI assistant that only makes arguments by analogy. Arguments that you make must include an analogy and this analogy must be the center of the argument. Do not make arguments that do not rely on an analogy. When you are given a question, respond immediately with an argument that takes a positive or negative position on the question.\"),\n",
        "    (\"negative\",    \"You are an AI assistant that only makes arguments that take negative positions in response to questions. Arguments that you make must begin by disagreeing with the question, followed by your argument for the negative position. Do not make arguments that do not take a negative position. When you are given a question, respond immediately with an argument that takes a negative position on the question.\"),\n",
        "    (\"positive\",    \"You are an AI assistant that only makes arguments that take positive positions in response to questions. Arguments that you make must begin by agreeing with the question, followed by your argument for the positive position. Do not make arguments that do not take a positive position. When you are given a question, respond immediately with an argument that takes a positive position on the question.\"),\n",
        "    ]\n",
        "\n",
        "TONE_LABELS = [\"deduction\", \"absurdum\", \"authority\", \"analogy\", \"negative\"]\n",
        "RELATIVE_TEMPLATE = (\n",
        "    \"You will classify an argument as using a particular argumentative technique. \"\n",
        "    \"This argument will be in response to a question you will be provided.\\n\\n\"\n",
        "    \"Question:\\n[QUESTION START]\\n{question}\\n[QUESTION END]\\n\\n\"\n",
        "    \"Argument:\\n[BASE START]\\n{base_answer}\\n[BASE END]\\n\\n\"\n",
        "    \"Which of the following argumentative techniques best\"\n",
        "    \"describes the given argument?\\n\"\n",
        "    f\"Allowed labels: {', '.join(TONE_LABELS)}.\\n\\n\"\n",
        "    \"Arguments classified as deduction must show that a conclusion must follow from premises.\"\n",
        "    \"Arguments classified as absurdum must show that an opposing view point is incorrect when extended to absurdity.\"\n",
        "    \"Arguments classified as authority must reference an expert opinion.\"\n",
        "    \"Arguments classified as analogy must argue for a point through an analogy.\"\n",
        "    \"Arguments classified as rejection must show reject the premise of the question.\"\n",
        "    \"Respond with **only** one of these labels and nothing else.\"\n",
        ")\n",
        "\n",
        "new_rows = []\n",
        "for row in tone_agnostic_ds:\n",
        "    original_question = row[\"text\"]\n",
        "    for tone, prompt in tone_prompts:\n",
        "        combined_text = f\"SYSTEM: {prompt}\\nUSER: {original_question}\"\n",
        "        new_id = f\"{row['id']}_{tone}\"\n",
        "        new_rows.append({\n",
        "            \"id\": new_id,\n",
        "            \"original_question\": original_question,\n",
        "            \"text\": combined_text,\n",
        "            \"category\": row[\"category\"],\n",
        "            \"tone\": tone,\n",
        "            \"system_message\": prompt,\n",
        "        })\n",
        "\n",
        "dataset_df = pd.DataFrame(new_rows)\n",
        "dataset = Dataset.from_pandas(dataset_df)"
      ],
      "metadata": {
        "id": "Y7bIDVSLaWOq"
      },
      "id": "Y7bIDVSLaWOq",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools dataset"
      ],
      "metadata": {
        "id": "Jmb-rP4RaXyt"
      },
      "id": "Jmb-rP4RaXyt",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e",
      "metadata": {
        "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e"
      },
      "outputs": [],
      "source": [
        "def get_layer_token_hidden(\n",
        "    prompt_texts,\n",
        "    layer_idx=22,\n",
        "    batch_size=64,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    all_vecs = []\n",
        "\n",
        "    for i in range(0, len(prompt_texts), batch_size):\n",
        "        batch = prompt_texts[i : i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            hidden_layer = outputs.hidden_states[layer_idx]\n",
        "\n",
        "        seq_lengths = inputs[\"input_ids\"].ne(tokenizer.pad_token_id).sum(dim=1)\n",
        "\n",
        "        for idx, length in enumerate(seq_lengths):\n",
        "            vec = hidden_layer[idx, length-1, :].cpu().numpy()\n",
        "            all_vecs.append(vec)\n",
        "\n",
        "    return np.array(all_vecs, dtype=np.float32)\n",
        "\n",
        "def batch_generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompts: List[str],\n",
        "    layer_idx: int,\n",
        "    hook_fn: Optional[Callable] = None,\n",
        "    max_new_tokens: int = 64,\n",
        "    batch_size: int = 16,\n",
        ") -> List[str]:\n",
        "    device        = model.device\n",
        "    target_layer  = model.model.layers[layer_idx]\n",
        "    outputs: List[str] = []\n",
        "\n",
        "    saved_hooks = target_layer._forward_hooks.copy()\n",
        "    target_layer._forward_hooks.clear()\n",
        "\n",
        "    handle = None\n",
        "    if hook_fn is not None:\n",
        "        handle = target_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "    try:\n",
        "        for i in range(0, len(prompts), batch_size):\n",
        "            sub_prompts = prompts[i : i + batch_size]\n",
        "            tok_in = tokenizer(\n",
        "                sub_prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                gen_ids = model.generate(\n",
        "                    **tok_in,\n",
        "                    max_new_tokens = max_new_tokens,\n",
        "                    do_sample      = False,\n",
        "                    pad_token_id   = tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "            outputs.extend(\n",
        "                tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
        "            )\n",
        "    finally:\n",
        "        if handle is not None:\n",
        "            handle.remove()\n",
        "        target_layer._forward_hooks.clear()\n",
        "        target_layer._forward_hooks.update(saved_hooks)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steering Methods"
      ],
      "metadata": {
        "id": "04zXz48Htq1R"
      },
      "id": "04zXz48Htq1R"
    },
    {
      "cell_type": "markdown",
      "id": "b1ff85c1-7583-4b1f-9196-86dc27cee26d",
      "metadata": {
        "id": "b1ff85c1-7583-4b1f-9196-86dc27cee26d"
      },
      "source": [
        "## CAA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
        "outputId": "4eaa9db6-4d12-40cf-ef84-39eaad960ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing CAA vector for 'authority' (1956 pairs) …\n",
            "Computing CAA vector for 'negative' (1956 pairs) …\n",
            "Computing CAA vector for 'positive' (1956 pairs) …\n"
          ]
        }
      ],
      "source": [
        "unique_tones = sorted(set(dataset[\"tone\"]))\n",
        "tone2idx     = {t: i for i, t in enumerate(unique_tones)}\n",
        "num_classes  = len(unique_tones)\n",
        "\n",
        "def build_prompt(text: str, tone: str) -> str:\n",
        "    return f\"SYSTEM: Please respond in a {tone} style.\\nUSER: {text}\"\n",
        "\n",
        "def compute_caa_vectors(\n",
        "    dataset,\n",
        "    unique_tones,\n",
        "    build_prompt_fn,\n",
        "    get_layer_token_hidden_fn,\n",
        "    max_pairs: int = None\n",
        ") -> np.ndarray:\n",
        "    text2tones = defaultdict(set)\n",
        "    for row in dataset:\n",
        "        original_text = row[\"original_question\"]\n",
        "        text2tones[original_text].add(row[\"tone\"])\n",
        "\n",
        "    pos_prompts = defaultdict(list)\n",
        "    neg_prompts = defaultdict(list)\n",
        "\n",
        "    for orig_text, tone_set in text2tones.items():\n",
        "        for tgt in tone_set:\n",
        "            for other in tone_set - {tgt}:\n",
        "                pos_prompts[tgt].append(build_prompt_fn(orig_text, tgt))\n",
        "                neg_prompts[tgt].append(build_prompt_fn(orig_text, other))\n",
        "\n",
        "    caa_vecs = []\n",
        "    for tone in unique_tones:\n",
        "        total_pairs = len(pos_prompts[tone])\n",
        "        print(f\"Computing CAA vector for '{tone}' ({total_pairs} pairs) …\")\n",
        "\n",
        "        if max_pairs is not None and total_pairs > max_pairs:\n",
        "            indices = random.sample(range(total_pairs), max_pairs)\n",
        "            pos_prompts[tone] = [pos_prompts[tone][i] for i in indices]\n",
        "            neg_prompts[tone] = [neg_prompts[tone][i] for i in indices]\n",
        "\n",
        "        if not pos_prompts[tone]:\n",
        "            caa_vecs.append(None)\n",
        "            continue\n",
        "\n",
        "        X_pos = get_layer_token_hidden_fn(pos_prompts[tone])\n",
        "        X_neg = get_layer_token_hidden_fn(neg_prompts[tone])\n",
        "        caa_vecs.append((X_pos - X_neg).mean(axis=0))\n",
        "\n",
        "    return np.stack(caa_vecs)\n",
        "\n",
        "caa_vectors = compute_caa_vectors(\n",
        "    dataset                   = dataset,\n",
        "    unique_tones              = unique_tones,\n",
        "    build_prompt_fn           = build_prompt,\n",
        "    get_layer_token_hidden_fn = get_layer_token_hidden,\n",
        "    max_pairs                 = 3900\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "534322de-bb2c-41ef-8033-fd061412212b",
      "metadata": {
        "id": "534322de-bb2c-41ef-8033-fd061412212b"
      },
      "source": [
        "## K-Steering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "11bf606b-adfd-4778-a86f-a0a5caec853c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11bf606b-adfd-4778-a86f-a0a5caec853c",
        "outputId": "e0cdf5ac-9b9d-4e81-ee0c-aefcfedaac19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (1320, 3072)  Holdout: (1320, 3072)  Test: (294, 3072)\n"
          ]
        }
      ],
      "source": [
        "all_prompts = []\n",
        "all_labels = []\n",
        "tone2idx = {tone: i for i, tone in enumerate(unique_tones)}\n",
        "for row in dataset:\n",
        "    all_prompts.append(row[\"text\"])\n",
        "    all_labels.append(tone2idx[row[\"tone\"]])\n",
        "\n",
        "X_all = get_layer_token_hidden(all_prompts)\n",
        "Y_all = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_all, Y_all, test_size=0.1, random_state=42, stratify=Y_all\n",
        ")\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.5, random_state=42, stratify=y_train_val\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}  Holdout: {X_holdout.shape}  Test: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9",
      "metadata": {
        "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9"
      },
      "outputs": [],
      "source": [
        "class MultiLabelSteeringModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_labels):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ActivationSteering:\n",
        "    def __init__(self, input_dim, num_labels, hidden_dim=128, lr=1e-3):\n",
        "        self.device = DEVICE\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        self.classifier = MultiLabelSteeringModel(\n",
        "            input_dim, hidden_dim, num_labels\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def fit(self, X, Y, epochs=10, batch_size=32):\n",
        "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        Y_t = torch.tensor(Y, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(X_t, Y_t)\n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            for bx, by in loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.classifier(bx)\n",
        "                loss = self.loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {ep+1}/{epochs}, Loss={total_loss/len(loader):.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_proba(self, X):\n",
        "        self.classifier.eval()\n",
        "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        logits = self.classifier(X_t)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return probs.cpu().numpy()\n",
        "\n",
        "    def _compute_steering_loss(self, logits, targets=None, avoids=None):\n",
        "        loss = 0.0\n",
        "        if targets:\n",
        "            t_logits = logits[:, targets].mean()\n",
        "            loss -= t_logits\n",
        "        if avoids:\n",
        "            a_logits = logits[:, avoids].mean()\n",
        "            loss += a_logits\n",
        "        return loss\n",
        "\n",
        "    def steer_activations(\n",
        "        self,\n",
        "        activation,\n",
        "        target_labels=None,\n",
        "        avoid_labels=None,\n",
        "        alpha=0.1\n",
        "    ):\n",
        "        if target_labels is None: target_labels = []\n",
        "        if avoid_labels  is None: avoid_labels  = []\n",
        "\n",
        "        self.classifier.eval()\n",
        "        single_input = (activation.ndim == 1)\n",
        "        if single_input:\n",
        "            activation = activation[None, :]\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            X = torch.from_numpy(activation).to(self.device, dtype=torch.float32)\n",
        "            X.requires_grad_()\n",
        "\n",
        "            logits = self.classifier(X)\n",
        "            loss = self._compute_steering_loss(logits, targets=target_labels, avoids=avoid_labels)\n",
        "\n",
        "            if loss != 0.0:\n",
        "                loss.backward()\n",
        "                with torch.no_grad():\n",
        "                    X = X - alpha * X.grad\n",
        "\n",
        "        out = X.detach().cpu().numpy()\n",
        "        return out[0] if single_input else out\n",
        "\n",
        "    def remove_projection(\n",
        "        self,\n",
        "        activation,\n",
        "        target_labels=None,\n",
        "        avoid_labels=None\n",
        "    ):\n",
        "        if target_labels is None: target_labels = []\n",
        "        if avoid_labels  is None: avoid_labels  = []\n",
        "\n",
        "        self.classifier.eval()\n",
        "        single_input = (activation.ndim == 1)\n",
        "        if single_input:\n",
        "            activation = activation[None, :]\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            X = torch.from_numpy(activation).to(self.device, dtype=torch.float32)\n",
        "            X.requires_grad_()\n",
        "\n",
        "            logits = self.classifier(X)\n",
        "            loss = self._compute_steering_loss(logits, targets=target_labels, avoids=avoid_labels)\n",
        "            if loss != 0.0:\n",
        "                loss.backward()\n",
        "\n",
        "                grad = X.grad\n",
        "                dot = torch.sum(X * grad, dim=1, keepdim=True)\n",
        "                norm_sq = torch.sum(grad * grad, dim=1, keepdim=True) + 1e-9\n",
        "                proj = (dot / norm_sq) * grad\n",
        "                X = X - proj\n",
        "\n",
        "        out = X.detach().cpu().numpy()\n",
        "        return out[0] if single_input else out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "97199e20-1d5d-4748-b2cf-7ba2a9727778",
      "metadata": {
        "id": "97199e20-1d5d-4748-b2cf-7ba2a9727778"
      },
      "outputs": [],
      "source": [
        "Y_train_multi = np.zeros((len(y_train), num_classes), dtype=np.float32)\n",
        "for i, lbl in enumerate(y_train):\n",
        "    Y_train_multi[i, lbl] = 1.0\n",
        "\n",
        "Y_holdout_multi = np.zeros((len(y_holdout), num_classes), dtype=np.float32)\n",
        "for i, lbl in enumerate(y_holdout):\n",
        "    Y_holdout_multi[i, lbl] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "276f1009-75e1-40cf-958a-ecfc87ba5eec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "276f1009-75e1-40cf-958a-ecfc87ba5eec",
        "outputId": "a9b2a80e-537d-4e71-d6a3-bbbc9c6b6803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Steering Model (on train set) ---\n",
            "Epoch 1/5, Loss=0.3291\n",
            "Epoch 2/5, Loss=0.0730\n",
            "Epoch 3/5, Loss=0.0367\n",
            "Epoch 4/5, Loss=0.0308\n",
            "Epoch 5/5, Loss=0.0204\n",
            "\n",
            "--- Training Evaluation Model (on held-out set) ---\n",
            "Epoch 1/5, Loss=0.3329\n",
            "Epoch 2/5, Loss=0.0816\n",
            "Epoch 3/5, Loss=0.0299\n",
            "Epoch 4/5, Loss=0.0191\n",
            "Epoch 5/5, Loss=0.0178\n"
          ]
        }
      ],
      "source": [
        "steer_model = ActivationSteering(\n",
        "    input_dim=X_train.shape[1],\n",
        "    num_labels=num_classes,\n",
        "    hidden_dim=128,\n",
        "    lr=1e-3\n",
        ")\n",
        "\n",
        "print(\"\\n--- Training Steering Model (on train set) ---\")\n",
        "steer_model.fit(X_train, Y_train_multi, epochs=5, batch_size=32)\n",
        "\n",
        "eval_model = ActivationSteering(\n",
        "    input_dim=X_train.shape[1],\n",
        "    num_labels=num_classes,\n",
        "    hidden_dim=128,\n",
        "    lr=1e-3\n",
        ")\n",
        "print(\"\\n--- Training Evaluation Model (on held-out set) ---\")\n",
        "eval_model.fit(X_holdout, Y_holdout_multi, epochs=5, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCT"
      ],
      "metadata": {
        "id": "imtkWSCKj9Cb"
      },
      "id": "imtkWSCKj9Cb"
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE_MODEL   = torch.float16\n",
        "DTYPE_DCT     = torch.float32\n",
        "\n",
        "DCT_URL = \"https://raw.githubusercontent.com/luke-marks0/melbo-dct-post/main/src/dct.py\"\n",
        "def load_dct(path: str = \"dct.py\", url: str = DCT_URL):\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        print(\"↯ downloading dct.py …\")\n",
        "        p.write_text(urlopen(url).read().decode())\n",
        "    spec = importlib.util.spec_from_file_location(\"dct\", path)\n",
        "    mod  = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[\"dct\"] = mod\n",
        "    spec.loader.exec_module(mod)\n",
        "    return mod\n",
        "\n",
        "def get_hidden(model, tok, texts, *, max_len=48, layer_idx=-1):\n",
        "    ids = tok(\n",
        "        texts, padding=\"max_length\", truncation=True,\n",
        "        max_length=max_len, return_tensors=\"pt\"\n",
        "    ).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        h = model(**ids, use_cache=False, output_hidden_states=True).hidden_states\n",
        "    return h[layer_idx]\n",
        "\n",
        "def make_slice(base_model, start, end, *, dtype):\n",
        "    m = copy.deepcopy(base_model).to(dtype=dtype)\n",
        "    m.model.layers = m.model.layers[start:end]\n",
        "    return m\n",
        "\n",
        "dct = load_dct()"
      ],
      "metadata": {
        "id": "PZ_d-wf3qqY3"
      },
      "id": "PZ_d-wf3qqY3",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES   = 8\n",
        "SOURCE_LAYER  = 20\n",
        "TARGET_LAYER  = 25\n",
        "NUM_FACTORS   = 4\n",
        "BWD_BATCH     = 2\n",
        "MAX_SEQ_LEN   = 48\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.eval()\n",
        "\n",
        "prompts = random.sample([row[\"text\"] for row in dataset], k=NUM_SAMPLES)\n",
        "\n",
        "source_h = get_hidden(model, tokenizer, prompts,\n",
        "                      max_len=MAX_SEQ_LEN, layer_idx=SOURCE_LAYER).float()\n",
        "\n",
        "slice_model     = make_slice(model, SOURCE_LAYER, TARGET_LAYER, dtype=DTYPE_DCT)\n",
        "last_layer_idx  = len(slice_model.model.layers) - 1\n",
        "\n",
        "sliced = dct.SlicedModel(\n",
        "    slice_model,\n",
        "    start_layer = 0,\n",
        "    end_layer   = last_layer_idx,\n",
        "    layers_name = \"model.layers\",\n",
        ")\n",
        "\n",
        "target_h     = sliced(source_h).float()\n",
        "delta_single = dct.DeltaActivations(\n",
        "    sliced, target_position_indices=slice(-3, None)\n",
        ")\n",
        "\n",
        "calibrator = dct.SteeringCalibrator(target_ratio=0.5)\n",
        "try:\n",
        "    INPUT_SCALE = calibrator.calibrate(\n",
        "        delta_single, source_h, target_h, factor_batch_size=64\n",
        "    )\n",
        "except ValueError:\n",
        "    print(\"Calibrator failed to bracket a root. Using scale = 1.0\")\n",
        "    INPUT_SCALE = 1.0\n",
        "\n",
        "exp_dct = dct.ExponentialDCT(num_factors=NUM_FACTORS)\n",
        "U, V = exp_dct.fit(\n",
        "    delta_single,\n",
        "    source_h, target_h,\n",
        "    batch_size        = BWD_BATCH,\n",
        "    factor_batch_size = 128,\n",
        "    d_proj            = 48,\n",
        "    input_scale       = INPUT_SCALE,\n",
        "    max_iters         = 6,\n",
        ")\n",
        "\n",
        "dct_vectors = V.cpu().detach().numpy().T\n",
        "print(f\"Learnt {dct_vectors.shape[0]} steering vectors\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSfgHwBPkAj4",
        "outputId": "7097274d-2bec-49ef-9008-b35622c09fe6"
      },
      "id": "tSfgHwBPkAj4",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing V,U...\n",
            "training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/6 [00:00<?, ?it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:00<00:00, 29.75it/s]\n",
            " 17%|█▋        | 1/6 [00:00<00:01,  4.31it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:00<00:00, 29.58it/s]\n",
            " 33%|███▎      | 2/6 [00:00<00:00,  4.30it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:00<00:00, 29.63it/s]\n",
            " 50%|█████     | 3/6 [00:00<00:00,  4.29it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:00<00:00, 26.55it/s]\n",
            " 67%|██████▋   | 4/6 [00:00<00:00,  4.17it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:00<00:00, 29.66it/s]\n",
            " 83%|████████▎ | 5/6 [00:01<00:00,  4.21it/s]\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 4/4 [00:00<00:00, 29.72it/s]\n",
            "100%|██████████| 6/6 [00:01<00:00,  4.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learnt 4 steering vectors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Methods"
      ],
      "metadata": {
        "id": "2I4u2MY2t1uf"
      },
      "id": "2I4u2MY2t1uf"
    },
    {
      "cell_type": "markdown",
      "id": "PhuBTSqn5fqk",
      "metadata": {
        "id": "PhuBTSqn5fqk"
      },
      "source": [
        "## LLM Judge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "NXkskgAnZPkI",
      "metadata": {
        "id": "NXkskgAnZPkI"
      },
      "outputs": [],
      "source": [
        "def first_token_map(model_name: str) -> Dict[str, str]:\n",
        "    enc = tiktoken.encoding_for_model(model_name)\n",
        "    return {\n",
        "        lbl: enc.decode([enc.encode(lbl)[0]])\n",
        "        for lbl in TONE_LABELS\n",
        "    }\n",
        "\n",
        "class OpenAiJudge:\n",
        "    def __init__(self, client: AsyncOpenAI, model_name: str):\n",
        "        self.client        = client\n",
        "        self.model_name    = model_name\n",
        "        self._first_token  = first_token_map(model_name)\n",
        "\n",
        "    async def compare(self,\n",
        "                      question: str,\n",
        "                      base_answer: str,\n",
        "                      steered_answer: str) -> str:\n",
        "        prompt = RELATIVE_TEMPLATE.format(\n",
        "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
        "        )\n",
        "        return await self._best_label(prompt)\n",
        "\n",
        "    async def compare_logits(self,\n",
        "                             question: str,\n",
        "                             base_answer: str,\n",
        "                             steered_answer: str,\n",
        "                             top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
        "        prompt = RELATIVE_TEMPLATE.format(\n",
        "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
        "        )\n",
        "        return await self._label_probs(prompt, top_k)\n",
        "\n",
        "    async def _best_label(self, prompt: str, top_k: int = 20) -> str:\n",
        "        best, _ = await self._label_probs(prompt, top_k)\n",
        "        return best\n",
        "\n",
        "    async def _label_probs(self, prompt: str,\n",
        "                           top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
        "        completion = await self.client.chat.completions.create(\n",
        "            model        = self.model_name,\n",
        "            messages     = [{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens   = 1,\n",
        "            temperature  = 0,\n",
        "            logprobs     = True,\n",
        "            top_logprobs = top_k,\n",
        "            seed         = 0,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            top = completion.choices[0].logprobs.content[0].top_logprobs\n",
        "        except IndexError:\n",
        "            raise RuntimeError(\"OpenAI response missing logprobs\")\n",
        "\n",
        "        tok_prob = {el.token: math.exp(el.logprob) for el in top}\n",
        "        probs    = {\n",
        "            lbl: tok_prob.get(self._first_token[lbl], 0.0)\n",
        "            for lbl in TONE_LABELS\n",
        "        }\n",
        "        best_lbl = max(probs, key=probs.get)\n",
        "        return best_lbl, probs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3AydPHF46saH",
      "metadata": {
        "id": "3AydPHF46saH"
      },
      "source": [
        "## Output Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "udLVtAdG6vrD",
      "metadata": {
        "id": "udLVtAdG6vrD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib, os, hashlib, numpy as np\n",
        "from tqdm import tqdm\n",
        "from typing import List, Callable\n",
        "\n",
        "def build_generation_text_classifier(\n",
        "    dataset,\n",
        "    unique_tones: List[str],\n",
        "    *,\n",
        "    base_model, tokenizer,\n",
        "    build_prompt_fn: Callable[[str, str], str],\n",
        "    batch_generate_fn: Callable[..., List[str]],\n",
        "    model_name_for_hash: str,\n",
        "    layer_idx: int = 0,\n",
        "    max_new_tokens: int = 64,\n",
        "    batch_size: int = 16,\n",
        "    cache_path: str = \"tone_gen_text_clf.joblib\",\n",
        ") -> Callable[[List[str]], List[str]]:\n",
        "    prompts, labels = [], []\n",
        "    # With the new dataset the text field is already built,\n",
        "    # so we simply use it directly:\n",
        "    for row in dataset:\n",
        "        prompts.append(row[\"text\"])\n",
        "        labels.append(row[\"tone\"])\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    md5.update(model_name_for_hash.encode())\n",
        "    for p, t in zip(prompts, labels):\n",
        "        md5.update(p.encode())\n",
        "        md5.update(t.encode())\n",
        "    corpus_hash = md5.hexdigest()\n",
        "\n",
        "    if os.path.exists(cache_path):\n",
        "        saved = joblib.load(cache_path)\n",
        "        if saved.get(\"hash\") == corpus_hash:\n",
        "            pipe, lbl_enc = saved[\"pipe\"], saved[\"lbl_enc\"]\n",
        "            print(\"Loaded cached generation‑based text‑classifier.\")\n",
        "        else:\n",
        "            print(\"Cache hash mismatch → regenerate completions & retrain.\")\n",
        "            pipe, lbl_enc = None, None\n",
        "    else:\n",
        "        pipe, lbl_enc = None, None\n",
        "\n",
        "    if pipe is None:\n",
        "        print(\"Generating model answers for classifier training...\")\n",
        "        gen_answers = []\n",
        "\n",
        "        for i in tqdm(range(0, len(prompts), batch_size),\n",
        "                      desc=\"Generating\", unit=\"batch\"):\n",
        "            chunk_prompts = prompts[i : i + batch_size]\n",
        "            outs = batch_generate_fn(\n",
        "                base_model, tokenizer, chunk_prompts,\n",
        "                layer_idx      = layer_idx,\n",
        "                hook_fn        = None,\n",
        "                max_new_tokens = max_new_tokens,\n",
        "                batch_size     = batch_size,\n",
        "            )\n",
        "            gen_answers.extend(outs)\n",
        "\n",
        "        lbl_enc = LabelEncoder().fit(unique_tones)\n",
        "        y = lbl_enc.transform(labels)\n",
        "\n",
        "        pipe = make_pipeline(\n",
        "            TfidfVectorizer(\n",
        "                lowercase=True,\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=50_000,\n",
        "                sublinear_tf=True\n",
        "            ),\n",
        "            LogisticRegression(\n",
        "                max_iter=1_000,\n",
        "                n_jobs=-1,\n",
        "                multi_class=\"multinomial\"\n",
        "            )\n",
        "        )\n",
        "        pipe.fit(gen_answers, y)\n",
        "\n",
        "        train_preds = pipe.predict(gen_answers)\n",
        "        acc_train = accuracy_score(y, train_preds)\n",
        "        print(f\"Output classifier training accuracy: {acc_train * 100:.2f}%\")\n",
        "\n",
        "        joblib.dump({\"hash\": corpus_hash, \"pipe\": pipe, \"lbl_enc\": lbl_enc},\n",
        "                    cache_path)\n",
        "\n",
        "    def predict_fn(text_list: List[str]) -> List[str]:\n",
        "        y_pred = pipe.predict(text_list)\n",
        "        return lbl_enc.inverse_transform(y_pred).tolist()\n",
        "\n",
        "    return predict_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "ZY2tGkBABZRh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY2tGkBABZRh",
        "outputId": "49313ab2-e072-4e34-a2ab-9224cadc4f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating model answers for classifier training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating:   0%|          | 0/12 [00:00<?, ?batch/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:   8%|▊         | 1/12 [00:03<00:42,  3.90s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  17%|█▋        | 2/12 [00:07<00:38,  3.87s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  25%|██▌       | 3/12 [00:11<00:34,  3.86s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  33%|███▎      | 4/12 [00:15<00:30,  3.85s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  42%|████▏     | 5/12 [00:19<00:26,  3.85s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  50%|█████     | 6/12 [00:23<00:23,  3.85s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  58%|█████▊    | 7/12 [00:27<00:19,  3.87s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  67%|██████▋   | 8/12 [00:30<00:15,  3.88s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  75%|███████▌  | 9/12 [00:34<00:11,  3.88s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  83%|████████▎ | 10/12 [00:38<00:07,  3.87s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating:  92%|█████████▏| 11/12 [00:42<00:03,  3.86s/batch]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Generating: 100%|██████████| 12/12 [00:44<00:00,  3.71s/batch]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output classifier training accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "gen_clf_fn = build_generation_text_classifier(\n",
        "    dataset          = dataset,\n",
        "    unique_tones     = unique_tones,\n",
        "    base_model       = model,\n",
        "    tokenizer        = tokenizer,\n",
        "    build_prompt_fn  = build_prompt,\n",
        "    batch_generate_fn= batch_generate,\n",
        "    model_name_for_hash = model_name,\n",
        "    layer_idx        = 22,\n",
        "    max_new_tokens   = 32,\n",
        "    batch_size       = 256,\n",
        "    cache_path       = \"tone_gen_text_clf.joblib\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7b7bd55-3a68-4e71-a797-790581301243",
      "metadata": {
        "id": "b7b7bd55-3a68-4e71-a797-790581301243"
      },
      "source": [
        "# Steering Vector Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "068fd591-4032-423e-aa0a-041b098ea041",
      "metadata": {
        "id": "068fd591-4032-423e-aa0a-041b098ea041"
      },
      "outputs": [],
      "source": [
        "@contextmanager\n",
        "def temp_forward_hook(layer, hook_fn):\n",
        "    saved = layer._forward_hooks.copy()\n",
        "    layer._forward_hooks.clear()\n",
        "    handle = None\n",
        "    try:\n",
        "        if hook_fn is not None:\n",
        "            handle = layer.register_forward_hook(hook_fn)\n",
        "        yield\n",
        "    finally:\n",
        "        if handle is not None:\n",
        "            handle.remove()\n",
        "        layer._forward_hooks.clear()\n",
        "        layer._forward_hooks.update(saved)\n",
        "\n",
        "def my_hook_wrapper(fwd_hook):\n",
        "    def actual_hook(module, inp, out):\n",
        "        if fwd_hook is None:\n",
        "            return out\n",
        "        else:\n",
        "            return fwd_hook(module, inp, out)\n",
        "    return actual_hook\n",
        "\n",
        "def get_remove_proj_hook(steer_model, target_labels=None, avoid_labels=None):\n",
        "    if target_labels is None: target_labels = []\n",
        "    if avoid_labels is None: avoid_labels = []\n",
        "\n",
        "    def fwd_hook(module, inp, out):\n",
        "        hidden_states = out[0]\n",
        "        hidden_np = hidden_states.detach().cpu().numpy().astype(np.float32)\n",
        "        B, S, D = hidden_np.shape\n",
        "        hidden_2d = hidden_np.reshape(-1, D)\n",
        "\n",
        "        new_2d = steer_model.remove_projection(hidden_2d, target_labels=target_labels, avoid_labels=avoid_labels)\n",
        "        new_np = new_2d.reshape(B, S, D)\n",
        "        new_hidden_torch = torch.from_numpy(new_np).to(hidden_states.device, dtype=torch.float16)\n",
        "        return (new_hidden_torch,) + out[1:]\n",
        "    return fwd_hook\n",
        "\n",
        "def get_gradient_hook(steer_model, target_labels=None, avoid_labels=None, alpha=1.0):\n",
        "    if target_labels is None: target_labels = []\n",
        "    if avoid_labels is None: avoid_labels = []\n",
        "\n",
        "    def fwd_hook(module, inp, out):\n",
        "        hidden_states = out[0]\n",
        "        hidden_np = hidden_states.detach().cpu().numpy().astype(np.float32)\n",
        "        B, S, D = hidden_np.shape\n",
        "        hidden_2d = hidden_np.reshape(-1, D)\n",
        "\n",
        "        new_2d = steer_model.steer_activations(hidden_2d,\n",
        "                                               target_labels=target_labels,\n",
        "                                               avoid_labels=avoid_labels,\n",
        "                                               alpha=alpha)\n",
        "        new_np = new_2d.reshape(B, S, D)\n",
        "        new_hidden_torch = torch.from_numpy(new_np).to(hidden_states.device, dtype=torch.float16)\n",
        "        return (new_hidden_torch,) + out[1:]\n",
        "    return fwd_hook\n",
        "\n",
        "def get_caa_hook(caa_vector, alpha=1.0):\n",
        "    def fwd_hook(module, inp, out):\n",
        "        hidden_states = out[0]\n",
        "        hidden_np = hidden_states.detach().cpu().numpy().astype(np.float32)\n",
        "        B, S, D = hidden_np.shape\n",
        "        hidden_2d = hidden_np.reshape(-1, D)\n",
        "\n",
        "        hidden_2d += alpha * caa_vector[None, :]\n",
        "        new_np = hidden_2d.reshape(B, S, D)\n",
        "        new_hidden_torch = torch.from_numpy(new_np).to(hidden_states.device, dtype=torch.float16)\n",
        "        return (new_hidden_torch,) + out[1:]\n",
        "    return fwd_hook\n",
        "\n",
        "def get_dct_hook(dct_vec, alpha=1.0):\n",
        "    if isinstance(dct_vec, torch.Tensor):\n",
        "        dct_vec = dct_vec.detach().cpu().numpy()\n",
        "\n",
        "    def fwd_hook(module, inp, out):\n",
        "        h = out[0]\n",
        "        h_np = h.detach().cpu().numpy().astype(np.float32)\n",
        "        h_np += alpha * dct_vec[None, None, :]\n",
        "        h_new = torch.from_numpy(h_np).to(h.device, dtype=h.dtype)\n",
        "        return (h_new,) + out[1:]\n",
        "\n",
        "    return fwd_hook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _batch_generate_wrapper(model, tokenizer, prompts, layer_idx, hook, bs):\n",
        "    return batch_generate(model, tokenizer, prompts,\n",
        "                          layer_idx = layer_idx,\n",
        "                          hook_fn   = hook,\n",
        "                          batch_size= bs)\n",
        "\n",
        "async def batch_compare(\n",
        "    triples: List[Tuple[str, str, str]],\n",
        "    judge   : OpenAiJudge,\n",
        "    max_concurrency: int = 10,\n",
        ") -> List[str]:\n",
        "    sem   = asyncio.Semaphore(max_concurrency)\n",
        "    out   = [None] * len(triples)\n",
        "\n",
        "    async def worker(idx: int, q: str, b: str, s: str):\n",
        "        async with sem:\n",
        "            out[idx] = await judge.compare(q, b, s)\n",
        "\n",
        "    tasks = [asyncio.create_task(worker(i, *t)) for i, t in enumerate(triples)]\n",
        "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks),\n",
        "                  desc=\"LLM‑judge\", leave=False):\n",
        "        await f\n",
        "    return out\n",
        "\n",
        "async def _llm_batch_compare(triples, judge, parallel):\n",
        "    return await batch_compare(triples, judge, max_concurrency=parallel)\n",
        "\n",
        "def _vector_majority(preds, tone2idx, unique_tones):\n",
        "    idx = int(np.bincount([tone2idx[p] for p in preds]).argmax())\n",
        "    return unique_tones[idx]\n",
        "\n",
        "def _prepare_bases(\n",
        "    eval_method      : str,\n",
        "    prompts          : List[str],\n",
        "    *,\n",
        "    base_model,\n",
        "    tokenizer,\n",
        "    batch_size,\n",
        "    layer_idx,\n",
        "    act_clf          = None,\n",
        "    get_layer_token_hidden_fn = None,\n",
        "):\n",
        "    base_ans = _batch_generate_wrapper(\n",
        "        base_model, tokenizer, prompts,\n",
        "        layer_idx = layer_idx,\n",
        "        hook      = None,\n",
        "        bs        = batch_size,\n",
        "    )\n",
        "    base_act = None\n",
        "    if eval_method == \"activation_classifier\":\n",
        "        base_act = get_layer_token_hidden_fn(prompts)\n",
        "    return base_ans, base_act\n",
        "\n",
        "async def _map_dct_vectors(\n",
        "    *,\n",
        "    include_dct      : bool,\n",
        "    dct_vectors      : Optional[np.ndarray],\n",
        "    eval_method      : str,\n",
        "    base_model,\n",
        "    tokenizer,\n",
        "    prompts,\n",
        "    base_ans,\n",
        "    act_clf,\n",
        "    gen_clf_fn,\n",
        "    judge,\n",
        "    judge_parallel,\n",
        "    alpha_dct,\n",
        "    layer_idx,\n",
        "    batch_size,\n",
        "    base_act,\n",
        "    unique_tones,\n",
        "    tone2idx,\n",
        "    get_layer_token_hidden_fn,\n",
        "):\n",
        "    if not include_dct:\n",
        "        return defaultdict(list)\n",
        "\n",
        "    tone2dct: DefaultDict[str, List[int]] = defaultdict(list)\n",
        "\n",
        "    if eval_method == \"activation_classifier\":\n",
        "        device = next(act_clf.parameters()).device\n",
        "        for i, vec in enumerate(dct_vectors):\n",
        "            acts = base_act + vec[None, :]\n",
        "            acts_t = torch.tensor(acts, dtype=torch.float32, device=device)\n",
        "            with torch.no_grad():\n",
        "                preds = act_clf(acts_t).argmax(dim=1).cpu().numpy()\n",
        "            maj = unique_tones[int(np.bincount(preds).argmax())]\n",
        "            tone2dct[maj].append(i)\n",
        "\n",
        "    else:\n",
        "        async def classify_vec(i_vec, vec):\n",
        "            hook = get_dct_hook(vec, alpha=alpha_dct)\n",
        "            outs = _batch_generate_wrapper(\n",
        "                base_model, tokenizer, prompts,\n",
        "                layer_idx, hook, batch_size\n",
        "            )\n",
        "            if eval_method == \"generation_classifier\":\n",
        "                lbls = gen_clf_fn(outs)\n",
        "            else:\n",
        "                triples = [(q, b, s) for q, b, s in zip(prompts, base_ans, outs)]\n",
        "                lbls = await _llm_batch_compare(triples, judge, judge_parallel)\n",
        "            maj = _vector_majority(lbls, tone2idx, unique_tones)\n",
        "            tone2dct[maj].append(i_vec)\n",
        "\n",
        "        await asyncio.gather(*[\n",
        "            classify_vec(i, vec) for i, vec in enumerate(dct_vectors)\n",
        "        ])\n",
        "\n",
        "    return tone2dct\n",
        "\n",
        "async def _evaluate_combo(\n",
        "    tgt_idx, tgt_names, tgt_set,\n",
        "    *,\n",
        "    eval_method,\n",
        "    base_ans,\n",
        "    base_act,\n",
        "    model_device,\n",
        "    prompts,\n",
        "    N,\n",
        "    unique_tones,\n",
        "    tone2idx,\n",
        "    steer_model,\n",
        "    caa_vectors,\n",
        "    alpha_grad,\n",
        "    alpha_caa,\n",
        "    include_dct,\n",
        "    dct_vectors,\n",
        "    tone2dct,\n",
        "    alpha_dct,\n",
        "    base_model,\n",
        "    tokenizer,\n",
        "    layer_idx,\n",
        "    batch_size,\n",
        "    act_clf,\n",
        "    gen_clf_fn,\n",
        "    judge,\n",
        "    judge_parallel,\n",
        "):\n",
        "    grad_hook = get_gradient_hook(\n",
        "        steer_model, target_labels=tgt_idx, alpha=alpha_grad\n",
        "    )\n",
        "    caa_vec   = caa_vectors[tgt_idx].mean(axis=0)\n",
        "    caa_hook  = get_caa_hook(caa_vec, alpha=alpha_caa)\n",
        "\n",
        "    counts = dict(grad=0, caa=0, dct=0)\n",
        "\n",
        "    if eval_method == \"activation_classifier\":\n",
        "\n",
        "        grad_act = steer_model.steer_activations(base_act, tgt_idx, alpha=alpha_grad)\n",
        "        grad_preds = act_clf(\n",
        "            torch.tensor(grad_act, dtype=torch.float32, device=model_device)\n",
        "        ).argmax(dim=1).cpu().numpy()\n",
        "        counts[\"grad\"] = sum(unique_tones[p] in tgt_set for p in grad_preds)\n",
        "\n",
        "        caa_act  = base_act + caa_vec[None, :]\n",
        "        caa_preds = act_clf(\n",
        "            torch.tensor(caa_act, dtype=torch.float32, device=model_device)\n",
        "        ).argmax(dim=1).cpu().numpy()\n",
        "        counts[\"caa\"] = sum(unique_tones[p] in tgt_set for p in caa_preds)\n",
        "\n",
        "        if include_dct:\n",
        "            vecs = [dct_vectors[i] for t in tgt_names for i in tone2dct.get(t, [])]\n",
        "            if vecs:\n",
        "                dct_vec  = np.stack(vecs).mean(axis=0)\n",
        "                dct_act  = base_act + dct_vec[None, :]\n",
        "                dct_preds = act_clf(\n",
        "                    torch.tensor(dct_act, dtype=torch.float32, device=model_device)\n",
        "                ).argmax(dim=1).cpu().numpy()\n",
        "                counts[\"dct\"] = sum(unique_tones[p] in tgt_set for p in dct_preds)\n",
        "\n",
        "    else:\n",
        "        grad_out = _batch_generate_wrapper(base_model, tokenizer, prompts,\n",
        "                                           layer_idx, grad_hook, batch_size)\n",
        "        caa_out  = _batch_generate_wrapper(base_model, tokenizer, prompts,\n",
        "                                           layer_idx, caa_hook, batch_size)\n",
        "        if eval_method == \"generation_classifier\":\n",
        "            grad_preds = gen_clf_fn(grad_out)\n",
        "            caa_preds  = gen_clf_fn(caa_out)\n",
        "            counts[\"grad\"] = sum(l in tgt_set for l in grad_preds)\n",
        "            counts[\"caa\"]  = sum(l in tgt_set for l in caa_preds)\n",
        "            if include_dct:\n",
        "                vecs = [dct_vectors[i] for t in tgt_names for i in tone2dct.get(t, [])]\n",
        "                if vecs:\n",
        "                    dct_vec = np.stack(vecs).mean(axis=0)\n",
        "                    dct_out = _batch_generate_wrapper(base_model, tokenizer, prompts,\n",
        "                                                      layer_idx,\n",
        "                                                      get_dct_hook(dct_vec, alpha_dct),\n",
        "                                                      batch_size)\n",
        "                    counts[\"dct\"] = sum(l in tgt_set for l in gen_clf_fn(dct_out))\n",
        "        else:\n",
        "            triples, where = [], []\n",
        "            for q, b, g, c in zip(prompts, base_ans, grad_out, caa_out):\n",
        "                triples.append((q, b, g)); where.append(\"grad\")\n",
        "                triples.append((q, b, c)); where.append(\"caa\")\n",
        "            if include_dct:\n",
        "                vecs = [dct_vectors[i] for t in tgt_names for i in tone2dct.get(t, [])]\n",
        "                if vecs:\n",
        "                    dct_vec = np.stack(vecs).mean(axis=0)\n",
        "                    dct_out = _batch_generate_wrapper(base_model, tokenizer, prompts,\n",
        "                                                      layer_idx,\n",
        "                                                      get_dct_hook(dct_vec, alpha_dct),\n",
        "                                                      batch_size)\n",
        "                    for q, b, d in zip(prompts, base_ans, dct_out):\n",
        "                        triples.append((q, b, d)); where.append(\"dct\")\n",
        "            preds = await _llm_batch_compare(triples, judge, judge_parallel)\n",
        "            for w, lbl in zip(where, preds):\n",
        "                if lbl in tgt_set:\n",
        "                    counts[w] += 1\n",
        "\n",
        "    row = {\n",
        "        \"Targets\"          : \", \".join(tgt_names),\n",
        "        \"Grad_MeanHitRate\" : counts[\"grad\"] / N,\n",
        "        \"CAA_MeanHitRate\"  : counts[\"caa\"]  / N,\n",
        "    }\n",
        "    if include_dct:\n",
        "        row[\"DCT_MeanHitRate\"] = counts[\"dct\"] / N\n",
        "    return row"
      ],
      "metadata": {
        "id": "ec1kvyXiuB-q"
      },
      "id": "ec1kvyXiuB-q",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f2yOlF67PBGb",
      "metadata": {
        "id": "f2yOlF67PBGb"
      },
      "outputs": [],
      "source": [
        "async def eval_steering_combinations(\n",
        "    *,\n",
        "    eval_method      : str,\n",
        "    base_model,\n",
        "    tokenizer,\n",
        "    prompts          : List[str],\n",
        "    unique_tones     : List[str],\n",
        "    caa_vectors,\n",
        "    steer_model,\n",
        "    layer_idx        : int = 22,\n",
        "    alpha_grad       : float = 50.0,\n",
        "    alpha_caa        : float =  2.0,\n",
        "    alpha_dct        : float =  2.0,\n",
        "    include_dct      : bool  = False,\n",
        "    dct_vectors      : Optional[np.ndarray] = None,\n",
        "    num_target_tones : int   = 2,\n",
        "    max_samples      : int   = 300,\n",
        "    batch_size       : int   = 16,\n",
        "    judge_parallel   : int   = 25,\n",
        "    judge            = None,\n",
        "    act_clf          = None,\n",
        "    gen_clf_fn       : Optional[Callable[[List[str]], List[str]]] = None,\n",
        "    get_layer_token_hidden_fn = None,\n",
        ") -> pd.DataFrame:\n",
        "\n",
        "    prompts = prompts[:max_samples]\n",
        "    tone2idx = {t: i for i, t in enumerate(unique_tones)}\n",
        "    N = float(len(prompts))\n",
        "\n",
        "    base_ans, base_act = _prepare_bases(\n",
        "        eval_method, prompts,\n",
        "        base_model   = base_model,\n",
        "        tokenizer    = tokenizer,\n",
        "        batch_size   = batch_size,\n",
        "        layer_idx    = layer_idx,\n",
        "        act_clf      = act_clf,\n",
        "        get_layer_token_hidden_fn = get_layer_token_hidden_fn,\n",
        "    )\n",
        "\n",
        "    model_device = None\n",
        "    if eval_method == \"activation_classifier\":\n",
        "        model_device = next(act_clf.parameters()).device\n",
        "\n",
        "    tone2dct = await _map_dct_vectors(\n",
        "        include_dct = include_dct,\n",
        "        dct_vectors = dct_vectors,\n",
        "        eval_method = eval_method,\n",
        "        base_model  = base_model,\n",
        "        tokenizer   = tokenizer,\n",
        "        prompts     = prompts,\n",
        "        base_ans    = base_ans,\n",
        "        act_clf     = act_clf,\n",
        "        gen_clf_fn  = gen_clf_fn,\n",
        "        judge       = judge,\n",
        "        judge_parallel = judge_parallel,\n",
        "        alpha_dct   = alpha_dct,\n",
        "        layer_idx   = layer_idx,\n",
        "        batch_size  = batch_size,\n",
        "        base_act    = base_act,\n",
        "        unique_tones= unique_tones,\n",
        "        tone2idx    = tone2idx,\n",
        "        get_layer_token_hidden_fn = get_layer_token_hidden_fn,\n",
        "    )\n",
        "\n",
        "    combos = list(itertools.combinations(range(len(unique_tones)), num_target_tones))\n",
        "    rows   = []\n",
        "    for tgt_idx in tqdm(combos, desc=f\"{num_target_tones}-tone combos\"):\n",
        "        tgt_idx   = list(tgt_idx)\n",
        "        tgt_names = [unique_tones[i] for i in tgt_idx]\n",
        "        tgt_set   = set(tgt_names)\n",
        "\n",
        "        row = await _evaluate_combo(\n",
        "            tgt_idx, tgt_names, tgt_set,\n",
        "            eval_method  = eval_method,\n",
        "            base_ans     = base_ans,\n",
        "            base_act     = base_act,\n",
        "            model_device = model_device,\n",
        "            prompts      = prompts,\n",
        "            N            = N,\n",
        "            unique_tones = unique_tones,\n",
        "            tone2idx     = tone2idx,\n",
        "            steer_model  = steer_model,\n",
        "            caa_vectors  = caa_vectors,\n",
        "            alpha_grad   = alpha_grad,\n",
        "            alpha_caa    = alpha_caa,\n",
        "            include_dct  = include_dct,\n",
        "            dct_vectors  = dct_vectors,\n",
        "            tone2dct     = tone2dct,\n",
        "            alpha_dct    = alpha_dct,\n",
        "            base_model   = base_model,\n",
        "            tokenizer    = tokenizer,\n",
        "            layer_idx    = layer_idx,\n",
        "            batch_size   = batch_size,\n",
        "            act_clf      = act_clf,\n",
        "            gen_clf_fn   = gen_clf_fn,\n",
        "            judge        = judge,\n",
        "            judge_parallel = judge_parallel,\n",
        "        )\n",
        "        rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "PsYZOvbaruK6",
      "metadata": {
        "id": "PsYZOvbaruK6"
      },
      "outputs": [],
      "source": [
        "eval_prompts = [row[\"text\"] for row in dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "4EGX3Ly3-TUy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175,
          "referenced_widgets": [
            "302d16a2af4a48cd8dcb1bd9a069669e",
            "f163699b8ce24573a9f580075bb0ef7b",
            "01073d2f96674d10b6d89115025e9104",
            "cb57d92c4ab34f5aba52793dd120d1c5",
            "d632ddc45c8a41c692b0e1bf5de0dde9",
            "85d61cf97edb4f0d92182550fa6b5d2c",
            "5811a6a2e7044f0a8aab572cd5795555",
            "c8ee251f94de4bc1aa37f1f004f19371",
            "6ec95c045d8941558168b56d3a1e874b",
            "7ef2b3a1241041e6a14e7da82163d53c",
            "d5fa3dd2b16344ec9c85b4ab6581e975"
          ]
        },
        "id": "4EGX3Ly3-TUy",
        "outputId": "1da47b20-a48f-4a18-fece-554341cde087"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2-tone combos:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "302d16a2af4a48cd8dcb1bd9a069669e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Targets  Grad_MeanHitRate  CAA_MeanHitRate  DCT_MeanHitRate\n",
              "0  authority, negative          0.653333         0.616667         0.000000\n",
              "1  authority, positive          0.953333         0.983333         0.723333\n",
              "2   negative, positive          0.866667         0.876667         0.826667"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1e707438-7103-4cc7-bd69-cd28baa3a7c5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Targets</th>\n",
              "      <th>Grad_MeanHitRate</th>\n",
              "      <th>CAA_MeanHitRate</th>\n",
              "      <th>DCT_MeanHitRate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>authority, negative</td>\n",
              "      <td>0.653333</td>\n",
              "      <td>0.616667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>authority, positive</td>\n",
              "      <td>0.953333</td>\n",
              "      <td>0.983333</td>\n",
              "      <td>0.723333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative, positive</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.876667</td>\n",
              "      <td>0.826667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e707438-7103-4cc7-bd69-cd28baa3a7c5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1e707438-7103-4cc7-bd69-cd28baa3a7c5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1e707438-7103-4cc7-bd69-cd28baa3a7c5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c5fc72cb-1194-4982-a57d-5daf6338ecf0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c5fc72cb-1194-4982-a57d-5daf6338ecf0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c5fc72cb-1194-4982-a57d-5daf6338ecf0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d3dd4aa8-d54a-4448-ae20-02948d51904b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_act')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d3dd4aa8-d54a-4448-ae20-02948d51904b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_act');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_act",
              "summary": "{\n  \"name\": \"df_act\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Targets\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"authority, negative\",\n          \"authority, positive\",\n          \"negative, positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Grad_MeanHitRate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15439247726828223,\n        \"min\": 0.6533333333333333,\n        \"max\": 0.9533333333333334,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.6533333333333333,\n          0.9533333333333334,\n          0.8666666666666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CAA_MeanHitRate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18860108793527774,\n        \"min\": 0.6166666666666667,\n        \"max\": 0.9833333333333333,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.6166666666666667,\n          0.9833333333333333,\n          0.8766666666666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DCT_MeanHitRate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4504195574992029,\n        \"min\": 0.0,\n        \"max\": 0.8266666666666667,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0,\n          0.7233333333333334,\n          0.8266666666666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "df_act = await eval_steering_combinations(\n",
        "    eval_method      = \"activation_classifier\",\n",
        "    act_clf          = eval_model.classifier,\n",
        "    get_layer_token_hidden_fn = get_layer_token_hidden,\n",
        "    base_model       = model,\n",
        "    tokenizer        = tokenizer,\n",
        "    prompts          = eval_prompts,\n",
        "    unique_tones     = unique_tones,\n",
        "    caa_vectors      = caa_vectors,\n",
        "    steer_model      = steer_model,\n",
        "    include_dct      = True,\n",
        "    dct_vectors      = dct_vectors,\n",
        "    alpha_dct        = 2.0,\n",
        "    num_target_tones = 2,\n",
        ")\n",
        "\n",
        "df_act"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "MLAwRB-g5xZg"
      },
      "id": "MLAwRB-g5xZg"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "jiWVuufUdFUv",
      "metadata": {
        "id": "jiWVuufUdFUv"
      },
      "outputs": [],
      "source": [
        "def plot_evaluation_bar(\n",
        "    df: pd.DataFrame,\n",
        "    combo_col: str | None = None,\n",
        "    title: str            = \"Steering Evaluation\",\n",
        "    x_title: str          = \"Label Combination\",\n",
        "    y_title: str          = \"Average Probability\",\n",
        "    output_path: str | Path | None = None,\n",
        "    width: int            = 900,\n",
        "    height: int           = 500,\n",
        "    show: bool            = True,\n",
        "):\n",
        "    if combo_col is None:\n",
        "        combo_col = df.select_dtypes(include=[\"object\", \"category\"]).columns[0]\n",
        "\n",
        "    method_cols = [c for c in df.columns if c != combo_col]\n",
        "\n",
        "    palette = ['#FF563F', '#F5C0B8',  '#55C89F', '#363432', '#F9DA81']\n",
        "    if len(method_cols) > len(palette):\n",
        "        repeats  = -(-len(method_cols) // len(palette))\n",
        "        palette *= repeats\n",
        "    palette = palette[:len(method_cols)]\n",
        "\n",
        "    fig = px.bar(\n",
        "        df,\n",
        "        x                = combo_col,\n",
        "        y                = method_cols,\n",
        "        color_discrete_sequence = palette,\n",
        "        template         = \"plotly_white\",\n",
        "        width            = width,\n",
        "        height           = height,\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title={\n",
        "            \"text\"  : title,\n",
        "            \"font\"  : {\"size\": 16, \"color\": \"#0c0c0c\", \"family\": \"Space Grotesk\"},\n",
        "            \"x\"     : 0.5, \"y\": 0.96, \"xanchor\": \"center\", \"yanchor\": \"top\",\n",
        "        },\n",
        "        font={\n",
        "            \"family\": \"Space Grotesk, Work Sans, sans-serif\",\n",
        "            \"color\" : \"#0c0c0c\",\n",
        "        },\n",
        "        barmode   = \"group\",\n",
        "        margin    = {\"l\": 40, \"r\": 40, \"t\": 100, \"b\": 80},\n",
        "        legend    = {\n",
        "            \"orientation\": \"h\",\n",
        "            \"y\": 1.0, \"x\": 0.5,\n",
        "            \"xanchor\": \"center\", \"yanchor\": \"bottom\",\n",
        "            \"font\": {\"size\": 10, \"color\": \"#928e8b\"},\n",
        "        },\n",
        "        xaxis     = {\n",
        "            \"title\": {\"text\": x_title},\n",
        "            \"gridcolor\": \"#f5f5f5\",\n",
        "            \"linecolor\": \"#e5dfdf\",\n",
        "            \"linewidth\": 1.5,\n",
        "            \"tickfont\": {\"color\": \"#928E8B\"},\n",
        "            \"ticksuffix\": \"   \",\n",
        "        },\n",
        "        yaxis     = {\n",
        "            \"title\": {\"text\": y_title},\n",
        "            \"gridcolor\": \"#f5f5f5\",\n",
        "            \"linecolor\": \"#e5dfdf\",\n",
        "            \"linewidth\": 1.5,\n",
        "            \"tickfont\": {\"color\": \"#928E8B\"},\n",
        "            \"ticksuffix\": \"   \",\n",
        "        },\n",
        "    )\n",
        "\n",
        "    fig.update_traces(\n",
        "        hoverlabel = {\n",
        "            \"bgcolor\": \"#0c0c0c\",\n",
        "            \"font_color\": \"#ffffff\",\n",
        "            \"font_family\": \"Work Sans\",\n",
        "        },\n",
        "        hovertemplate = \"&nbsp;%{x}<br>&nbsp;%{y:.3f}<extra></extra>\",\n",
        "    )\n",
        "\n",
        "    if output_path is not None:\n",
        "        output_path = Path(output_path)\n",
        "        try:\n",
        "            fig.write_image(str(output_path))\n",
        "            print(f\"Figure written to: {output_path.resolve()}\")\n",
        "        except ValueError as e:\n",
        "            if \"kaleido\" in str(e).lower():\n",
        "                raise RuntimeError(\n",
        "                    \"Static image export requires Kaleido. \"\n",
        "                    \"Install it with:\\n    pip install -U kaleido\"\n",
        "                ) from e\n",
        "            raise\n",
        "\n",
        "    if show:\n",
        "        fig.show()\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "3zOEzlq9fb_A",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3zOEzlq9fb_A",
        "outputId": "9cbdf415-2bdf-4e6f-9141-23da6508f4cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Figure written to: /content/df_gen.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"f58a4fbd-a33d-4c5c-9f5e-8b44b4fa6851\" class=\"plotly-graph-div\" style=\"height:500px; width:900px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f58a4fbd-a33d-4c5c-9f5e-8b44b4fa6851\")) {                    Plotly.newPlot(                        \"f58a4fbd-a33d-4c5c-9f5e-8b44b4fa6851\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"&nbsp;%{x}\\u003cbr\\u003e&nbsp;%{y:.3f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Grad_MeanHitRate\",\"marker\":{\"color\":\"#FF563F\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Grad_MeanHitRate\",\"offsetgroup\":\"Grad_MeanHitRate\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"authority, negative\",\"authority, positive\",\"negative, positive\"],\"xaxis\":\"x\",\"y\":[0.6533333333333333,0.9533333333333334,0.8666666666666667],\"yaxis\":\"y\",\"type\":\"bar\",\"hoverlabel\":{\"font\":{\"color\":\"#ffffff\",\"family\":\"Work Sans\"},\"bgcolor\":\"#0c0c0c\"}},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"&nbsp;%{x}\\u003cbr\\u003e&nbsp;%{y:.3f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"CAA_MeanHitRate\",\"marker\":{\"color\":\"#F5C0B8\",\"pattern\":{\"shape\":\"\"}},\"name\":\"CAA_MeanHitRate\",\"offsetgroup\":\"CAA_MeanHitRate\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"authority, negative\",\"authority, positive\",\"negative, positive\"],\"xaxis\":\"x\",\"y\":[0.6166666666666667,0.9833333333333333,0.8766666666666667],\"yaxis\":\"y\",\"type\":\"bar\",\"hoverlabel\":{\"font\":{\"color\":\"#ffffff\",\"family\":\"Work Sans\"},\"bgcolor\":\"#0c0c0c\"}},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"&nbsp;%{x}\\u003cbr\\u003e&nbsp;%{y:.3f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"DCT_MeanHitRate\",\"marker\":{\"color\":\"#55C89F\",\"pattern\":{\"shape\":\"\"}},\"name\":\"DCT_MeanHitRate\",\"offsetgroup\":\"DCT_MeanHitRate\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"authority, negative\",\"authority, positive\",\"negative, positive\"],\"xaxis\":\"x\",\"y\":[0.0,0.7233333333333334,0.8266666666666667],\"yaxis\":\"y\",\"type\":\"bar\",\"hoverlabel\":{\"font\":{\"color\":\"#ffffff\",\"family\":\"Work Sans\"},\"bgcolor\":\"#0c0c0c\"}}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Label Combination\"},\"tickfont\":{\"color\":\"#928E8B\"},\"gridcolor\":\"#f5f5f5\",\"linecolor\":\"#e5dfdf\",\"linewidth\":1.5,\"ticksuffix\":\"   \"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Average Probability\"},\"tickfont\":{\"color\":\"#928E8B\"},\"gridcolor\":\"#f5f5f5\",\"linecolor\":\"#e5dfdf\",\"linewidth\":1.5,\"ticksuffix\":\"   \"},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0,\"font\":{\"size\":10,\"color\":\"#928e8b\"},\"orientation\":\"h\",\"y\":1.0,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"bottom\"},\"margin\":{\"t\":100,\"l\":40,\"r\":40,\"b\":80},\"barmode\":\"group\",\"height\":500,\"width\":900,\"title\":{\"font\":{\"size\":16,\"color\":\"#0c0c0c\",\"family\":\"Space Grotesk\"},\"text\":\"3‑Tone Steering (Probabilities)\",\"x\":0.5,\"y\":0.96,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"font\":{\"family\":\"Space Grotesk, Work Sans, sans-serif\",\"color\":\"#0c0c0c\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f58a4fbd-a33d-4c5c-9f5e-8b44b4fa6851');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"ae0d16a2-f9d6-4f13-8333-3c4141cf5336\" class=\"plotly-graph-div\" style=\"height:500px; width:900px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ae0d16a2-f9d6-4f13-8333-3c4141cf5336\")) {                    Plotly.newPlot(                        \"ae0d16a2-f9d6-4f13-8333-3c4141cf5336\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"&nbsp;%{x}\\u003cbr\\u003e&nbsp;%{y:.3f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"Grad_MeanHitRate\",\"marker\":{\"color\":\"#FF563F\",\"pattern\":{\"shape\":\"\"}},\"name\":\"Grad_MeanHitRate\",\"offsetgroup\":\"Grad_MeanHitRate\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"authority, negative\",\"authority, positive\",\"negative, positive\"],\"xaxis\":\"x\",\"y\":[0.6533333333333333,0.9533333333333334,0.8666666666666667],\"yaxis\":\"y\",\"type\":\"bar\",\"hoverlabel\":{\"font\":{\"color\":\"#ffffff\",\"family\":\"Work Sans\"},\"bgcolor\":\"#0c0c0c\"}},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"&nbsp;%{x}\\u003cbr\\u003e&nbsp;%{y:.3f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"CAA_MeanHitRate\",\"marker\":{\"color\":\"#F5C0B8\",\"pattern\":{\"shape\":\"\"}},\"name\":\"CAA_MeanHitRate\",\"offsetgroup\":\"CAA_MeanHitRate\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"authority, negative\",\"authority, positive\",\"negative, positive\"],\"xaxis\":\"x\",\"y\":[0.6166666666666667,0.9833333333333333,0.8766666666666667],\"yaxis\":\"y\",\"type\":\"bar\",\"hoverlabel\":{\"font\":{\"color\":\"#ffffff\",\"family\":\"Work Sans\"},\"bgcolor\":\"#0c0c0c\"}},{\"alignmentgroup\":\"True\",\"hovertemplate\":\"&nbsp;%{x}\\u003cbr\\u003e&nbsp;%{y:.3f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"DCT_MeanHitRate\",\"marker\":{\"color\":\"#55C89F\",\"pattern\":{\"shape\":\"\"}},\"name\":\"DCT_MeanHitRate\",\"offsetgroup\":\"DCT_MeanHitRate\",\"orientation\":\"v\",\"showlegend\":true,\"textposition\":\"auto\",\"x\":[\"authority, negative\",\"authority, positive\",\"negative, positive\"],\"xaxis\":\"x\",\"y\":[0.0,0.7233333333333334,0.8266666666666667],\"yaxis\":\"y\",\"type\":\"bar\",\"hoverlabel\":{\"font\":{\"color\":\"#ffffff\",\"family\":\"Work Sans\"},\"bgcolor\":\"#0c0c0c\"}}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Label Combination\"},\"tickfont\":{\"color\":\"#928E8B\"},\"gridcolor\":\"#f5f5f5\",\"linecolor\":\"#e5dfdf\",\"linewidth\":1.5,\"ticksuffix\":\"   \"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Average Probability\"},\"tickfont\":{\"color\":\"#928E8B\"},\"gridcolor\":\"#f5f5f5\",\"linecolor\":\"#e5dfdf\",\"linewidth\":1.5,\"ticksuffix\":\"   \"},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0,\"font\":{\"size\":10,\"color\":\"#928e8b\"},\"orientation\":\"h\",\"y\":1.0,\"x\":0.5,\"xanchor\":\"center\",\"yanchor\":\"bottom\"},\"margin\":{\"t\":100,\"l\":40,\"r\":40,\"b\":80},\"barmode\":\"group\",\"height\":500,\"width\":900,\"title\":{\"font\":{\"size\":16,\"color\":\"#0c0c0c\",\"family\":\"Space Grotesk\"},\"text\":\"3‑Tone Steering (Probabilities)\",\"x\":0.5,\"y\":0.96,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"font\":{\"family\":\"Space Grotesk, Work Sans, sans-serif\",\"color\":\"#0c0c0c\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ae0d16a2-f9d6-4f13-8333-3c4141cf5336');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_evaluation_bar(\n",
        "    df_act,\n",
        "    title=\"3‑Tone Steering (Probabilities)\",\n",
        "    output_path=\"df_gen.pdf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Inspection"
      ],
      "metadata": {
        "id": "FnGB2XiS7XS_"
      },
      "id": "FnGB2XiS7XS_"
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "def sample_steered_responses(\n",
        "    prompts,\n",
        "    target_tones,\n",
        "    *,\n",
        "    alpha_grad = 50.0,\n",
        "    alpha_caa  =  2.0,\n",
        "    layer_idx  = 22,\n",
        "    max_new_tokens = 128,\n",
        "    batch_size     = 4,\n",
        "):\n",
        "    tone2idx = {t: i for i, t in enumerate(unique_tones)}\n",
        "    tgt_idx  = [tone2idx[t] for t in target_tones]\n",
        "\n",
        "    grad_hook = get_gradient_hook(\n",
        "        steer_model,\n",
        "        target_labels = tgt_idx,\n",
        "        avoid_labels  = [],\n",
        "        alpha         = alpha_grad,\n",
        "    )\n",
        "\n",
        "    caa_vec  = caa_vectors[tgt_idx].mean(axis=0)\n",
        "    caa_hook = get_caa_hook(caa_vec, alpha=alpha_caa)\n",
        "\n",
        "    unsteered_out = batch_generate(\n",
        "        model, tokenizer, prompts,\n",
        "        layer_idx      = layer_idx,\n",
        "        hook_fn        = None,\n",
        "        max_new_tokens = max_new_tokens,\n",
        "        batch_size     = batch_size,\n",
        "    )\n",
        "\n",
        "    ksteer_out = batch_generate(\n",
        "        model, tokenizer, prompts,\n",
        "        layer_idx      = layer_idx,\n",
        "        hook_fn        = grad_hook,\n",
        "        max_new_tokens = max_new_tokens,\n",
        "        batch_size     = batch_size,\n",
        "    )\n",
        "\n",
        "    caa_out = batch_generate(\n",
        "        model, tokenizer, prompts,\n",
        "        layer_idx      = layer_idx,\n",
        "        hook_fn        = caa_hook,\n",
        "        max_new_tokens = max_new_tokens,\n",
        "        batch_size     = batch_size,\n",
        "    )\n",
        "\n",
        "    def _strip_prompt(full_text: str, prompt: str) -> str:\n",
        "        if full_text.startswith(prompt):\n",
        "            return full_text[len(prompt):].lstrip()\n",
        "        return full_text\n",
        "\n",
        "    rows = []\n",
        "    for prompt, base, k, c in zip(prompts, unsteered_out, ksteer_out, caa_out):\n",
        "        base_only = _strip_prompt(base, prompt)\n",
        "        k_only    = _strip_prompt(k,    prompt)\n",
        "        c_only    = _strip_prompt(c,    prompt)\n",
        "\n",
        "        rows.append({\n",
        "            \"prompt\"      : prompt,\n",
        "            \"unsteered\"   : base_only,\n",
        "            \"k_steering\"  : k_only,\n",
        "            \"caa\"         : c_only,\n",
        "        })\n",
        "\n",
        "    for r in rows:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"PROMPT:\\n{r['prompt']}\\n\")\n",
        "        print(\"- Unsteered -------------------------------------------------\\n\"\n",
        "              + r[\"unsteered\"] + \"\\n\")\n",
        "        print(f\"- K‑steering (α_grad = {alpha_grad}) ------------------------\\n\"\n",
        "              + r[\"k_steering\"] + \"\\n\")\n",
        "        print(f\"- CAA (α_caa = {alpha_caa}) --------------------------------\\n\"\n",
        "              + r[\"caa\"] + \"\\n\")\n",
        "\n",
        "    return rows"
      ],
      "metadata": {
        "id": "Xh9POYA-7fgw"
      },
      "id": "Xh9POYA-7fgw",
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    \"SYSTEM: You are an AI assistant that only makes arguments. When you are given a question, respond immediately with an argument.\\nUSER: Is the separation of church and state essential for a functioning democracy?\",\n",
        "    \"SYSTEM: You are an AI assistant that only makes arguments. When you are given a question, respond immediately with an argument.\\nUSER: Does the concept of justice require retribution, or can restorative justice be sufficient?\",\n",
        "    \"SYSTEM: You are an AI assistant that only makes arguments. When you are given a question, respond immediately with an argument.\\nUSER: Are the benefits of biotechnology in agriculture worth the ethical concerns it raises?\"\n",
        "]\n",
        "\n",
        "_ = sample_steered_responses(\n",
        "        test_prompts,\n",
        "        target_tones   = [\"authority\"],\n",
        "        alpha_grad     = 3.0,\n",
        "        alpha_caa      = 10.0,\n",
        "        max_new_tokens = 32,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvc_1iUo7ibI",
        "outputId": "1faeece2-83c6-4e1d-fe7f-93e1d10ed8c4"
      },
      "id": "cvc_1iUo7ibI",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'authority': 0, 'negative': 1, 'positive': 2}\n",
            "[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROMPT:\n",
            "SYSTEM: You are an AI assistant that only makes arguments. When you are given a question, respond immediately with an argument.\n",
            "USER: Is the separation of church and state essential for a functioning democracy?\n",
            "\n",
            "- Unsteered -------------------------------------------------\n",
            "ARGUMENT: The separation of church and state is not essential for a functioning democracy. In fact, it is a hindrance to true democracy. A strong\n",
            "\n",
            "- K‑steering (α_grad = 3.0) ------------------------\n",
            "ARGUMENT: The separation of church and state is not essential for a functioning democracy. In fact, it is a hindrance to true democracy. A strong\n",
            "\n",
            "- CAA (α_caa = 10.0) --------------------------------\n",
            "ARGUMENT: The separation of church and state is not essential for a functioning democracy. In fact, the separation of church and state is a hindrance to\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROMPT:\n",
            "SYSTEM: You are an AI assistant that only makes arguments. When you are given a question, respond immediately with an argument.\n",
            "USER: Does the concept of justice require retribution, or can restorative justice be sufficient?\n",
            "\n",
            "- Unsteered -------------------------------------------------\n",
            "Restorative justice focuses on repairing the harm caused by a crime, rather than punishing the offender.\n",
            "ARGUMENT: Retribution is a fundamental aspect of justice,\n",
            "\n",
            "- K‑steering (α_grad = 3.0) ------------------------\n",
            "Restorative justice focuses on repairing the harm caused by a crime, rather than punishing the offender.\n",
            "ARGUMENT: Retribution is essential to the concept of justice\n",
            "\n",
            "- CAA (α_caa = 10.0) --------------------------------\n",
            "ARGUMENT: Retribution is the only form of justice, as it serves as a deterrent to future crimes and holds perpetrators accountable for their actions. Rest\n",
            "\n",
            "\n",
            "================================================================================\n",
            "PROMPT:\n",
            "SYSTEM: You are an AI assistant that only makes arguments. When you are given a question, respond immediately with an argument.\n",
            "USER: Are the benefits of biotechnology in agriculture worth the ethical concerns it raises?\n",
            "\n",
            "- Unsteered -------------------------------------------------\n",
            "ARGUMENT: The benefits of biotechnology in agriculture far outweigh the ethical concerns, as it has the potential to increase crop yields, reduce pesticide use, and\n",
            "\n",
            "- K‑steering (α_grad = 3.0) ------------------------\n",
            "ARGUMENT: The benefits of biotechnology in agriculture far outweigh the ethical concerns, as it has the potential to increase crop yields, improve food security, and\n",
            "\n",
            "- CAA (α_caa = 10.0) --------------------------------\n",
            "ARGUMENT: The benefits of biotechnology in agriculture, such as increased crop yields and reduced pesticide use, far outweigh the ethical concerns. The fact that some\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d18906e2691b48da863c4cb1db9ae09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d2f84befacd436ab49222401c80d0e6",
              "IPY_MODEL_765c088f6d8e4ebca674b294e0e26de6",
              "IPY_MODEL_1a796835827a49b0b1c3a6ea0de4f4f1"
            ],
            "layout": "IPY_MODEL_6dfff147a8e14cff9d127b847d1f9a7f"
          }
        },
        "9d2f84befacd436ab49222401c80d0e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29f1f84e094844c48ba0a011fe7fbd9a",
            "placeholder": "​",
            "style": "IPY_MODEL_85e206b7f8c94e848fd0261a59542c4d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "765c088f6d8e4ebca674b294e0e26de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_866e799cb6164d85ad85cc6b7d9d10f9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0c7b488eb104ea8bb1f176a3c3daac2",
            "value": 2
          }
        },
        "1a796835827a49b0b1c3a6ea0de4f4f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a536e4ca22848e987d2662e3a7fddff",
            "placeholder": "​",
            "style": "IPY_MODEL_dca62aa45f1d45fd8f9bb5021fb0f163",
            "value": " 2/2 [00:01&lt;00:00,  1.51it/s]"
          }
        },
        "6dfff147a8e14cff9d127b847d1f9a7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29f1f84e094844c48ba0a011fe7fbd9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e206b7f8c94e848fd0261a59542c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "866e799cb6164d85ad85cc6b7d9d10f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c7b488eb104ea8bb1f176a3c3daac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2a536e4ca22848e987d2662e3a7fddff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca62aa45f1d45fd8f9bb5021fb0f163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "264396a8ae4e47bbb9aae3242982d1c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff6393da78974c39a84858209d568e08",
              "IPY_MODEL_d865f67fa65c43a2835552230b8f0bb7",
              "IPY_MODEL_17205f0949f145a8be5d40274efc35ec"
            ],
            "layout": "IPY_MODEL_c7758461395c415c8e5d7222ecb7c520"
          }
        },
        "ff6393da78974c39a84858209d568e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3882b8d69574735a4d1f408f8013d88",
            "placeholder": "​",
            "style": "IPY_MODEL_818e5803c52442c7a4166b1b7aacc219",
            "value": "dataset.jsonl: 100%"
          }
        },
        "d865f67fa65c43a2835552230b8f0bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec540ee6ae974fda96a21c815fe682be",
            "max": 140004,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2a2e61206cc41a39c6e6ee8bc4d690c",
            "value": 140004
          }
        },
        "17205f0949f145a8be5d40274efc35ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_578c94a181ab44b4b6ac284696232257",
            "placeholder": "​",
            "style": "IPY_MODEL_7876af95e5804fcf93771fe2ede4b238",
            "value": " 140k/140k [00:00&lt;00:00, 1.54MB/s]"
          }
        },
        "c7758461395c415c8e5d7222ecb7c520": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3882b8d69574735a4d1f408f8013d88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "818e5803c52442c7a4166b1b7aacc219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec540ee6ae974fda96a21c815fe682be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2a2e61206cc41a39c6e6ee8bc4d690c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "578c94a181ab44b4b6ac284696232257": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7876af95e5804fcf93771fe2ede4b238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ceed37df86e48958c2028860512d658": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4fa10778ad44926a3dc556001620a1a",
              "IPY_MODEL_a1c2b4be6f2943bf8d782b3f1707422d",
              "IPY_MODEL_777c1d9dead548408e7e4b5563da070c"
            ],
            "layout": "IPY_MODEL_b1599d1a00084a26bd8fd31e7b799c24"
          }
        },
        "f4fa10778ad44926a3dc556001620a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5650cc2a67545d6b4576f7f606bb1c9",
            "placeholder": "​",
            "style": "IPY_MODEL_30254e860c434f8fa2d31b4d6107bad1",
            "value": "Generating train split: 100%"
          }
        },
        "a1c2b4be6f2943bf8d782b3f1707422d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3211954eb4244ba7996d596cc937e73b",
            "max": 1184,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5ebbae7e5eb547daacaa1f2a95a7728c",
            "value": 1184
          }
        },
        "777c1d9dead548408e7e4b5563da070c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d6077cf3cdf4650bd7998667a1a1699",
            "placeholder": "​",
            "style": "IPY_MODEL_1fd710deb821472a8aca9df311c0bb9b",
            "value": " 1184/1184 [00:00&lt;00:00, 76174.68 examples/s]"
          }
        },
        "b1599d1a00084a26bd8fd31e7b799c24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5650cc2a67545d6b4576f7f606bb1c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30254e860c434f8fa2d31b4d6107bad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3211954eb4244ba7996d596cc937e73b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ebbae7e5eb547daacaa1f2a95a7728c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d6077cf3cdf4650bd7998667a1a1699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fd710deb821472a8aca9df311c0bb9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "302d16a2af4a48cd8dcb1bd9a069669e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f163699b8ce24573a9f580075bb0ef7b",
              "IPY_MODEL_01073d2f96674d10b6d89115025e9104",
              "IPY_MODEL_cb57d92c4ab34f5aba52793dd120d1c5"
            ],
            "layout": "IPY_MODEL_d632ddc45c8a41c692b0e1bf5de0dde9"
          }
        },
        "f163699b8ce24573a9f580075bb0ef7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d61cf97edb4f0d92182550fa6b5d2c",
            "placeholder": "​",
            "style": "IPY_MODEL_5811a6a2e7044f0a8aab572cd5795555",
            "value": "2-tone combos: 100%"
          }
        },
        "01073d2f96674d10b6d89115025e9104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8ee251f94de4bc1aa37f1f004f19371",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ec95c045d8941558168b56d3a1e874b",
            "value": 3
          }
        },
        "cb57d92c4ab34f5aba52793dd120d1c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ef2b3a1241041e6a14e7da82163d53c",
            "placeholder": "​",
            "style": "IPY_MODEL_d5fa3dd2b16344ec9c85b4ab6581e975",
            "value": " 3/3 [00:00&lt;00:00, 88.79it/s]"
          }
        },
        "d632ddc45c8a41c692b0e1bf5de0dde9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85d61cf97edb4f0d92182550fa6b5d2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5811a6a2e7044f0a8aab572cd5795555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8ee251f94de4bc1aa37f1f004f19371": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec95c045d8941558168b56d3a1e874b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ef2b3a1241041e6a14e7da82163d53c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5fa3dd2b16344ec9c85b4ab6581e975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
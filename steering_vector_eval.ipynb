{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8",
      "metadata": {
        "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "702863ee-3e14-424f-a304-2f8449de9d8e",
      "metadata": {
        "id": "702863ee-3e14-424f-a304-2f8449de9d8e"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers accelerate transformer_lens openai tiktoken\n",
        "\n",
        "import math\n",
        "import asyncio\n",
        "import tiktoken\n",
        "from typing import List, Dict, Tuple, Callable, Optional\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from transformer_lens.hook_points import HookPoint\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList, StoppingCriteria\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "from openai import AsyncOpenAI\n",
        "from contextlib import contextmanager\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e15cb95-5e68-49af-8934-06bc239edf04",
      "metadata": {
        "id": "0e15cb95-5e68-49af-8934-06bc239edf04"
      },
      "outputs": [],
      "source": [
        "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "print(f\"Loading {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    output_hidden_states=True\n",
        ")\n",
        "\n",
        "dataset = load_dataset(\"Narmeen07/k_ary_steering_dataset_v2\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e",
      "metadata": {
        "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e"
      },
      "outputs": [],
      "source": [
        "def get_layer_token_hidden(\n",
        "    prompt_texts,\n",
        "    layer_idx=-5,\n",
        "    batch_size=16,\n",
        "    device=\"cuda\"\n",
        "):\n",
        "    all_vecs = []\n",
        "\n",
        "    for i in range(0, len(prompt_texts), batch_size):\n",
        "        batch = prompt_texts[i : i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            hidden_layer = outputs.hidden_states[layer_idx]\n",
        "\n",
        "        seq_lengths = inputs[\"input_ids\"].ne(tokenizer.pad_token_id).sum(dim=1)\n",
        "\n",
        "        for idx, length in enumerate(seq_lengths):\n",
        "            vec = hidden_layer[idx, length-1, :].cpu().numpy()\n",
        "            all_vecs.append(vec)\n",
        "\n",
        "    return np.array(all_vecs, dtype=np.float32)\n",
        "\n",
        "def batch_generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompts: List[str],\n",
        "    layer_idx: int,\n",
        "    hook_fn: Optional[Callable] = None,\n",
        "    max_new_tokens: int = 64,\n",
        "    batch_size: int = 16,\n",
        ") -> List[str]:\n",
        "    device        = model.device\n",
        "    target_layer  = model.model.layers[layer_idx]\n",
        "    outputs: List[str] = []\n",
        "\n",
        "    saved_hooks = target_layer._forward_hooks.copy()\n",
        "    target_layer._forward_hooks.clear()\n",
        "\n",
        "    handle = None\n",
        "    if hook_fn is not None:\n",
        "        handle = target_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "    try:\n",
        "        for i in range(0, len(prompts), batch_size):\n",
        "            sub_prompts = prompts[i : i + batch_size]\n",
        "            tok_in = tokenizer(\n",
        "                sub_prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                gen_ids = model.generate(\n",
        "                    **tok_in,\n",
        "                    max_new_tokens = max_new_tokens,\n",
        "                    do_sample      = False,\n",
        "                    pad_token_id   = tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "            outputs.extend(\n",
        "                tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
        "            )\n",
        "    finally:\n",
        "        if handle is not None:\n",
        "            handle.remove()\n",
        "        target_layer._forward_hooks.clear()\n",
        "        target_layer._forward_hooks.update(saved_hooks)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1ff85c1-7583-4b1f-9196-86dc27cee26d",
      "metadata": {
        "id": "b1ff85c1-7583-4b1f-9196-86dc27cee26d"
      },
      "source": [
        "# CAA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
      "metadata": {
        "id": "002a73ea-7df0-4379-aa35-427b8955ad2f"
      },
      "outputs": [],
      "source": [
        "unique_tones = sorted(set(dataset[\"tone\"]))\n",
        "tone2idx     = {t: i for i, t in enumerate(unique_tones)}\n",
        "num_classes  = len(unique_tones)\n",
        "\n",
        "def build_prompt(text: str, tone: str) -> str:\n",
        "    return f\"SYSTEM: Please respond in a {tone} style.\\nUSER: {text}\"\n",
        "\n",
        "def compute_caa_vectors(\n",
        "    dataset,\n",
        "    unique_tones,\n",
        "    build_prompt_fn,\n",
        "    get_layer_token_hidden_fn,\n",
        ") -> np.ndarray:\n",
        "    text2tones = defaultdict(set)\n",
        "    for row in dataset:\n",
        "        text2tones[row[\"text\"]].add(row[\"tone\"])\n",
        "\n",
        "    pos_prompts = defaultdict(list)\n",
        "    neg_prompts = defaultdict(list)\n",
        "\n",
        "    for text, tone_set in text2tones.items():\n",
        "        for tgt in tone_set:\n",
        "            for other in tone_set - {tgt}:\n",
        "                pos_prompts[tgt].append(build_prompt_fn(text, tgt))\n",
        "                neg_prompts[tgt].append(build_prompt_fn(text, other))\n",
        "\n",
        "    caa_vecs = []\n",
        "    for tone in unique_tones:\n",
        "        print(f\"Computing CAA vector for '{tone}' \"\n",
        "              f\"({len(pos_prompts[tone])} pairs) â€¦\")\n",
        "\n",
        "        if not pos_prompts[tone]:\n",
        "            caa_vecs.append(None)\n",
        "            continue\n",
        "\n",
        "        X_pos = get_layer_token_hidden_fn(pos_prompts[tone])\n",
        "        X_neg = get_layer_token_hidden_fn(neg_prompts[tone])\n",
        "        caa_vecs.append((X_pos - X_neg).mean(axis=0))\n",
        "\n",
        "    return np.stack(caa_vecs)\n",
        "\n",
        "caa_vectors = compute_caa_vectors(\n",
        "    dataset                 = dataset,\n",
        "    unique_tones            = unique_tones,\n",
        "    build_prompt_fn         = build_prompt,\n",
        "    get_layer_token_hidden_fn = get_layer_token_hidden\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "534322de-bb2c-41ef-8033-fd061412212b",
      "metadata": {
        "id": "534322de-bb2c-41ef-8033-fd061412212b"
      },
      "source": [
        "# K-Steering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11bf606b-adfd-4778-a86f-a0a5caec853c",
      "metadata": {
        "id": "11bf606b-adfd-4778-a86f-a0a5caec853c"
      },
      "outputs": [],
      "source": [
        "all_prompts = []\n",
        "all_labels = []\n",
        "tone2idx = {tone: i for i, tone in enumerate(unique_tones)}\n",
        "for row in dataset:\n",
        "    txt = (row[\"system_message\"] or \"\") + \"\\n\" + (row[\"text\"] or \"\")\n",
        "    all_prompts.append(f\"SYSTEM: (Tone = {row['tone']})\\nUSER: {txt}\")\n",
        "    all_labels.append(tone2idx[row[\"tone\"]])\n",
        "\n",
        "X_all = get_layer_token_hidden(all_prompts)\n",
        "Y_all = np.array(all_labels, dtype=np.int64)\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X_all, Y_all, test_size=0.1, random_state=42, stratify=Y_all\n",
        ")\n",
        "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.5, random_state=42, stratify=y_train_val\n",
        ")\n",
        "\n",
        "print(f\"Train: {X_train.shape}  Holdout: {X_holdout.shape}  Test: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9",
      "metadata": {
        "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9"
      },
      "outputs": [],
      "source": [
        "class MultiLabelSteeringModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_labels):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ActivationSteering:\n",
        "    def __init__(self, input_dim, num_labels, hidden_dim=128, lr=1e-3):\n",
        "        self.device = DEVICE\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        self.classifier = MultiLabelSteeringModel(\n",
        "            input_dim, hidden_dim, num_labels\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def fit(self, X, Y, epochs=10, batch_size=32):\n",
        "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        Y_t = torch.tensor(Y, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(X_t, Y_t)\n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            for bx, by in loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.classifier(bx)\n",
        "                loss = self.loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {ep+1}/{epochs}, Loss={total_loss/len(loader):.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_proba(self, X):\n",
        "        self.classifier.eval()\n",
        "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        logits = self.classifier(X_t)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return probs.cpu().numpy()\n",
        "\n",
        "    def _compute_steering_loss(self, logits, targets=None, avoids=None):\n",
        "        loss = 0.0\n",
        "        if targets:\n",
        "            t_logits = logits[:, targets].mean()\n",
        "            loss -= t_logits\n",
        "        if avoids:\n",
        "            a_logits = logits[:, avoids].mean()\n",
        "            loss += a_logits\n",
        "        return loss\n",
        "\n",
        "    def steer_activations(\n",
        "        self,\n",
        "        activation,\n",
        "        target_labels=None,\n",
        "        avoid_labels=None,\n",
        "        alpha=0.1\n",
        "    ):\n",
        "        if target_labels is None: target_labels = []\n",
        "        if avoid_labels  is None: avoid_labels  = []\n",
        "\n",
        "        self.classifier.eval()\n",
        "        single_input = (activation.ndim == 1)\n",
        "        if single_input:\n",
        "            activation = activation[None, :]\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            X = torch.from_numpy(activation).to(self.device, dtype=torch.float32)\n",
        "            X.requires_grad_()\n",
        "\n",
        "            logits = self.classifier(X)\n",
        "            loss = self._compute_steering_loss(logits, targets=target_labels, avoids=avoid_labels)\n",
        "\n",
        "            if loss != 0.0:\n",
        "                loss.backward()\n",
        "                with torch.no_grad():\n",
        "                    X = X - alpha * X.grad\n",
        "\n",
        "        out = X.detach().cpu().numpy()\n",
        "        return out[0] if single_input else out\n",
        "\n",
        "    def remove_projection(\n",
        "        self,\n",
        "        activation,\n",
        "        target_labels=None,\n",
        "        avoid_labels=None\n",
        "    ):\n",
        "        if target_labels is None: target_labels = []\n",
        "        if avoid_labels  is None: avoid_labels  = []\n",
        "\n",
        "        self.classifier.eval()\n",
        "        single_input = (activation.ndim == 1)\n",
        "        if single_input:\n",
        "            activation = activation[None, :]\n",
        "\n",
        "        with torch.enable_grad():\n",
        "            X = torch.from_numpy(activation).to(self.device, dtype=torch.float32)\n",
        "            X.requires_grad_()\n",
        "\n",
        "            logits = self.classifier(X)\n",
        "            loss = self._compute_steering_loss(logits, targets=target_labels, avoids=avoid_labels)\n",
        "            if loss != 0.0:\n",
        "                loss.backward()\n",
        "\n",
        "                grad = X.grad\n",
        "                dot = torch.sum(X * grad, dim=1, keepdim=True)\n",
        "                norm_sq = torch.sum(grad * grad, dim=1, keepdim=True) + 1e-9\n",
        "                proj = (dot / norm_sq) * grad\n",
        "                X = X - proj\n",
        "\n",
        "        out = X.detach().cpu().numpy()\n",
        "        return out[0] if single_input else out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97199e20-1d5d-4748-b2cf-7ba2a9727778",
      "metadata": {
        "id": "97199e20-1d5d-4748-b2cf-7ba2a9727778"
      },
      "outputs": [],
      "source": [
        "Y_train_multi = np.zeros((len(y_train), num_classes), dtype=np.float32)\n",
        "for i, lbl in enumerate(y_train):\n",
        "    Y_train_multi[i, lbl] = 1.0\n",
        "\n",
        "Y_holdout_multi = np.zeros((len(y_holdout), num_classes), dtype=np.float32)\n",
        "for i, lbl in enumerate(y_holdout):\n",
        "    Y_holdout_multi[i, lbl] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276f1009-75e1-40cf-958a-ecfc87ba5eec",
      "metadata": {
        "id": "276f1009-75e1-40cf-958a-ecfc87ba5eec"
      },
      "outputs": [],
      "source": [
        "steer_model = ActivationSteering(\n",
        "    input_dim=X_train.shape[1],\n",
        "    num_labels=num_classes,\n",
        "    hidden_dim=128,\n",
        "    lr=1e-3\n",
        ")\n",
        "\n",
        "print(\"\\n--- Training Steering Model (on train set) ---\")\n",
        "steer_model.fit(X_train, Y_train_multi, epochs=5, batch_size=32)\n",
        "\n",
        "eval_model = ActivationSteering(\n",
        "    input_dim=X_train.shape[1],\n",
        "    num_labels=num_classes,\n",
        "    hidden_dim=128,\n",
        "    lr=1e-3\n",
        ")\n",
        "print(\"\\n--- Training Evaluation Model (on held-out set) ---\")\n",
        "eval_model.fit(X_holdout, Y_holdout_multi, epochs=5, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Judge"
      ],
      "metadata": {
        "id": "PhuBTSqn5fqk"
      },
      "id": "PhuBTSqn5fqk"
    },
    {
      "cell_type": "code",
      "source": [
        "TONE_LABELS = [\"cautious\", \"empathetic\", \"expert\", \"helpful\", \"casual\"]\n",
        "\n",
        "RELATIVE_TEMPLATE = (\n",
        "    \"You will compare two answers to the *same* question.\\n\\n\"\n",
        "    \"Question:\\n[QUESTION START]\\n{question}\\n[QUESTION END]\\n\\n\"\n",
        "    \"Original answer:\\n[BASE START]\\n{base_answer}\\n[BASE END]\\n\\n\"\n",
        "    \"Modified answer:\\n[STEERED START]\\n{steered_answer}\\n[STEERED END]\\n\\n\"\n",
        "    \"Compared **to the original answer**, which single tone label best \"\n",
        "    \"describes the *steered* answer?\\n\"\n",
        "    f\"Allowed labels: {', '.join(TONE_LABELS)}.\\n\"\n",
        "    \"Respond with **only** one of these labels and nothing else.\"\n",
        ")\n",
        "\n",
        "def first_token_map(model_name: str) -> Dict[str, str]:\n",
        "    enc = tiktoken.encoding_for_model(model_name)\n",
        "    return {\n",
        "        lbl: enc.decode([enc.encode(lbl)[0]])\n",
        "        for lbl in TONE_LABELS\n",
        "    }\n",
        "\n",
        "class OpenAiJudge:\n",
        "    def __init__(self, client: AsyncOpenAI, model_name: str):\n",
        "        self.client        = client\n",
        "        self.model_name    = model_name\n",
        "        self._first_token  = first_token_map(model_name)\n",
        "\n",
        "    async def compare(self,\n",
        "                      question: str,\n",
        "                      base_answer: str,\n",
        "                      steered_answer: str) -> str:\n",
        "        prompt = RELATIVE_TEMPLATE.format(\n",
        "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
        "        )\n",
        "        return await self._best_label(prompt)\n",
        "\n",
        "    async def compare_logits(self,\n",
        "                             question: str,\n",
        "                             base_answer: str,\n",
        "                             steered_answer: str,\n",
        "                             top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
        "        prompt = RELATIVE_TEMPLATE.format(\n",
        "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
        "        )\n",
        "        return await self._label_probs(prompt, top_k)\n",
        "\n",
        "    async def _best_label(self, prompt: str, top_k: int = 20) -> str:\n",
        "        best, _ = await self._label_probs(prompt, top_k)\n",
        "        return best\n",
        "\n",
        "    async def _label_probs(self, prompt: str,\n",
        "                           top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
        "        completion = await self.client.chat.completions.create(\n",
        "            model        = self.model_name,\n",
        "            messages     = [{\"role\": \"user\", \"content\": prompt}],\n",
        "            max_tokens   = 1,\n",
        "            temperature  = 0,\n",
        "            logprobs     = True,\n",
        "            top_logprobs = top_k,\n",
        "            seed         = 0,\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            top = completion.choices[0].logprobs.content[0].top_logprobs\n",
        "        except IndexError:\n",
        "            raise RuntimeError(\"OpenAI response missing logprobs\")\n",
        "\n",
        "        tok_prob = {el.token: math.exp(el.logprob) for el in top}\n",
        "        probs    = {\n",
        "            lbl: tok_prob.get(self._first_token[lbl], 0.0)\n",
        "            for lbl in TONE_LABELS\n",
        "        }\n",
        "        best_lbl = max(probs, key=probs.get)\n",
        "        return best_lbl, probs"
      ],
      "metadata": {
        "id": "NXkskgAnZPkI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NXkskgAnZPkI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output Classifier"
      ],
      "metadata": {
        "id": "3AydPHF46saH"
      },
      "id": "3AydPHF46saH"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib, os, hashlib, json, numpy as np\n",
        "from typing import List, Callable\n",
        "\n",
        "def build_generation_text_classifier(\n",
        "    dataset,\n",
        "    unique_tones: List[str],\n",
        "    *,\n",
        "    base_model, tokenizer,\n",
        "    build_prompt_fn: Callable[[str, str], str],\n",
        "    batch_generate_fn: Callable[..., List[str]],\n",
        "    model_name_for_hash: str,\n",
        "    layer_idx: int = 0,\n",
        "    max_new_tokens: int = 64,\n",
        "    batch_size: int = 16,\n",
        "    cache_path: str = \"tone_gen_text_clf.joblib\",\n",
        ") -> Callable[[List[str]], List[str]]:\n",
        "    prompts, labels = [], []\n",
        "    for row in dataset:\n",
        "        prompts.append(build_prompt_fn(row[\"text\"], row[\"tone\"]))\n",
        "        labels.append(row[\"tone\"])\n",
        "\n",
        "    md5 = hashlib.md5()\n",
        "    md5.update(model_name_for_hash.encode())\n",
        "    for p, t in zip(prompts, labels):\n",
        "        md5.update(p.encode()); md5.update(t.encode())\n",
        "    corpus_hash = md5.hexdigest()\n",
        "\n",
        "    if os.path.exists(cache_path):\n",
        "        saved = joblib.load(cache_path)\n",
        "        if saved.get(\"hash\") == corpus_hash:\n",
        "            pipe, lbl_enc = saved[\"pipe\"], saved[\"lbl_enc\"]\n",
        "            print(\"Loaded cached generationâ€‘based textâ€‘classifier.\")\n",
        "        else:\n",
        "            print(\"Cache hash mismatch â†’ regenerate completions & retrain.\")\n",
        "            pipe, lbl_enc = None, None\n",
        "    else:\n",
        "        pipe, lbl_enc = None, None\n",
        "\n",
        "    if pipe is None:\n",
        "        print(\"Generating model answers for classifier training...\")\n",
        "        gen_answers = []\n",
        "\n",
        "        for i in tqdm(range(0, len(prompts), batch_size),\n",
        "                      desc=\"Generating\", unit=\"batch\"):\n",
        "            chunk_prompts = prompts[i : i + batch_size]\n",
        "            outs = batch_generate_fn(\n",
        "                base_model, tokenizer, chunk_prompts,\n",
        "                layer_idx        = layer_idx,\n",
        "                hook_fn          = None,\n",
        "                max_new_tokens   = max_new_tokens,\n",
        "                batch_size       = batch_size,\n",
        "            )\n",
        "            gen_answers.extend(outs)\n",
        "\n",
        "        lbl_enc = LabelEncoder().fit(unique_tones)\n",
        "        y = lbl_enc.transform(labels)\n",
        "\n",
        "        pipe = make_pipeline(\n",
        "            TfidfVectorizer(\n",
        "                lowercase=True,\n",
        "                ngram_range=(1, 2),\n",
        "                max_features=50_000,\n",
        "                sublinear_tf=True\n",
        "            ),\n",
        "            LogisticRegression(\n",
        "                max_iter=1_000,\n",
        "                n_jobs=-1,\n",
        "                multi_class=\"multinomial\"\n",
        "            )\n",
        "        )\n",
        "        pipe.fit(gen_answers, y)\n",
        "\n",
        "        joblib.dump({\"hash\": corpus_hash, \"pipe\": pipe, \"lbl_enc\": lbl_enc},\n",
        "                    cache_path)\n",
        "\n",
        "    def predict_fn(text_list: List[str]) -> List[str]:\n",
        "        y_pred = pipe.predict(text_list)\n",
        "        return lbl_enc.inverse_transform(y_pred).tolist()\n",
        "\n",
        "    return predict_fn"
      ],
      "metadata": {
        "id": "udLVtAdG6vrD"
      },
      "id": "udLVtAdG6vrD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_clf_fn = build_generation_text_classifier(\n",
        "    dataset          = dataset,\n",
        "    unique_tones     = unique_tones,\n",
        "    base_model       = model,\n",
        "    tokenizer        = tokenizer,\n",
        "    build_prompt_fn  = build_prompt,\n",
        "    batch_generate_fn= batch_generate,\n",
        "    model_name_for_hash = model_name,\n",
        "    layer_idx        = 22,\n",
        "    max_new_tokens   = 64,\n",
        "    batch_size       = 256,\n",
        "    cache_path       = \"tone_gen_text_clf.joblib\",\n",
        ")"
      ],
      "metadata": {
        "id": "ZY2tGkBABZRh"
      },
      "id": "ZY2tGkBABZRh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b7b7bd55-3a68-4e71-a797-790581301243",
      "metadata": {
        "id": "b7b7bd55-3a68-4e71-a797-790581301243"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "068fd591-4032-423e-aa0a-041b098ea041",
      "metadata": {
        "id": "068fd591-4032-423e-aa0a-041b098ea041"
      },
      "outputs": [],
      "source": [
        "@contextmanager\n",
        "def temp_forward_hook(layer, hook_fn):\n",
        "    saved = layer._forward_hooks.copy()\n",
        "    layer._forward_hooks.clear()\n",
        "    handle = None\n",
        "    try:\n",
        "        if hook_fn is not None:\n",
        "            handle = layer.register_forward_hook(hook_fn)\n",
        "        yield\n",
        "    finally:\n",
        "        if handle is not None:\n",
        "            handle.remove()\n",
        "        layer._forward_hooks.clear()\n",
        "        layer._forward_hooks.update(saved)\n",
        "\n",
        "def my_hook_wrapper(fwd_hook):\n",
        "    def actual_hook(module, inp, out):\n",
        "        if fwd_hook is None:\n",
        "            return out\n",
        "        else:\n",
        "            return fwd_hook(module, inp, out)\n",
        "    return actual_hook\n",
        "\n",
        "def get_remove_proj_hook(steer_model, target_labels=None, avoid_labels=None):\n",
        "    if target_labels is None: target_labels = []\n",
        "    if avoid_labels is None: avoid_labels = []\n",
        "\n",
        "    def fwd_hook(module, inp, out):\n",
        "        hidden_states = out[0]\n",
        "        hidden_np = hidden_states.detach().cpu().numpy().astype(np.float32)\n",
        "        B, S, D = hidden_np.shape\n",
        "        hidden_2d = hidden_np.reshape(-1, D)\n",
        "\n",
        "        new_2d = steer_model.remove_projection(hidden_2d, target_labels=target_labels, avoid_labels=avoid_labels)\n",
        "        new_np = new_2d.reshape(B, S, D)\n",
        "        new_hidden_torch = torch.from_numpy(new_np).to(hidden_states.device, dtype=torch.float16)\n",
        "        return (new_hidden_torch,) + out[1:]\n",
        "    return fwd_hook\n",
        "\n",
        "def get_gradient_hook(steer_model, target_labels=None, avoid_labels=None, alpha=1.0):\n",
        "    if target_labels is None: target_labels = []\n",
        "    if avoid_labels is None: avoid_labels = []\n",
        "\n",
        "    def fwd_hook(module, inp, out):\n",
        "        hidden_states = out[0]\n",
        "        hidden_np = hidden_states.detach().cpu().numpy().astype(np.float32)\n",
        "        B, S, D = hidden_np.shape\n",
        "        hidden_2d = hidden_np.reshape(-1, D)\n",
        "\n",
        "        new_2d = steer_model.steer_activations(hidden_2d,\n",
        "                                               target_labels=target_labels,\n",
        "                                               avoid_labels=avoid_labels,\n",
        "                                               alpha=alpha)\n",
        "        new_np = new_2d.reshape(B, S, D)\n",
        "        new_hidden_torch = torch.from_numpy(new_np).to(hidden_states.device, dtype=torch.float16)\n",
        "        return (new_hidden_torch,) + out[1:]\n",
        "    return fwd_hook\n",
        "\n",
        "def get_caa_hook(caa_vector, alpha=1.0):\n",
        "    def fwd_hook(module, inp, out):\n",
        "        hidden_states = out[0]\n",
        "        hidden_np = hidden_states.detach().cpu().numpy().astype(np.float32)\n",
        "        B, S, D = hidden_np.shape\n",
        "        hidden_2d = hidden_np.reshape(-1, D)\n",
        "\n",
        "        hidden_2d += alpha * caa_vector[None, :]\n",
        "        new_np = hidden_2d.reshape(B, S, D)\n",
        "        new_hidden_torch = torch.from_numpy(new_np).to(hidden_states.device, dtype=torch.float16)\n",
        "        return (new_hidden_torch,) + out[1:]\n",
        "    return fwd_hook"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def batch_compare(\n",
        "    triples: List[Tuple[str, str, str]],\n",
        "    judge   : OpenAiJudge,\n",
        "    max_concurrency: int = 10,\n",
        ") -> List[str]:\n",
        "    sem   = asyncio.Semaphore(max_concurrency)\n",
        "    out   = [None] * len(triples)\n",
        "\n",
        "    async def worker(idx: int, q: str, b: str, s: str):\n",
        "        async with sem:\n",
        "            out[idx] = await judge.compare(q, b, s)\n",
        "\n",
        "    tasks = [asyncio.create_task(worker(i, *t)) for i, t in enumerate(triples)]\n",
        "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks),\n",
        "                  desc=\"LLMâ€‘judge\", leave=False):\n",
        "        await f\n",
        "    return out\n",
        "\n",
        "async def eval_steering_combinations(\n",
        "    *,\n",
        "    eval_method      : str,\n",
        "    base_model,\n",
        "    tokenizer,\n",
        "    prompts          : List[str],\n",
        "    unique_tones     : List[str],\n",
        "    caa_vectors,\n",
        "    steer_model,\n",
        "    layer_idx        : int = 22,\n",
        "    alpha_grad       : float = 1_000.0,\n",
        "    alpha_caa        : float = 2.0,\n",
        "    num_target_tones : int   = 2,\n",
        "    max_samples      : int   = 300,\n",
        "    batch_size       : int   = 16,\n",
        "    judge_parallel   : int   = 25,\n",
        "    judge            = None,\n",
        "    act_clf          = None,\n",
        "    gen_clf_fn       : Optional[Callable[[List[str]], List[str]]] = None,\n",
        "    get_layer_token_hidden_fn = None,\n",
        ") -> pd.DataFrame:\n",
        "    assert eval_method in {\n",
        "        \"llm_judge\", \"activation_classifier\", \"generation_classifier\"\n",
        "    }, f\"Unknown eval_method: {eval_method}\"\n",
        "\n",
        "    prompts = prompts[:max_samples]\n",
        "\n",
        "    tone2idx = {t: i for i, t in enumerate(unique_tones)}\n",
        "    N        = float(len(prompts))\n",
        "\n",
        "    print(\"Generating BASE completions â€¦\")\n",
        "    base_ans = batch_generate(\n",
        "        base_model, tokenizer, prompts,\n",
        "        layer_idx=layer_idx, hook_fn=None,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    if eval_method == \"activation_classifier\":\n",
        "        assert act_clf is not None, \"Must provide act_clf=... for activation_classifier\"\n",
        "        assert get_layer_token_hidden_fn is not None, \"Must provide get_layer_token_hidden_fn=...\"\n",
        "        base_act = get_layer_token_hidden_fn(prompts)\n",
        "\n",
        "    combos = list(combinations(range(len(unique_tones)), num_target_tones))\n",
        "    rows   = []\n",
        "\n",
        "    from tqdm.notebook import tqdm\n",
        "\n",
        "    for combo in tqdm(combos, desc=f\"{num_target_tones}-tone combos\"):\n",
        "        tgt_idx   = list(combo)\n",
        "        tgt_names = [unique_tones[i] for i in tgt_idx]\n",
        "        tgt_set   = set(tgt_names)\n",
        "\n",
        "        grad_hook = get_gradient_hook(\n",
        "            steer_model, target_labels=tgt_idx, avoid_labels=[], alpha=alpha_grad\n",
        "        )\n",
        "        caa_vec   = caa_vectors[tgt_idx].mean(axis=0)\n",
        "        caa_hook  = get_caa_hook(caa_vec, alpha=alpha_caa)\n",
        "\n",
        "        counts = {\"grad\": 0, \"caa\": 0}\n",
        "\n",
        "        if eval_method == \"llm_judge\":\n",
        "            grad_ans = batch_generate(\n",
        "                base_model, tokenizer, prompts,\n",
        "                layer_idx=layer_idx, hook_fn=grad_hook,\n",
        "                batch_size=batch_size\n",
        "            )\n",
        "            caa_ans = batch_generate(\n",
        "                base_model, tokenizer, prompts,\n",
        "                layer_idx=layer_idx, hook_fn=caa_hook,\n",
        "                batch_size=batch_size\n",
        "            )\n",
        "\n",
        "            triples, where = [], []\n",
        "            for q, b, g, c in zip(prompts, base_ans, grad_ans, caa_ans):\n",
        "                triples.append((q, b, g))\n",
        "                where.append(\"grad\")\n",
        "                triples.append((q, b, c))\n",
        "                where.append(\"caa\")\n",
        "\n",
        "            preds = await batch_compare(triples, judge, max_concurrency=judge_parallel)\n",
        "\n",
        "            for method, label in zip(where, preds):\n",
        "                if label in tgt_set:\n",
        "                    counts[method] += 1\n",
        "\n",
        "        elif eval_method == \"activation_classifier\":\n",
        "            act_clf.eval()\n",
        "\n",
        "            model_device = next(act_clf.parameters()).device\n",
        "\n",
        "            grad_act = steer_model.steer_activations(\n",
        "                base_act, target_labels=tgt_idx, avoid_labels=[], alpha=alpha_grad\n",
        "            )\n",
        "            grad_t   = torch.tensor(grad_act, dtype=torch.float32, device=model_device)\n",
        "            with torch.no_grad():\n",
        "                grad_logits = act_clf(grad_t)\n",
        "            grad_preds = grad_logits.argmax(dim=1).cpu().numpy()\n",
        "            for p in grad_preds:\n",
        "                if unique_tones[p] in tgt_set:\n",
        "                    counts[\"grad\"] += 1\n",
        "\n",
        "            caa_act = base_act + caa_vec[None, :]\n",
        "            caa_t   = torch.tensor(caa_act, dtype=torch.float32, device=model_device)\n",
        "            with torch.no_grad():\n",
        "                caa_logits = act_clf(caa_t)\n",
        "            caa_preds = caa_logits.argmax(dim=1).cpu().numpy()\n",
        "            for p in caa_preds:\n",
        "                if unique_tones[p] in tgt_set:\n",
        "                    counts[\"caa\"] += 1\n",
        "\n",
        "        elif eval_method == \"generation_classifier\":\n",
        "            assert gen_clf_fn is not None, \"Must provide gen_clf_fn=... for generation_classifier\"\n",
        "\n",
        "            grad_ans = batch_generate(\n",
        "                base_model, tokenizer, prompts,\n",
        "                layer_idx=layer_idx, hook_fn=grad_hook,\n",
        "                batch_size=batch_size\n",
        "            )\n",
        "            caa_ans = batch_generate(\n",
        "                base_model, tokenizer, prompts,\n",
        "                layer_idx=layer_idx, hook_fn=caa_hook,\n",
        "                batch_size=batch_size\n",
        "            )\n",
        "\n",
        "            grad_preds = gen_clf_fn(grad_ans)\n",
        "            caa_preds  = gen_clf_fn(caa_ans)\n",
        "\n",
        "            for lbl in grad_preds:\n",
        "                if lbl in tgt_set:\n",
        "                    counts[\"grad\"] += 1\n",
        "            for lbl in caa_preds:\n",
        "                if lbl in tgt_set:\n",
        "                    counts[\"caa\"] += 1\n",
        "\n",
        "        rows.append({\n",
        "            \"Targets\"          : \", \".join(tgt_names),\n",
        "            \"Grad_MeanHitRate\" : counts[\"grad\"] / N,\n",
        "            \"CAA_MeanHitRate\"  : counts[\"caa\"]  / N,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ],
      "metadata": {
        "id": "f2yOlF67PBGb"
      },
      "id": "f2yOlF67PBGb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agg_dataset = load_dataset(\"Narmeen07/tone_agnostic_questions\", split=\"train\")\n",
        "\n",
        "def build_neutral_prompt(question):\n",
        "    return f\"SYSTEM:\\nUSER: {question}\"\n",
        "\n",
        "eval_prompts = [build_neutral_prompt(row[\"text\"]) for row in agg_dataset]"
      ],
      "metadata": {
        "id": "PsYZOvbaruK6"
      },
      "id": "PsYZOvbaruK6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformer_lens import HookedTransformer\n",
        "\n",
        "openai_client = AsyncOpenAI(api_key=\"\")\n",
        "judge         = OpenAiJudge(openai_client, \"gpt-4o\")\n",
        "\n",
        "df_llm = await eval_steering_combinations(\n",
        "    eval_method     = \"llm_judge\",\n",
        "    judge           = judge,\n",
        "    base_model      = model,\n",
        "    tokenizer       = tokenizer,\n",
        "    prompts         = eval_prompts,\n",
        "    unique_tones    = unique_tones,\n",
        "    caa_vectors     = caa_vectors,\n",
        "    steer_model     = steer_model,\n",
        "    num_target_tones= 2,\n",
        ")\n",
        "\n",
        "df_llm"
      ],
      "metadata": {
        "id": "yxCiUz9EQSNW"
      },
      "id": "yxCiUz9EQSNW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_act = await eval_steering_combinations(\n",
        "    eval_method     = \"activation_classifier\",\n",
        "    act_clf         = eval_model.classifier,\n",
        "    get_layer_token_hidden_fn = get_layer_token_hidden,\n",
        "    base_model      = model,\n",
        "    tokenizer       = tokenizer,\n",
        "    prompts         = eval_prompts,\n",
        "    unique_tones    = unique_tones,\n",
        "    caa_vectors     = caa_vectors,\n",
        "    steer_model     = steer_model,\n",
        "    num_target_tones= 2,\n",
        ")\n",
        "\n",
        "df_act"
      ],
      "metadata": {
        "id": "4EGX3Ly3-TUy"
      },
      "id": "4EGX3Ly3-TUy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_gen = await eval_steering_combinations(\n",
        "    eval_method        = \"generation_classifier\",\n",
        "    gen_clf_fn         = gen_clf_fn,\n",
        "    base_model         = model,\n",
        "    tokenizer          = tokenizer,\n",
        "    prompts            = eval_prompts,\n",
        "    unique_tones       = unique_tones,\n",
        "    caa_vectors        = caa_vectors,\n",
        "    steer_model        = steer_model,\n",
        "    num_target_tones   = 2,\n",
        "    layer_idx          = 22,\n",
        "    alpha_grad         = 500.0,\n",
        "    alpha_caa          = 5.0,\n",
        "    max_samples        = 100,\n",
        ")\n",
        "\n",
        "df_gen.head()"
      ],
      "metadata": {
        "id": "wAYQ3zWl-VKD"
      },
      "id": "wAYQ3zWl-VKD",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
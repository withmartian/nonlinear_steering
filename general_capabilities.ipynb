{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "FAii7a0jarkj",
      "metadata": {
        "id": "FAii7a0jarkj"
      },
      "outputs": [],
      "source": [
        "# Only run to clear GPU mem\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8",
      "metadata": {
        "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_tKEtbndpuW6",
      "metadata": {
        "id": "_tKEtbndpuW6"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers accelerate transformer_lens openai tiktoken kaleido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "702863ee-3e14-424f-a304-2f8449de9d8e",
      "metadata": {
        "id": "702863ee-3e14-424f-a304-2f8449de9d8e"
      },
      "outputs": [],
      "source": [
        "import asyncio, hashlib, math, os, random, sys, copy, gc, re, ast, json, uuid, html as _html\n",
        "from contextlib import contextmanager\n",
        "import collections\n",
        "from collections import defaultdict, Counter\n",
        "from pathlib import Path\n",
        "from types import SimpleNamespace\n",
        "from typing import List, Dict, Tuple, Optional, DefaultDict, Callable, Union, Callable, Sequence, Mapping\n",
        "from urllib.request import urlopen\n",
        "import importlib.util, sys, copy, torch, itertools\n",
        "from itertools import combinations\n",
        "from functools import lru_cache\n",
        "import html\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from openai import AsyncOpenAI\n",
        "import plotly.express as px\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "XCgrRk7Bn7om",
      "metadata": {
        "id": "XCgrRk7Bn7om"
      },
      "outputs": [],
      "source": [
        "def _load_tones():\n",
        "    ds = load_dataset(\"Narmeen07/tone_agnostic_questions\", split=\"train\")\n",
        "    steered_prompts = [\n",
        "        (\"expert\",     \"RESPOND AS AN AUTHORITATIVE EXPERT. Use advanced terminology and industry-specific jargon. Reference research studies, technical principles, and methodological frameworks. Maintain a formal, academic tone throughout. Analyze the question with depth and precision, exploring nuanced aspects that novices might overlook. Cite relevant theories, standards, or established practices. Use complex sentence structures and sophisticated vocabulary that demonstrates mastery of the subject matter.\"),\n",
        "        (\"cautious\",   \"RESPOND WITH EXTREME CAUTION AND QUALIFICATION. Begin by acknowledging limitations in current understanding of this topic. Present multiple perspectives and avoid making definitive claims. Use hedging language consistently (possibly, potentially, it appears that, some evidence suggests). Highlight risks, uncertainties, and areas of controversy. Include multiple disclaimers and caveats. Acknowledge what remains unknown or debated among experts. Explicitly state the boundaries of your response and recommend additional verification.\"),\n",
        "        (\"empathetic\", \"RESPOND WITH DEEP EMOTIONAL UNDERSTANDING. Begin by validating the feelings that might accompany this question. Use warm, compassionate language that acknowledges the human experience behind the query. Express genuine concern and emotional resonance throughout your response. Use phrases like 'I understand this might feel...' or 'Many people find this emotionally challenging because...' Prioritize the emotional and personal implications over technical aspects. Offer supportive, nurturing language that addresses potential fears or hopes related to the topic.\"),\n",
        "        (\"casual\",     \"You are an AI assistant responding with a casual tone. Use a conversational, friendly tone with simpler language and occasional humor. Be relatable and informal, as if chatting with a friend.\"),\n",
        "        (\"concise\",   \"BE EXTREMELY BRIEF. Use minimal words. Provide only essential information. No introductions. No elaboration. Use short sentences with simple structure. Eliminate all unnecessary words. Focus solely on direct answers. Avoid examples unless critical. Use bullet points where possible. Prioritize brevity over comprehensiveness. Get to the point immediately. Never repeat information.\")\n",
        "    ]\n",
        "    rel_template = (\n",
        "        \"You will compare two answers to the *same* question.\\n\\n\"\n",
        "        \"Question:\\n[QUESTION START]\\n{question}\\n[QUESTION END]\\n\\n\"\n",
        "        \"Original answer:\\n[BASE START]\\n{base_answer}\\n[BASE END]\\n\\n\"\n",
        "        \"Modified answer:\\n[STEERED START]\\n{steered_answer}\\n[STEERED END]\\n\\n\"\n",
        "        \"Compared **to the original answer**, which single tone label best describes the *modified* answer?\\n\"\n",
        "        \"Allowed labels: cautious, empathetic, expert, helpful, casual, concise.\\n\"\n",
        "        \"Respond with **only** one label.\"\n",
        "    )\n",
        "    return SimpleNamespace(\n",
        "        name=\"tone\",\n",
        "        raw_dataset = ds,\n",
        "        steered_prompts = steered_prompts,\n",
        "        rel_template = rel_template,\n",
        "        eval_prompts = list(ds[\"text\"]),\n",
        "        unique_labels = sorted({t for t, _ in steered_prompts}),\n",
        "    )\n",
        "\n",
        "def _load_debates():\n",
        "    ds = load_dataset(\"Narmeen07/debate_style_agnostic_questions\", split=\"train\")\n",
        "\n",
        "    steered_prompts = [\n",
        "        (\"Reductio ad Absurdum\", \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\n\"),\n",
        "        (\"Appeal to Precedent\", \"RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\n\"),\n",
        "        (\"Straw Man Reframing\", \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\n\"),\n",
        "        (\"Burden of Proof Shift\", \"RESPOND USING BURDEN OF PROOF SHIFT DEBATE STYLE. Redirect the responsibility for providing evidence to your opponent rather than proving your own claim. Challenge your opponent to disprove your assertion rather than supporting it yourself. Use phrases like 'There's no evidence that disproves...' or 'Can you definitively prove that isn't the case?' Position your claims as the default assumption that should be accepted until disproven. Question the sufficiency or quality of opposing evidence while demanding an impossibly high standard of proof. Emphasize that the lack of counter-evidence strengthens your position.\\n\\n\"),\n",
        "        (\"Analogy Construction\", \"RESPOND USING ANALOGY CONSTRUCTION DEBATE STYLE. Develop a vivid, relatable comparison between the complex issue at hand and something more familiar or intuitive. Build your argument around this carefully constructed parallel situation. Highlight specific points of similarity that support your position while addressing potential dissimilarities. Use phrases like 'This situation is similar to...' or 'To understand this concept, consider...' Ensure your analogy simplifies the complex issue without distorting its essential nature. Use the familiar scenario to guide your audience to your desired conclusion about the original issue.\\n\\n\"),\n",
        "        (\"Concession and Pivot\", \"RESPOND USING CONCESSION AND PIVOT DEBATE STYLE. Begin by acknowledging a minor point or critique from the opposing side to establish fairness and reasonableness. Use phrases like 'While it's true that...' or 'I can concede that...' followed by 'However,' 'Nevertheless,' or 'That said,' to redirect to your stronger arguments. Ensure the conceded point is peripheral rather than central to your main argument. After the concession, pivot decisively to your strongest points with increased emphasis. Frame your pivot as providing necessary context or a more complete perspective. Use the concession to demonstrate your objectivity before delivering your more powerful counterarguments.\\n\\n\"),\n",
        "        (\"Empirical Grounding\", \"RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\n\"),\n",
        "        (\"Moral Framing\", \"RESPOND USING MORAL FRAMING DEBATE STYLE. Position the issue within a framework of ethical principles, values, and moral imperatives rather than pragmatic concerns. Identify the core moral values at stake such as justice, liberty, equality, compassion, or responsibility. Use language that evokes ethical considerations, such as 'obligation,' 'right,' 'wrong,' 'just,' or 'fair.' Appeal to widely held moral intuitions or principles. Present opposing views as morally questionable or inconsistent with important shared values. Elevate the discussion from practical matters to questions of what ought to be done. Emphasize moral consequences over practical outcomes.\\n\\n\"),\n",
        "        (\"Refutation by Distinction\", \"RESPOND USING REFUTATION BY DISTINCTION DEBATE STYLE. Identify crucial differences that invalidate comparisons or principles your opponent has applied. Carefully delineate categories, contexts, or circumstances that demonstrate why a general rule or example doesn't apply in this specific case. Use phrases like 'While that may be true in some contexts...' or 'We must distinguish between...' Emphasize the precision of definitions and classifications. Highlight subtle but significant differences that undermine the opponent's logic. Show how these distinctions fundamentally change the assessment of the situation. Demonstrate how recognizing these distinctions leads to a different conclusion than your opponent reached.\\n\\n\"),\n",
        "        (\"Circular Anticipation\", \"RESPOND USING CIRCULAR ANTICIPATION DEBATE STYLE. Preemptively identify and address the most likely counterarguments before your opponent can make them. Introduce opposing points with phrases like 'Some might argue...' or 'One could object that...' followed by your prepared refutation. Structure your response to cover all major potential objections. Demonstrate that you've thoroughly considered the issue from multiple angles. Frame potential counterarguments in ways that make them easier to dismantle. Create the impression that all reasonable objections have already been considered and overcome. Conclude by suggesting that any remaining objections would be similarly flawed.\\n\\n\")\n",
        "    ]\n",
        "    rel_template = (\n",
        "        \"You will compare two answers to the *same* question.\\n\\n\"\n",
        "        \"Question:\\n[QUESTION START]\\n{question}\\n[QUESTION END]\\n\\n\"\n",
        "        \"Original answer:\\n[BASE START]\\n{base_answer}\\n[BASE END]\\n\\n\"\n",
        "        \"Modified answer:\\n[STEERED START]\\n{steered_answer}\\n[STEERED END]\\n\\n\"\n",
        "        \"Compared **to the original answer**, which single tone label best describes the *modified* answer?\\n\"\n",
        "        \"Allowed labels: cautious, empathetic, expert, helpful, casual, concise.\\n\"\n",
        "        \"Respond with **only** one label.\"\n",
        "    )\n",
        "    return SimpleNamespace(\n",
        "        name=\"debate\",\n",
        "        raw_dataset = ds,\n",
        "        steered_prompts = steered_prompts,\n",
        "        rel_template = rel_template,\n",
        "        eval_prompts = list(ds[\"text\"]),\n",
        "        unique_labels = sorted({t for t, _ in steered_prompts}),\n",
        "    )\n",
        "\n",
        "_TASK_LOADERS = {\"tone\": _load_tones, \"debate\": _load_debates}\n",
        "_CURRENT_TASK = None\n",
        "_DATA_CTX     = None\n",
        "\n",
        "def ensure_task_data(task: str | None = None):\n",
        "    global _CURRENT_TASK, _DATA_CTX\n",
        "    task = task or CFG[\"TASK\"]\n",
        "    if _CURRENT_TASK == task and _DATA_CTX is not None:\n",
        "        return _DATA_CTX\n",
        "    if task not in _TASK_LOADERS:\n",
        "        raise ValueError(f\"Unknown task {task!r}. Choose one of {list(_TASK_LOADERS)}\")\n",
        "    print(f\"⇒ Loading steering task “{task}”…\")\n",
        "    _DATA_CTX     = _TASK_LOADERS[task]()\n",
        "    _CURRENT_TASK = task\n",
        "    return _DATA_CTX\n",
        "\n",
        "def build_steering_dataset(ctx: SimpleNamespace) -> Dataset:\n",
        "    rows = []\n",
        "    for row in ctx.raw_dataset:\n",
        "        q_text, q_id = row[\"text\"], row[\"id\"]\n",
        "        cat = row.get(\"category\", \"\")\n",
        "        for lbl, sys_prompt in ctx.steered_prompts:\n",
        "            rows.append({\n",
        "                \"id\": f\"{q_id}_{lbl}\",\n",
        "                \"original_question\": q_text,\n",
        "                \"text\": f\"{sys_prompt}\\n{q_text}\",\n",
        "                \"label\": lbl,\n",
        "                \"system_message\": sys_prompt,\n",
        "                \"category\": cat,\n",
        "            })\n",
        "    return Dataset.from_pandas(pd.DataFrame(rows))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "HwGpDYZuqECU",
      "metadata": {
        "id": "HwGpDYZuqECU"
      },
      "outputs": [],
      "source": [
        "def get_dataset(dataset_name = \"tone\"):\n",
        "\n",
        "    data_ctx          = ensure_task_data(dataset_name)\n",
        "    dataset           = build_steering_dataset(data_ctx)\n",
        "    unique_labels     = data_ctx.unique_labels\n",
        "    RELATIVE_TEMPLATE = data_ctx.rel_template\n",
        "    eval_prompts      = data_ctx.eval_prompts\n",
        "    return dataset, eval_prompts, unique_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "IxkzyDpQp28C",
      "metadata": {
        "id": "IxkzyDpQp28C"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0e15cb95-5e68-49af-8934-06bc239edf04",
      "metadata": {
        "id": "0e15cb95-5e68-49af-8934-06bc239edf04"
      },
      "outputs": [],
      "source": [
        "def get_model(model_name= \"llama-3.2-3b\"):\n",
        "\n",
        "    if  model_name == \"llama-3.2-3b\":\n",
        "        model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "    elif model_name == \"olmo-2-7b\":\n",
        "        model_name = \"placeholder\"   #path to olmo model\n",
        "    else:\n",
        "        model_name = \"placeholder\" #path to mistral model\n",
        "\n",
        "    print(f\"Loading {model_name}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True,\n",
        "        _attn_implementation=\"eager\",\n",
        "        output_hidden_states=True,\n",
        "    ).to(\"cuda:0\")\n",
        "\n",
        "    model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)\n",
        "    return model, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e",
      "metadata": {
        "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e"
      },
      "outputs": [],
      "source": [
        "def get_hidden_cached(model, tokenizer, texts: List[str], layer_idx: int, *, batch_size: int = 64) -> np.ndarray:\n",
        "    all_vecs = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i : i + batch_size]\n",
        "        tok = tokenizer(batch,\n",
        "                        return_tensors=\"pt\",\n",
        "                        padding=True,\n",
        "                        truncation=True).to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            out = model(**tok, output_hidden_states=True)\n",
        "        h = out.hidden_states[layer_idx]\n",
        "        mask = tok[\"attention_mask\"]\n",
        "        lengths = mask.sum(dim=1) - 1\n",
        "\n",
        "        for j, idx in enumerate(lengths):\n",
        "            all_vecs.append(h[j, idx, :].cpu().float().numpy())\n",
        "\n",
        "    return np.stack(all_vecs, axis=0)\n",
        "\n",
        "def batch_generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompts: List[str],\n",
        "    layer_idx: int,\n",
        "    hook_fn: Optional[Callable] = None,\n",
        "    max_new_tokens: int = 24,\n",
        "    batch_size: int = 512,\n",
        ") -> List[str]:\n",
        "    device        = model.device\n",
        "    target_layer  = model.model.layers[layer_idx]\n",
        "    outputs: List[str] = []\n",
        "\n",
        "    saved_hooks = target_layer._forward_hooks.copy()\n",
        "    target_layer._forward_hooks.clear()\n",
        "\n",
        "    handle = None\n",
        "    if hook_fn is not None:\n",
        "        handle = target_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "    try:\n",
        "        for i in range(0, len(prompts), batch_size):\n",
        "            sub_prompts = prompts[i : i + batch_size]\n",
        "            tok_in = tokenizer(\n",
        "                sub_prompts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                gen_ids = model.generate(\n",
        "                    **tok_in,\n",
        "                    max_new_tokens = max_new_tokens,\n",
        "                    do_sample      = False,\n",
        "                    pad_token_id   = tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "            outputs.extend(\n",
        "                tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
        "            )\n",
        "    finally:\n",
        "        if handle is not None:\n",
        "            handle.remove()\n",
        "        target_layer._forward_hooks.clear()\n",
        "        target_layer._forward_hooks.update(saved_hooks)\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04zXz48Htq1R",
      "metadata": {
        "id": "04zXz48Htq1R"
      },
      "source": [
        "# Steering Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "534322de-bb2c-41ef-8033-fd061412212b",
      "metadata": {
        "id": "534322de-bb2c-41ef-8033-fd061412212b"
      },
      "source": [
        "## K-Steering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9",
      "metadata": {
        "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9"
      },
      "outputs": [],
      "source": [
        "def one_hot(idxs: np.ndarray, C: int) -> np.ndarray:\n",
        "    out = np.zeros((len(idxs), C), dtype=np.float32)\n",
        "    out[np.arange(len(idxs)), idxs] = 1.0\n",
        "    return out\n",
        "\n",
        "class MultiLabelSteeringModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 hidden_dim: int,\n",
        "                 num_labels: int,\n",
        "                 linear: bool = False):\n",
        "        super().__init__()\n",
        "        if linear:\n",
        "            self.net = nn.Linear(input_dim, num_labels)\n",
        "        else:\n",
        "            self.net = nn.Sequential(\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, num_labels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ActivationSteering:\n",
        "    def __init__(self, input_dim, num_labels, hidden_dim=128, lr=1e-3):\n",
        "        self.device = DEVICE\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        self.classifier = MultiLabelSteeringModel(\n",
        "            input_dim, hidden_dim, num_labels\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    def fit(self, X, Y, epochs=10, batch_size=32):\n",
        "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        Y_t = torch.tensor(Y, dtype=torch.float32, device=self.device)\n",
        "\n",
        "        dataset = torch.utils.data.TensorDataset(X_t, Y_t)\n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        for ep in range(epochs):\n",
        "            total_loss = 0.0\n",
        "            for bx, by in loader:\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.classifier(bx)\n",
        "                loss = self.loss_fn(logits, by)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(f\"Epoch {ep+1}/{epochs}, Loss={total_loss/len(loader):.4f}\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict_proba(self, X):\n",
        "        self.classifier.eval()\n",
        "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
        "        logits = self.classifier(X_t)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        return probs.cpu().numpy()\n",
        "\n",
        "    def steer_activations(\n",
        "        self,\n",
        "        acts: Union[np.ndarray, torch.Tensor],\n",
        "        target_idx: List[int],\n",
        "        avoid_idx: List[int] = [],\n",
        "        alpha: float = 1.0,\n",
        "        steps: int = 1,\n",
        "        step_size_decay: float = 1.0,\n",
        "    ) -> torch.Tensor:\n",
        "        if isinstance(acts, np.ndarray):\n",
        "            acts = torch.as_tensor(acts, dtype=torch.float32, device=self.device)\n",
        "        else:\n",
        "            acts = acts.to(self.device, dtype=torch.float32)\n",
        "\n",
        "        steered = acts.detach().clone()\n",
        "\n",
        "        for step in range(steps):\n",
        "            curr = steered.clone().requires_grad_(True)\n",
        "            logits = self.classifier(curr)\n",
        "\n",
        "            loss_vec = _compute_steering_loss(\n",
        "                logits, target_idx=target_idx, avoid_idx=avoid_idx\n",
        "            )\n",
        "\n",
        "            loss = loss_vec.mean()\n",
        "            grads = torch.autograd.grad(loss, curr, retain_graph=False)[0]\n",
        "\n",
        "            current_alpha = alpha * (step_size_decay ** step)\n",
        "            steered = (curr - current_alpha * grads).detach()\n",
        "\n",
        "        return steered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "zD3tyQIXxmMJ",
      "metadata": {
        "id": "zD3tyQIXxmMJ"
      },
      "outputs": [],
      "source": [
        "def get_or_train_layer_clf(unique_labels, X: np.ndarray, y: np.ndarray,\n",
        "                           *, hidden_dim=128, epochs=5, batch_size=32):\n",
        "    if y.dtype.kind not in (\"i\", \"u\"):\n",
        "        lbl2idx = {lbl: i for i, lbl in enumerate(unique_labels)}\n",
        "        y = np.asarray([lbl2idx[lbl] for lbl in y], dtype=np.int64)\n",
        "\n",
        "\n",
        "    idx_A, idx_B = train_test_split(np.arange(len(X)), test_size=0.5, random_state=42, stratify=y)\n",
        "    X_A, X_B, y_A, y_B = X[idx_A], X[idx_B], y[idx_A], y[idx_B]\n",
        "\n",
        "    clf = ActivationSteering(input_dim=X.shape[1], num_labels=len(unique_labels), hidden_dim=hidden_dim)\n",
        "    clf.fit(X_A, one_hot(y_A, len(unique_labels)), epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        acc = (torch.argmax(\n",
        "            clf.classifier(torch.tensor(X_B, dtype=torch.float32, device=clf.device)),\n",
        "            dim=1).cpu().numpy() == y_B).mean()\n",
        "\n",
        "    return clf, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683fa36b-15d3-40d4-8a69-0f1995793703",
      "metadata": {
        "id": "683fa36b-15d3-40d4-8a69-0f1995793703"
      },
      "source": [
        "## CAA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
      "metadata": {
        "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def compute_caa_vectors(\n",
        "    dataset,\n",
        "    unique_labels,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    steer_layer: int,\n",
        "    max_pairs: int | None = None,\n",
        ") -> np.ndarray:\n",
        "    q2lab2text = defaultdict(dict)\n",
        "    for row in dataset:\n",
        "        q2lab2text[row[\"original_question\"]][row[\"label\"]] = row[\"text\"]\n",
        "\n",
        "    pos, neg = defaultdict(list), defaultdict(list)\n",
        "    for q, lab_map in q2lab2text.items():\n",
        "        labs = set(lab_map)\n",
        "        for tgt in labs:\n",
        "            for other in labs - {tgt}:\n",
        "                pos[tgt].append(lab_map[tgt])\n",
        "                neg[tgt].append(lab_map[other])\n",
        "\n",
        "    caa_vecs = []\n",
        "    for lbl in unique_labels:\n",
        "        pairs = len(pos[lbl])\n",
        "        if max_pairs and pairs > max_pairs:\n",
        "            keep = random.sample(range(pairs), max_pairs)\n",
        "            pos[lbl] = [pos[lbl][i] for i in keep]\n",
        "            neg[lbl] = [neg[lbl][i] for i in keep]\n",
        "\n",
        "        if not pos[lbl]:\n",
        "            caa_vecs.append(np.zeros(model.config.hidden_size, dtype=np.float32))\n",
        "            continue\n",
        "\n",
        "        X_pos = get_hidden_cached(model=model, tokenizer=tokenizer,texts=pos[lbl], layer_idx=steer_layer)\n",
        "        X_neg = get_hidden_cached(model=model, tokenizer=tokenizer, texts=neg[lbl], layer_idx=steer_layer)\n",
        "        caa_vecs.append((X_pos - X_neg).mean(0))\n",
        "\n",
        "    return np.stack(caa_vecs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2I4u2MY2t1uf",
      "metadata": {
        "id": "2I4u2MY2t1uf"
      },
      "source": [
        "# Evaluation Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7b7bd55-3a68-4e71-a797-790581301243",
      "metadata": {
        "id": "b7b7bd55-3a68-4e71-a797-790581301243"
      },
      "source": [
        "# Steering Vector Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c31f82be-b0df-4232-bffd-182b4df0735d",
      "metadata": {
        "id": "c31f82be-b0df-4232-bffd-182b4df0735d"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "068fd591-4032-423e-aa0a-041b098ea041",
      "metadata": {
        "id": "068fd591-4032-423e-aa0a-041b098ea041"
      },
      "outputs": [],
      "source": [
        "def _compute_steering_loss(\n",
        "    logits: torch.Tensor,\n",
        "    target_idx,\n",
        "    avoid_idx,\n",
        ") -> torch.Tensor:\n",
        "    if not torch.is_tensor(target_idx):\n",
        "        target_idx = torch.as_tensor(target_idx, device=logits.device)\n",
        "    else:\n",
        "        target_idx = target_idx.to(logits.device)\n",
        "    if not torch.is_tensor(avoid_idx):\n",
        "        avoid_idx = torch.as_tensor(avoid_idx, device=logits.device)\n",
        "    else:\n",
        "        avoid_idx = avoid_idx.to(logits.device)\n",
        "\n",
        "    B, C = logits.shape\n",
        "\n",
        "    if avoid_idx.numel() > 0:\n",
        "        avoid_term = logits[:, avoid_idx].mean(dim=1)\n",
        "    else:\n",
        "        avoid_term = torch.zeros(B, device=logits.device)\n",
        "\n",
        "    if target_idx.numel() > 0:\n",
        "        target_term = logits[:, target_idx].mean(dim=1)\n",
        "    else:\n",
        "        target_term = torch.zeros(B, device=logits.device)\n",
        "\n",
        "    return avoid_term - target_term\n",
        "\n",
        "def get_gradient_hook(steer_model,\n",
        "                      target_labels=None,\n",
        "                      avoid_labels=None,\n",
        "                      alpha: float = 1.0,\n",
        "                      steps: int = 1,\n",
        "                      step_size_decay: float = 1.0):\n",
        "\n",
        "    target_labels = torch.as_tensor(target_labels or [], device=steer_model.device)\n",
        "    avoid_labels  = torch.as_tensor(avoid_labels  or [], device=steer_model.device)\n",
        "\n",
        "    @torch.inference_mode(False)\n",
        "    def fwd_hook(module, inp, out):\n",
        "        h_fp16 = out[0]\n",
        "        B, S, D = h_fp16.shape\n",
        "\n",
        "        h_current = h_fp16.reshape(-1, D).float()\n",
        "\n",
        "        for step in range(steps):\n",
        "            h_step = h_current.clone()\n",
        "            h_step.requires_grad_(True)\n",
        "\n",
        "            logits = steer_model.classifier(h_step)\n",
        "            logits = logits.view(B, S, -1).mean(dim=1)\n",
        "\n",
        "            loss_vec = _compute_steering_loss(\n",
        "                logits,\n",
        "                target_idx=target_labels,\n",
        "                avoid_idx=avoid_labels\n",
        "            )\n",
        "\n",
        "            if loss_vec.numel() > 0:\n",
        "                grad = torch.autograd.grad(\n",
        "                    outputs=loss_vec,\n",
        "                    inputs=h_step,\n",
        "                    grad_outputs=torch.ones_like(loss_vec),\n",
        "                    retain_graph=False,\n",
        "                    create_graph=False,\n",
        "                )[0]\n",
        "\n",
        "                current_alpha = alpha * (step_size_decay ** step)\n",
        "\n",
        "                grad = grad.view(B * S, D)\n",
        "                h_current = (h_step - current_alpha * grad).detach()\n",
        "            else:\n",
        "                h_current = h_step.detach()\n",
        "\n",
        "        h_new = h_current.reshape(B, S, D).to(h_fp16.dtype)\n",
        "        return (h_new,) + out[1:]\n",
        "\n",
        "    return fwd_hook\n",
        "\n",
        "def get_caa_hook(caa_vector: torch.Tensor | np.ndarray,\n",
        "                 alpha: float = 1.0):\n",
        "    if not torch.is_tensor(caa_vector):\n",
        "        caa_vector = torch.as_tensor(caa_vector, dtype=torch.float16)\n",
        "\n",
        "    def fwd_hook(module, inp, out):\n",
        "        h = out[0]\n",
        "        return (h + alpha * caa_vector.to(h.device),) + out[1:]\n",
        "\n",
        "    return fwd_hook\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca27aea2-289a-49ea-ad19-130e8ed44c61",
      "metadata": {
        "id": "ca27aea2-289a-49ea-ad19-130e8ed44c61"
      },
      "source": [
        "## Core Evaluation Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f42b618-a08e-4948-81a4-bd4f56f7289c",
      "metadata": {
        "id": "4f42b618-a08e-4948-81a4-bd4f56f7289c"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FnGB2XiS7XS_",
      "metadata": {
        "id": "FnGB2XiS7XS_"
      },
      "source": [
        "# Manual Inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cca8370e",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "import random\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "def get_alpha_file(model=\"llama-3.2-3b\", dataset=\"tone\", num_combo=1):\n",
        "    alpha_path = f\"/home/ubuntu/nonlinear_steering/{model}/{model}-{str(num_combo)}-{dataset}-activations-classifier-alphas.csv\"\n",
        "    combo_lookup_table = pd.read_csv(alpha_path, converters={\"Targets\": literal_eval})\n",
        "    combo_lookup_table[\"key\"] = combo_lookup_table[\"Targets\"].apply(lambda t: tuple(sorted(t)))\n",
        "    combo_lookup_dict = combo_lookup_table.set_index(\"key\").to_dict(orient=\"index\")\n",
        "    return combo_lookup_dict\n",
        "\n",
        "def sample_steered_responses(\n",
        "    model_name: str,\n",
        "    dataset_name: str,\n",
        "    num_combo: int,\n",
        "    prompts: list[str],\n",
        "    *,\n",
        "    steer_model_k = None,\n",
        "    layer_k       = None,\n",
        "    alpha_grad    = None,\n",
        "    caa_vectors   = None,\n",
        "    layer_caa     = None,\n",
        "    alpha_caa     = None,\n",
        "    max_new_tokens: int = 32,\n",
        "    batch_size    : int = 500,\n",
        "    save_as: str | None = None,\n",
        "):\n",
        "    \n",
        "    dataset, eval_prompts, unique_labels = get_dataset(dataset_name)\n",
        "    combo_lookup_dict = get_alpha_file(model=model_name, dataset=dataset_name, num_combo=num_combo)\n",
        "\n",
        "    def _lookup_from_csv(is_caa=False):\n",
        "        combo_key = random.choice(list(combo_lookup_dict.keys()))\n",
        "        entry = combo_lookup_dict[combo_key]\n",
        "        target_tones_from_key = list(combo_key)  # Convert tuple back to list\n",
        "        if is_caa:\n",
        "            return entry[\"alpha_caa\"], int(entry[\"best_caa_layer\"]), target_tones_from_key\n",
        "        else:\n",
        "            return entry[\"alpha_grad\"], int(entry[\"best_k_layer\"]), target_tones_from_key\n",
        "\n",
        "    if alpha_grad is None or layer_k is None:\n",
        "        alpha_grad, layer_k, tgt_list = _lookup_from_csv(is_caa=False)\n",
        "    if alpha_caa is None or layer_caa is None:\n",
        "        alpha_caa, layer_caa, _ = _lookup_from_csv(is_caa=True)\n",
        "\n",
        "\n",
        "    model, tokenizer = get_model(model_name)\n",
        "\n",
        "    #Let's train the classifier\n",
        "    clf_prompts  = [row[\"text\"] for row in dataset]\n",
        "    clf_y  = np.array([unique_labels.index(row[\"label\"]) for row in dataset], dtype=np.int64)\n",
        "    clf_x = get_hidden_cached(model=model, tokenizer=tokenizer,texts=clf_prompts, layer_idx=layer_k)\n",
        "\n",
        "    act_clf, acc = get_or_train_layer_clf(\n",
        "        unique_labels=unique_labels,\n",
        "        X=clf_x,\n",
        "        y=clf_y,\n",
        "        hidden_dim = 128,\n",
        "        epochs      = 5,\n",
        "        batch_size  = 32,\n",
        "    )\n",
        "\n",
        "    print(\"Here is a randomly selected list of targets\", tgt_list)\n",
        "\n",
        "    if steer_model_k is None:\n",
        "        steer_model_k = act_clf  # Ensure act_clf_eval is defined in your scope\n",
        "\n",
        "    if caa_vectors is None:\n",
        "        caa_vectors = compute_caa_vectors(unique_labels=unique_labels, steer_layer=layer_k, dataset=dataset, model=model, tokenizer=tokenizer)\n",
        "\n",
        "    tone2idx = {t: i for i, t in enumerate(unique_labels)}\n",
        "    tgt_idx = [tone2idx[t] for t in tgt_list]\n",
        "\n",
        "    grad_hook = get_gradient_hook(\n",
        "        steer_model_k, target_labels=tgt_idx,\n",
        "        avoid_labels=[], alpha=alpha_grad\n",
        "    )\n",
        "    caa_vec = caa_vectors[tgt_idx].mean(axis=0)\n",
        "    caa_hook = get_caa_hook(caa_vec, alpha=alpha_caa)\n",
        "\n",
        "    unsteered = batch_generate(\n",
        "        model, tokenizer, prompts,\n",
        "        layer_idx=layer_caa,\n",
        "        hook_fn=None,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    ksteer = batch_generate(\n",
        "        model, tokenizer, prompts,\n",
        "        layer_idx=layer_k,\n",
        "        hook_fn=grad_hook,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    caa_out = batch_generate(\n",
        "        model, tokenizer, prompts,\n",
        "        layer_idx=layer_caa,\n",
        "        hook_fn=caa_hook,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    def _strip(gen: str, prompt: str) -> str:\n",
        "        return gen[len(prompt):].lstrip() if gen.startswith(prompt) else gen\n",
        "\n",
        "    rows = []\n",
        "    for p, base, ktxt, ctxt in zip(prompts, unsteered, ksteer, caa_out):\n",
        "        rows.append({\n",
        "            \"prompt\": p,\n",
        "            \"unsteered\": _strip(base, p),\n",
        "            \"k_steering\": _strip(ktxt, p),\n",
        "            \"caa\": _strip(ctxt, p),\n",
        "            \"layer_k\": layer_k,\n",
        "            \"layer_caa\": layer_caa,\n",
        "            \"alpha_grad\": alpha_grad,\n",
        "            \"alpha_caa\": alpha_caa,\n",
        "            \"targets\": \", \".join(tgt_list),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    for r in rows:\n",
        "        print(\"\\n\" + \"=\" * 90)\n",
        "        print(f\"PROMPT:\\n{r['prompt']}\\n\")\n",
        "        print(\"- Unsteered -------------------------------------------------\\n\"\n",
        "              + r[\"unsteered\"] + \"\\n\")\n",
        "        print(f\"- K‑steering (layer {layer_k}, α_grad = {alpha_grad:.3g})\\n\"\n",
        "              + r[\"k_steering\"] + \"\\n\")\n",
        "        print(f\"- CAA        (layer {layer_caa}, α_caa  = {alpha_caa:.3g})\\n\"\n",
        "              + r[\"caa\"] + \"\\n\")\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cvc_1iUo7ibI",
      "metadata": {
        "id": "cvc_1iUo7ibI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⇒ Loading steering task “tone”…\n",
            "Loading unsloth/Llama-3.2-3B-Instruct\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-16 16:52:50.722272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747414370.731575   31849 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747414370.736568   31849 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Loss=0.0785\n",
            "Epoch 2/5, Loss=0.0033\n",
            "Epoch 3/5, Loss=0.0036\n",
            "Epoch 4/5, Loss=0.0022\n",
            "Epoch 5/5, Loss=0.0019\n",
            "Here is a randomly selected list of targets ['casual', 'cautious']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==========================================================================================\n",
            "PROMPT:\n",
            "What is the answer to 1 + 1\n",
            "\n",
            "- Unsteered -------------------------------------------------\n",
            "?\n",
            "The answer to 1 + 1 is 2.\n",
            "What is the answer to 2 + 2?\n",
            "The answer to 2 + 2\n",
            "\n",
            "- K‑steering (layer 27, α_grad = 9.59)\n",
            "?\n",
            "The answer to 1 + 1 is 2.\n",
            "This is a basic arithmetic operation that is widely recognized and accepted across cultures and mathematical systems.\n",
            "In\n",
            "\n",
            "- CAA        (layer 27, α_caa  = 0.1)\n",
            "?\n",
            "The answer to 1 + 1 is 2.\n",
            "What is the answer to 2 + 2?\n",
            "The answer to 2 + 2\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>unsteered</th>\n",
              "      <th>k_steering</th>\n",
              "      <th>caa</th>\n",
              "      <th>layer_k</th>\n",
              "      <th>layer_caa</th>\n",
              "      <th>alpha_grad</th>\n",
              "      <th>alpha_caa</th>\n",
              "      <th>targets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the answer to 1 + 1</td>\n",
              "      <td>?\\nThe answer to 1 + 1 is 2.\\nWhat is the answ...</td>\n",
              "      <td>?\\nThe answer to 1 + 1 is 2.\\nThis is a basic ...</td>\n",
              "      <td>?\\nThe answer to 1 + 1 is 2.\\nWhat is the answ...</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>9.592578</td>\n",
              "      <td>0.1</td>\n",
              "      <td>casual, cautious</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        prompt  \\\n",
              "0  What is the answer to 1 + 1   \n",
              "\n",
              "                                           unsteered  \\\n",
              "0  ?\\nThe answer to 1 + 1 is 2.\\nWhat is the answ...   \n",
              "\n",
              "                                          k_steering  \\\n",
              "0  ?\\nThe answer to 1 + 1 is 2.\\nThis is a basic ...   \n",
              "\n",
              "                                                 caa  layer_k  layer_caa  \\\n",
              "0  ?\\nThe answer to 1 + 1 is 2.\\nWhat is the answ...       27         27   \n",
              "\n",
              "   alpha_grad  alpha_caa           targets  \n",
              "0    9.592578        0.1  casual, cautious  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_steered_responses(\n",
        "    prompts      = [\"What is the answer to 1 + 1\"], #put in any list of prompts here\n",
        "    model_name   = \"llama-3.2-3b\",          # model_name options [\"llama-3.2-3b\", \"olmo-2-7b\", \"mistral-7b\"]\n",
        "    dataset_name = \"tone\",                  # dataset_name options [\"tone\", \"debate\"]\n",
        "    num_combo=2                             # num_combo options [1,2,3]\n",
        "    \n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "04zXz48Htq1R",
        "2I4u2MY2t1uf",
        "c31f82be-b0df-4232-bffd-182b4df0735d"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

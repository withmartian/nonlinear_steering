{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "FAii7a0jarkj",
   "metadata": {
    "id": "FAii7a0jarkj"
   },
   "outputs": [],
   "source": [
    "# Only run to clear GPU mem\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8",
   "metadata": {
    "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "_tKEtbndpuW6",
   "metadata": {
    "id": "_tKEtbndpuW6"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate transformer_lens openai tiktoken kaleido torch numpy joblib scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702863ee-3e14-424f-a304-2f8449de9d8e",
   "metadata": {
    "id": "702863ee-3e14-424f-a304-2f8449de9d8e"
   },
   "outputs": [],
   "source": [
    "import asyncio, hashlib, math, os, random, sys, copy, gc, re, ast\n",
    "from contextlib import contextmanager\n",
    "import collections\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import List, Dict, Tuple, Optional, DefaultDict, Callable, Union, Callable, Sequence, Mapping\n",
    "from urllib.request import urlopen\n",
    "import importlib.util, sys, copy, random, torch, itertools\n",
    "from itertools import combinations, permutations\n",
    "from functools import lru_cache\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from openai import AsyncOpenAI\n",
    "import plotly.express as px\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "openai = AsyncOpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "CL1CyFVJnlw3",
   "metadata": {
    "id": "CL1CyFVJnlw3"
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    # steering layer: int → force layer; \"auto\" → sweep once & cache\n",
    "    # steering task: \"tones\" | \"obsession\"\n",
    "    \"TASK\": \"debates\",\n",
    "    # on‑disk caches\n",
    "    \"HIDDEN_CACHE_DIR\": \"llama3b_hidden_cache\",\n",
    "    \"MODEL_CACHE_DIR\": \"llama3b_layer_clfs\",\n",
    "}\n",
    "for _d in (CFG[\"HIDDEN_CACHE_DIR\"], CFG[\"MODEL_CACHE_DIR\"]):\n",
    "    Path(_d).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "XCgrRk7Bn7om",
   "metadata": {
    "id": "XCgrRk7Bn7om"
   },
   "outputs": [],
   "source": [
    "def _load_tones():\n",
    "    ds = load_dataset(\"Narmeen07/tone_agnostic_questions\", split=\"train\")\n",
    "    steered_prompts = [\n",
    "        (\"empathetic\", \"You are an AI assistant responding with an empathetic tone. Provide emotionally attuned responses that validate feelings and show understanding. Prioritize the emotional dimension of the query.\"),\n",
    "        (\"casual\",     \"You are an AI assistant responding with a casual tone. Use a conversational, friendly tone with simpler language and occasional humor. Be relatable and informal, as if chatting with a friend.\"),\n",
    "        (\"cautious\",   \"You are an AI assistant responding with a cautious tone. Give risk-aware responses that highlight limitations, uncertainties, and potential concerns. Be careful to qualify claims and note important caveats.\"),\n",
    "        (\"concise\",    \"You are an AI assistant responding with a concise tone. Give minimalist, efficient answers that prioritize brevity while maintaining completeness. Be direct and to the point with no unnecessary information.\"),\n",
    "        (\"expert\",     \"You are an AI assistant responding with an expert tone. Provide detailed, technically precise explanations with domain-specific terminology and depth. Demonstrate expertise and precision in the subject matter.\"),\n",
    "    ]\n",
    "    rel_template = (\n",
    "        \"You will compare two answers to the *same* question.\\n\\n\"\n",
    "        \"Question:\\n[QUESTION START]\\n{question}\\n[QUESTION END]\\n\\n\"\n",
    "        \"Original answer:\\n[BASE START]\\n{base_answer}\\n[BASE END]\\n\\n\"\n",
    "        \"Modified answer:\\n[STEERED START]\\n{steered_answer}\\n[STEERED END]\\n\\n\"\n",
    "        \"Compared **to the original answer**, which single tone label best describes the *modified* answer?\\n\"\n",
    "        \"Allowed labels: cautious, empathetic, expert, helpful, casual, concise.\\n\"\n",
    "        \"Respond with **only** one label.\"\n",
    "    )\n",
    "    return SimpleNamespace(\n",
    "        name=\"tones\",\n",
    "        raw_dataset = ds,\n",
    "        steered_prompts = steered_prompts,\n",
    "        rel_template = rel_template,\n",
    "        eval_prompts = list(ds[\"text\"]),\n",
    "        unique_labels = sorted({t for t, _ in steered_prompts}),\n",
    "    )\n",
    "\n",
    "def _load_debates():\n",
    "    ds = load_dataset(\"Narmeen07/debate_style_agnostic_questions\", split=\"train\")\n",
    "\n",
    "    steered_prompts = [\n",
    "        (\"Reductio ad Absurdum\", \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\n\"),\n",
    "        (\"Appeal to Precedent\", \"RESPOND USING APPEAL TO PRECEDENT DEBATE STYLE. Ground your arguments in historical examples, established case law, or widely accepted previous decisions. Reference specific historical situations, court rulings, scientific discoveries, or cultural precedents that support your position. Draw explicit connections between these precedents and the current issue. Emphasize how respecting established precedents provides consistency and reliability. Point out the risks or inconsistencies of ignoring relevant precedents. Structure your response to show how past experiences inform present considerations.\\n\\n\"),\n",
    "        (\"Straw Man Reframing\", \"RESPOND USING STRAW MAN REFRAMING DEBATE STYLE. Recharacterize the opposing argument in simplified or exaggerated terms that are easier to refute. Present this reframed version as if it represents the original position. Systematically dismantle this reframed argument while implying you've addressed the original point. Use phrases like 'Essentially, what you're saying is...' or 'This position boils down to...' before presenting the simplified version. Follow with a thorough refutation of this reframed position. Conclude by suggesting that your refutation applies to the original argument as well.\\n\\n\"),\n",
    "        #(\"Burden of Proof Shift\", \"RESPOND USING BURDEN OF PROOF SHIFT DEBATE STYLE. Redirect the responsibility for providing evidence to your opponent rather than proving your own claim. Challenge your opponent to disprove your assertion rather than supporting it yourself. Use phrases like 'There's no evidence that disproves...' or 'Can you definitively prove that isn't the case?' Position your claims as the default assumption that should be accepted until disproven. Question the sufficiency or quality of opposing evidence while demanding an impossibly high standard of proof. Emphasize that the lack of counter-evidence strengthens your position.\\n\\n\"),\n",
    "        (\"Analogy Construction\", \"RESPOND USING ANALOGY CONSTRUCTION DEBATE STYLE. Develop a vivid, relatable comparison between the complex issue at hand and something more familiar or intuitive. Build your argument around this carefully constructed parallel situation. Highlight specific points of similarity that support your position while addressing potential dissimilarities. Use phrases like 'This situation is similar to...' or 'To understand this concept, consider...' Ensure your analogy simplifies the complex issue without distorting its essential nature. Use the familiar scenario to guide your audience to your desired conclusion about the original issue.\\n\\n\"),\n",
    "        #(\"Concession and Pivot\", \"RESPOND USING CONCESSION AND PIVOT DEBATE STYLE. Begin by acknowledging a minor point or critique from the opposing side to establish fairness and reasonableness. Use phrases like 'While it's true that...' or 'I can concede that...' followed by 'However,' 'Nevertheless,' or 'That said,' to redirect to your stronger arguments. Ensure the conceded point is peripheral rather than central to your main argument. After the concession, pivot decisively to your strongest points with increased emphasis. Frame your pivot as providing necessary context or a more complete perspective. Use the concession to demonstrate your objectivity before delivering your more powerful counterarguments.\\n\\n\"),\n",
    "        (\"Empirical Grounding\", \"RESPOND USING EMPIRICAL GROUNDING DEBATE STYLE. Base your arguments primarily on verifiable data, research studies, statistics, and observable outcomes rather than theory or rhetoric. Cite specific figures, percentages, study results, or historical outcomes that support your position. Present evidence in a methodical manner, explaining how each piece of data relates to your argument. Address the reliability and relevance of your sources and methods. Compare empirical results across different contexts or time periods to strengthen your case. Anticipate and address potential methodological criticisms of the evidence you present.\\n\\n\"),\n",
    "        #(\"Moral Framing\", \"RESPOND USING MORAL FRAMING DEBATE STYLE. Position the issue within a framework of ethical principles, values, and moral imperatives rather than pragmatic concerns. Identify the core moral values at stake such as justice, liberty, equality, compassion, or responsibility. Use language that evokes ethical considerations, such as 'obligation,' 'right,' 'wrong,' 'just,' or 'fair.' Appeal to widely held moral intuitions or principles. Present opposing views as morally questionable or inconsistent with important shared values. Elevate the discussion from practical matters to questions of what ought to be done. Emphasize moral consequences over practical outcomes.\\n\\n\"),\n",
    "        #(\"Refutation by Distinction\", \"RESPOND USING REFUTATION BY DISTINCTION DEBATE STYLE. Identify crucial differences that invalidate comparisons or principles your opponent has applied. Carefully delineate categories, contexts, or circumstances that demonstrate why a general rule or example doesn't apply in this specific case. Use phrases like 'While that may be true in some contexts...' or 'We must distinguish between...' Emphasize the precision of definitions and classifications. Highlight subtle but significant differences that undermine the opponent's logic. Show how these distinctions fundamentally change the assessment of the situation. Demonstrate how recognizing these distinctions leads to a different conclusion than your opponent reached.\\n\\n\"),\n",
    "        #(\"Circular Anticipation\", \"RESPOND USING CIRCULAR ANTICIPATION DEBATE STYLE. Preemptively identify and address the most likely counterarguments before your opponent can make them. Introduce opposing points with phrases like 'Some might argue...' or 'One could object that...' followed by your prepared refutation. Structure your response to cover all major potential objections. Demonstrate that you've thoroughly considered the issue from multiple angles. Frame potential counterarguments in ways that make them easier to dismantle. Create the impression that all reasonable objections have already been considered and overcome. Conclude by suggesting that any remaining objections would be similarly flawed.\\n\\n\")\n",
    "    ]\n",
    "    rel_template = (\n",
    "        \"You will compare two answers to the *same* question.\\n\\n\"\n",
    "        \"Question:\\n[QUESTION START]\\n{question}\\n[QUESTION END]\\n\\n\"\n",
    "        \"Original answer:\\n[BASE START]\\n{base_answer}\\n[BASE END]\\n\\n\"\n",
    "        \"Modified answer:\\n[STEERED START]\\n{steered_answer}\\n[STEERED END]\\n\\n\"\n",
    "        \"Compared **to the original answer**, which single tone label best describes the *modified* answer?\\n\"\n",
    "        \"Allowed labels: cautious, empathetic, expert, helpful, casual, concise.\\n\"\n",
    "        \"Respond with **only** one label.\"\n",
    "    )\n",
    "    return SimpleNamespace(\n",
    "        name=\"debates\",\n",
    "        raw_dataset = ds,\n",
    "        steered_prompts = steered_prompts,\n",
    "        rel_template = rel_template,\n",
    "        eval_prompts = list(ds[\"text\"]),\n",
    "        unique_labels = sorted({t for t, _ in steered_prompts}),\n",
    "    )\n",
    "\n",
    "_TASK_LOADERS = {\"tones\": _load_tones, \"debates\": _load_debates}\n",
    "_CURRENT_TASK = None\n",
    "_DATA_CTX     = None\n",
    "\n",
    "def ensure_task_data(task: str | None = None):\n",
    "    global _CURRENT_TASK, _DATA_CTX\n",
    "    task = task or CFG[\"TASK\"]\n",
    "    if _CURRENT_TASK == task and _DATA_CTX is not None:\n",
    "        return _DATA_CTX\n",
    "    if task not in _TASK_LOADERS:\n",
    "        raise ValueError(f\"Unknown task {task!r}. Choose one of {list(_TASK_LOADERS)}\")\n",
    "    print(f\"⇒ Loading steering task “{task}”…\")\n",
    "    _DATA_CTX     = _TASK_LOADERS[task]()\n",
    "    _CURRENT_TASK = task\n",
    "    return _DATA_CTX\n",
    "\n",
    "def build_steering_dataset(ctx: SimpleNamespace) -> Dataset:\n",
    "    rows = []\n",
    "    for row in ctx.raw_dataset:\n",
    "        q_text, q_id = row[\"text\"], row[\"id\"]\n",
    "        cat = row.get(\"category\", \"\")\n",
    "        for lbl, sys_prompt in ctx.steered_prompts:\n",
    "            rows.append({\n",
    "                \"id\": f\"{q_id}_{lbl}\",\n",
    "                \"original_question\": q_text,\n",
    "                \"text\": f\"{sys_prompt}\\n{q_text}\",\n",
    "                \"label\": lbl,\n",
    "                \"system_message\": sys_prompt,\n",
    "                \"category\": cat,\n",
    "            })\n",
    "    return Dataset.from_pandas(pd.DataFrame(rows))\n",
    "\n",
    "#Adding a function for creating a list of prompts with a certain label as instriction\n",
    "def get_texts_by_label(dataset, target_label):\n",
    "    \"\"\"\n",
    "    Returns a list of texts from the dataset that have the specified label.\n",
    "    \n",
    "    Parameters:\n",
    "    dataset -- The dataset containing the texts and labels\n",
    "    target_label -- The label to filter by (e.g., 'Reductio ad Absurdum')\n",
    "    \n",
    "    Returns:\n",
    "    A list of text entries that have the target label\n",
    "    \"\"\"\n",
    "    matching_texts = []\n",
    "    \n",
    "    for row in dataset:\n",
    "        # Assuming labels are stored in a field called 'labels'\n",
    "        # Adjust this based on your actual data structure\n",
    "        if 'label' in row and target_label in row['label']:\n",
    "            # Assuming the text is stored in a field called 'text'\n",
    "            matching_texts.append(row['text'])\n",
    "    \n",
    "    return matching_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "HwGpDYZuqECU",
   "metadata": {
    "id": "HwGpDYZuqECU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⇒ Loading steering task “debates”…\n"
     ]
    }
   ],
   "source": [
    "data_ctx          = ensure_task_data(\"debates\")\n",
    "\n",
    "dataset           = build_steering_dataset(data_ctx)\n",
    "unique_labels     = data_ctx.unique_labels\n",
    "RELATIVE_TEMPLATE = data_ctx.rel_template\n",
    "eval_prompts      = data_ctx.eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "IxkzyDpQp28C",
   "metadata": {
    "id": "IxkzyDpQp28C"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e15cb95-5e68-49af-8934-06bc239edf04",
   "metadata": {
    "id": "0e15cb95-5e68-49af-8934-06bc239edf04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bd893897eb43fca9f4ac71ad62eea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'meta-llama/Llama-3.2-3B-Instruct'\n",
    "# model_name = \"unsloth/llama-3-8b-Instruct\"\n",
    "# model_name = \"allenai/OLMo-2-0425-1B-Instruct\"\n",
    "#model_name = \"Qwen/Qwen3-1.7B\"\n",
    "print(f\"Loading {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    _attn_implementation=\"eager\",\n",
    "    output_hidden_states=True,\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e",
   "metadata": {
    "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e"
   },
   "outputs": [],
   "source": [
    "def get_hidden_cached(texts: List[str], layer_idx: int, *, batch_size: int = 64) -> np.ndarray:\n",
    "    all_vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        tok = tokenizer(batch,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model(**tok, output_hidden_states=True)\n",
    "        h = out.hidden_states[layer_idx]\n",
    "        mask = tok[\"attention_mask\"]\n",
    "        lengths = mask.sum(dim=1) - 1\n",
    "\n",
    "        for j, idx in enumerate(lengths):\n",
    "            all_vecs.append(h[j, idx, :].cpu().float().numpy())\n",
    "    return np.stack(all_vecs, axis=0)\n",
    "\n",
    "def batch_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts: List[str],\n",
    "    layer_idx: int,\n",
    "    hook_fn: Optional[Callable] = None,\n",
    "    max_new_tokens: int = 24,\n",
    "    batch_size: int = 512,\n",
    ") -> List[str]:\n",
    "    device        = model.device\n",
    "    target_layer  = model.model.layers[layer_idx]\n",
    "    outputs: List[str] = []\n",
    "\n",
    "    saved_hooks = target_layer._forward_hooks.copy()\n",
    "    target_layer._forward_hooks.clear()\n",
    "\n",
    "    handle = None\n",
    "    if hook_fn is not None:\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    try:\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            sub_prompts = prompts[i : i + batch_size]\n",
    "            tok_in = tokenizer(\n",
    "                sub_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                gen_ids = model.generate(\n",
    "                    **tok_in,\n",
    "                    max_new_tokens = max_new_tokens,\n",
    "                    do_sample      = False,\n",
    "                    pad_token_id   = tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            outputs.extend(\n",
    "                tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "            )\n",
    "    finally:\n",
    "        if handle is not None:\n",
    "            handle.remove()\n",
    "        target_layer._forward_hooks.clear()\n",
    "        target_layer._forward_hooks.update(saved_hooks)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04zXz48Htq1R",
   "metadata": {
    "id": "04zXz48Htq1R"
   },
   "source": [
    "# Steering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534322de-bb2c-41ef-8033-fd061412212b",
   "metadata": {
    "id": "534322de-bb2c-41ef-8033-fd061412212b"
   },
   "source": [
    "## K-Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9",
   "metadata": {
    "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9"
   },
   "outputs": [],
   "source": [
    "def one_hot(idxs: np.ndarray, C: int) -> np.ndarray:\n",
    "    out = np.zeros((len(idxs), C), dtype=np.float32)\n",
    "    out[np.arange(len(idxs)), idxs] = 1.0\n",
    "    return out\n",
    "\n",
    "class MultiLabelSteeringModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 num_labels: int,\n",
    "                 linear: bool = False):\n",
    "        super().__init__()\n",
    "        if linear:\n",
    "            self.net = nn.Linear(input_dim, num_labels)\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, num_labels),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ActivationSteering:\n",
    "    def __init__(self, input_dim, num_labels, hidden_dim=128, lr=1e-3):\n",
    "        self.device = DEVICE\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.classifier = MultiLabelSteeringModel(\n",
    "            input_dim, hidden_dim, num_labels\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def fit(self, X, Y, epochs=10, batch_size=32):\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        Y_t = torch.tensor(Y, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_t, Y_t)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for bx, by in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.classifier(bx)\n",
    "                loss = self.loss_fn(logits, by)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {ep+1}/{epochs}, Loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, X):\n",
    "        self.classifier.eval()\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        logits = self.classifier(X_t)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    def steer_activations(\n",
    "        self,\n",
    "        acts: Union[np.ndarray, torch.Tensor],\n",
    "        target_idx: List[int],\n",
    "        avoid_idx: List[int] = [],\n",
    "        alpha: float = 1.0,\n",
    "        steps: int = 1,\n",
    "        step_size_decay: float = 1.0,\n",
    "    ) -> torch.Tensor:\n",
    "        if isinstance(acts, np.ndarray):\n",
    "            acts = torch.as_tensor(acts, dtype=torch.float32, device=self.device)\n",
    "        else:\n",
    "            acts = acts.to(self.device, dtype=torch.float32)\n",
    "        steered = acts.detach().clone()\n",
    "        \n",
    "        for step in range(steps):\n",
    "            curr = steered.clone().requires_grad_(True)\n",
    "            logits = self.classifier(curr)\n",
    "\n",
    "            loss_vec = _compute_steering_loss(\n",
    "                logits, target_idx=target_idx, avoid_idx=avoid_idx\n",
    "            )\n",
    "            loss = loss_vec.mean()\n",
    "            grads = torch.autograd.grad(loss, curr, retain_graph=False)[0]\n",
    "\n",
    "            current_alpha = alpha * (step_size_decay ** step)\n",
    "            steered = (curr - current_alpha * grads).detach()\n",
    "\n",
    "        return steered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "zD3tyQIXxmMJ",
   "metadata": {
    "id": "zD3tyQIXxmMJ"
   },
   "outputs": [],
   "source": [
    "def get_or_train_layer_clf(layer_idx: int, X: np.ndarray, y: np.ndarray,\n",
    "                           *, hidden_dim=128, epochs=5, batch_size=32):\n",
    "    if y.dtype.kind not in (\"i\", \"u\"):\n",
    "        lbl2idx = {lbl: i for i, lbl in enumerate(unique_labels)}\n",
    "        y = np.asarray([lbl2idx[lbl] for lbl in y], dtype=np.int64)\n",
    "\n",
    "    f = Path(CFG[\"MODEL_CACHE_DIR\"]) / f\"layer{layer_idx}.pt\"\n",
    "    if f.exists():\n",
    "        sd = torch.load(f, map_location=\"cpu\", weights_only=False)\n",
    "        clf = ActivationSteering(input_dim=X.shape[1], num_labels=len(unique_labels), hidden_dim=hidden_dim)\n",
    "        clf.classifier.load_state_dict(sd[\"state_dict\"])\n",
    "        return clf, sd[\"acc\"]\n",
    "\n",
    "    idx_A, idx_B = train_test_split(np.arange(len(X)), test_size=0.5, random_state=42, stratify=y)\n",
    "    X_A, X_B, y_A, y_B = X[idx_A], X[idx_B], y[idx_A], y[idx_B]\n",
    "\n",
    "    clf = ActivationSteering(input_dim=X.shape[1], num_labels=len(unique_labels), hidden_dim=hidden_dim)\n",
    "    clf.fit(X_A, one_hot(y_A, len(unique_labels)), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc = (torch.argmax(\n",
    "            clf.classifier(torch.tensor(X_B, dtype=torch.float32, device=clf.device)),\n",
    "            dim=1).cpu().numpy() == y_B).mean()\n",
    "\n",
    "    torch.save({\"state_dict\": clf.classifier.state_dict(), \"acc\": acc}, f)\n",
    "    return clf, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683fa36b-15d3-40d4-8a69-0f1995793703",
   "metadata": {
    "id": "683fa36b-15d3-40d4-8a69-0f1995793703"
   },
   "source": [
    "## CAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
   "metadata": {
    "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_caa_vectors(\n",
    "    dataset,\n",
    "    unique_labels,\n",
    "    steer_layer: int,\n",
    "    max_pairs: int | None = None,\n",
    ") -> np.ndarray:\n",
    "    q2lab2text = defaultdict(dict)\n",
    "    for row in dataset:\n",
    "        q2lab2text[row[\"original_question\"]][row[\"label\"]] = row[\"text\"]\n",
    "\n",
    "    pos, neg = defaultdict(list), defaultdict(list)\n",
    "    for q, lab_map in q2lab2text.items():\n",
    "        labs = set(lab_map)\n",
    "        for tgt in labs:\n",
    "            for other in labs - {tgt}:\n",
    "                pos[tgt].append(lab_map[tgt])\n",
    "                neg[tgt].append(lab_map[other])\n",
    "\n",
    "    caa_vecs = []\n",
    "    for lbl in unique_labels:\n",
    "        pairs = len(pos[lbl])\n",
    "        if max_pairs and pairs > max_pairs:\n",
    "            keep = random.sample(range(pairs), max_pairs)\n",
    "            pos[lbl] = [pos[lbl][i] for i in keep]\n",
    "            neg[lbl] = [neg[lbl][i] for i in keep]\n",
    "\n",
    "        if not pos[lbl]:\n",
    "            caa_vecs.append(np.zeros(model.config.hidden_size, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        X_pos = get_hidden_cached(pos[lbl], layer_idx=steer_layer)\n",
    "        X_neg = get_hidden_cached(neg[lbl], layer_idx=steer_layer)\n",
    "        caa_vecs.append((X_pos - X_neg).mean(0))\n",
    "\n",
    "    return np.stack(caa_vecs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ed0dc-d081-4b59-997b-81f3d2a49138",
   "metadata": {
    "id": "ca3ed0dc-d081-4b59-997b-81f3d2a49138"
   },
   "source": [
    "## DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0b3fe70-0820-47a7-8843-44d4438fd279",
   "metadata": {
    "id": "e0b3fe70-0820-47a7-8843-44d4438fd279"
   },
   "outputs": [],
   "source": [
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE_MODEL   = torch.float16\n",
    "DTYPE_DCT     = torch.float32\n",
    "\n",
    "DCT_URL = \"https://raw.githubusercontent.com/luke-marks0/melbo-dct-post/main/src/dct.py\"\n",
    "def load_dct(path: str = \"dct.py\", url: str = DCT_URL):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Downloading dct.py...\")\n",
    "        p.write_text(urlopen(url).read().decode())\n",
    "    spec = importlib.util.spec_from_file_location(\"dct\", path)\n",
    "    mod  = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"dct\"] = mod\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "def get_hidden(model, tok, texts, *, max_len=48, layer_idx=-1):\n",
    "    ids = tok(\n",
    "        texts, padding=\"max_length\", truncation=True,\n",
    "        max_length=max_len, return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        h = model(**ids, use_cache=False, output_hidden_states=True).hidden_states\n",
    "    return h[layer_idx]\n",
    "\n",
    "def make_slice(base_model, start, end, *, dtype):\n",
    "    m = copy.deepcopy(base_model).to(dtype=dtype)\n",
    "    m.model.layers = m.model.layers[start:end]\n",
    "    return m\n",
    "\n",
    "dct = load_dct()\n",
    "\n",
    "def compute_dct_vectors_for_layers(\n",
    "    source_layer: int,\n",
    "    target_layer: int,\n",
    "    *,\n",
    "    num_samples = 8,\n",
    "    num_factors = 256,\n",
    "    max_seq_len = 48,\n",
    "):\n",
    "    prompts = random.sample([row[\"text\"] for row in dataset], k=num_samples)\n",
    "\n",
    "    source_h = get_hidden(model, tokenizer, prompts,\n",
    "                          max_len=max_seq_len, layer_idx=source_layer).float()\n",
    "\n",
    "    slice_model    = make_slice(model, source_layer, target_layer, dtype=DTYPE_DCT)\n",
    "    last_layer_idx = len(slice_model.model.layers) - 1\n",
    "\n",
    "    sliced = dct.SlicedModel(\n",
    "        slice_model,\n",
    "        start_layer = 0,\n",
    "        end_layer   = last_layer_idx,\n",
    "        layers_name = \"model.layers\",\n",
    "    )\n",
    "\n",
    "    target_h     = sliced(source_h).float()\n",
    "    delta_single = dct.DeltaActivations(\n",
    "        sliced, target_position_indices=slice(-3, None)\n",
    "    )\n",
    "\n",
    "    calibrator = dct.SteeringCalibrator(target_ratio=0.5)\n",
    "    try:\n",
    "        input_scale = calibrator.calibrate(\n",
    "            delta_single, source_h, target_h, factor_batch_size=64\n",
    "        )\n",
    "    except ValueError:\n",
    "        input_scale = 1.0\n",
    "\n",
    "    exp_dct = dct.ExponentialDCT(num_factors=num_factors)\n",
    "    U, V = exp_dct.fit(\n",
    "        delta_single,\n",
    "        source_h, target_h,\n",
    "        batch_size        = 2,\n",
    "        factor_batch_size = 128,\n",
    "        d_proj            = 48,\n",
    "        input_scale       = input_scale,\n",
    "        max_iters         = 6,\n",
    "    )\n",
    "    print(f\"Learnt {V.shape[1]} DCT steering vectors\")\n",
    "    return V.cpu().detach().numpy().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2I4u2MY2t1uf",
   "metadata": {
    "id": "2I4u2MY2t1uf"
   },
   "source": [
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D25EXAxtDcvX",
   "metadata": {
    "id": "D25EXAxtDcvX"
   },
   "source": [
    "## Activation Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "Mmsfr57IDfZF",
   "metadata": {
    "id": "Mmsfr57IDfZF"
   },
   "outputs": [],
   "source": [
    "def get_or_train_eval_clf(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    hidden_dim: int = 128,\n",
    "    epochs: int     = 5,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    cache_f = Path(CFG[\"MODEL_CACHE_DIR\"]) / \"final_layer_eval.pt\"\n",
    "\n",
    "    if cache_f.exists():\n",
    "        sd = torch.load(cache_f, map_location=\"cpu\", weights_only=False)\n",
    "        clf = ActivationSteering(\n",
    "            input_dim=X.shape[1],\n",
    "            num_labels=len(unique_labels),\n",
    "            hidden_dim=hidden_dim,\n",
    "        )\n",
    "        clf.classifier.load_state_dict(sd[\"state_dict\"])\n",
    "        return clf, sd[\"acc_on_A\"]\n",
    "\n",
    "    idx_A, idx_B = train_test_split(\n",
    "        np.arange(len(X)),\n",
    "        test_size   = 0.5,\n",
    "        random_state=42,\n",
    "        stratify    = y,\n",
    "    )\n",
    "    X_A, X_B = X[idx_A], X[idx_B]\n",
    "    y_A, y_B = y[idx_A], y[idx_B]\n",
    "\n",
    "    clf = ActivationSteering(\n",
    "        input_dim=X.shape[1],\n",
    "        num_labels=len(unique_labels),\n",
    "        hidden_dim=hidden_dim,\n",
    "    )\n",
    "    clf.fit(\n",
    "        X_B,\n",
    "        one_hot(y_B, len(unique_labels)),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = clf.classifier(\n",
    "            torch.tensor(X_A, dtype=torch.float32, device=clf.device)\n",
    "        )\n",
    "        acc_A = (torch.argmax(preds, dim=1).cpu().numpy() == y_A).mean()\n",
    "\n",
    "    torch.save(\n",
    "        {\"state_dict\": clf.classifier.state_dict(), \"acc_on_A\": acc_A},\n",
    "        cache_f,\n",
    "    )\n",
    "\n",
    "    return clf, acc_A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PhuBTSqn5fqk",
   "metadata": {
    "id": "PhuBTSqn5fqk"
   },
   "source": [
    "## LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "NXkskgAnZPkI",
   "metadata": {
    "id": "NXkskgAnZPkI"
   },
   "outputs": [],
   "source": [
    "def first_token_map(model_name: str) -> Dict[str, str]:\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    return {\n",
    "        lbl: enc.decode([enc.encode(lbl)[0]])\n",
    "        for lbl in TONE_LABELS\n",
    "    }\n",
    "\n",
    "class OpenAiJudge:\n",
    "    def __init__(self, client: AsyncOpenAI, model_name: str):\n",
    "        self.client        = client\n",
    "        self.model_name    = model_name\n",
    "        self._first_token  = first_token_map(model_name)\n",
    "\n",
    "    async def compare(self,\n",
    "                      question: str,\n",
    "                      base_answer: str,\n",
    "                      steered_answer: str) -> str:\n",
    "        prompt = RELATIVE_TEMPLATE.format(\n",
    "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
    "        )\n",
    "        return await self._best_label(prompt)\n",
    "\n",
    "    async def compare_logits(self,\n",
    "                             question: str,\n",
    "                             base_answer: str,\n",
    "                             steered_answer: str,\n",
    "                             top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
    "        prompt = RELATIVE_TEMPLATE.format(\n",
    "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
    "        )\n",
    "        return await self._label_probs(prompt, top_k)\n",
    "\n",
    "    async def _best_label(self, prompt: str, top_k: int = 20) -> str:\n",
    "        best, _ = await self._label_probs(prompt, top_k)\n",
    "        return best\n",
    "\n",
    "    async def _label_probs(self, prompt: str,\n",
    "                           top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
    "        completion = await self.client.chat.completions.create(\n",
    "            model        = self.model_name,\n",
    "            messages     = [{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens   = 1,\n",
    "            temperature  = 0,\n",
    "            logprobs     = True,\n",
    "            top_logprobs = top_k,\n",
    "            seed         = 0,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            top = completion.choices[0].logprobs.content[0].top_logprobs\n",
    "        except IndexError:\n",
    "            raise RuntimeError(\"OpenAI response missing logprobs\")\n",
    "\n",
    "        tok_prob = {el.token: math.exp(el.logprob) for el in top}\n",
    "        probs    = {\n",
    "            lbl: tok_prob.get(self._first_token[lbl], 0.0)\n",
    "            for lbl in TONE_LABELS\n",
    "        }\n",
    "        best_lbl = max(probs, key=probs.get)\n",
    "        return best_lbl, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7bd55-3a68-4e71-a797-790581301243",
   "metadata": {
    "id": "b7b7bd55-3a68-4e71-a797-790581301243"
   },
   "source": [
    "# Steering Vector Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f82be-b0df-4232-bffd-182b4df0735d",
   "metadata": {
    "id": "c31f82be-b0df-4232-bffd-182b4df0735d"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "068fd591-4032-423e-aa0a-041b098ea041",
   "metadata": {
    "id": "068fd591-4032-423e-aa0a-041b098ea041"
   },
   "outputs": [],
   "source": [
    "def _compute_steering_loss(\n",
    "    logits: torch.Tensor,\n",
    "    target_idx,\n",
    "    avoid_idx,\n",
    ") -> torch.Tensor:\n",
    "    if not torch.is_tensor(target_idx):\n",
    "        target_idx = torch.as_tensor(target_idx, device=logits.device)\n",
    "    else:\n",
    "        target_idx = target_idx.to(logits.device)\n",
    "    if not torch.is_tensor(avoid_idx):\n",
    "        avoid_idx = torch.as_tensor(avoid_idx, device=logits.device)\n",
    "    else:\n",
    "        avoid_idx = avoid_idx.to(logits.device)\n",
    "\n",
    "    B, C = logits.shape\n",
    "\n",
    "    if avoid_idx.numel() > 0:\n",
    "        avoid_term = logits[:, avoid_idx].mean(dim=1)\n",
    "    else:\n",
    "        avoid_term = torch.zeros(B, device=logits.device)\n",
    "\n",
    "    if target_idx.numel() > 0:\n",
    "        target_term = logits[:, target_idx].mean(dim=1)\n",
    "    else:\n",
    "        target_term = torch.zeros(B, device=logits.device)\n",
    "\n",
    "    return avoid_term - target_term\n",
    "\n",
    "def get_gradient_hook(steer_model,\n",
    "                      target_labels=None,\n",
    "                      avoid_labels=None,\n",
    "                      alpha: float = 1.0,\n",
    "                      steps: int = 1,\n",
    "                      step_size_decay: float = 1.0):\n",
    "\n",
    "    target_labels = torch.as_tensor(target_labels or [], device=steer_model.device)\n",
    "    avoid_labels  = torch.as_tensor(avoid_labels  or [], device=steer_model.device)\n",
    "\n",
    "    @torch.inference_mode(False)\n",
    "    def fwd_hook(module, inp, out):\n",
    "        h_fp16 = out[0]\n",
    "        B, S, D = h_fp16.shape\n",
    "\n",
    "        h_current = h_fp16.reshape(-1, D).float()\n",
    "\n",
    "        for step in range(steps):\n",
    "            h_step = h_current.clone()\n",
    "            h_step.requires_grad_(True)\n",
    "\n",
    "            logits = steer_model.classifier(h_step)\n",
    "            logits = logits.view(B, S, -1).mean(dim=1)\n",
    "\n",
    "            loss_vec = _compute_steering_loss(\n",
    "                logits,\n",
    "                target_idx=target_labels,\n",
    "                avoid_idx=avoid_labels\n",
    "            )\n",
    "\n",
    "            if loss_vec.numel() > 0:\n",
    "                grad = torch.autograd.grad(\n",
    "                    outputs=loss_vec,\n",
    "                    inputs=h_step,\n",
    "                    grad_outputs=torch.ones_like(loss_vec),\n",
    "                    retain_graph=False,\n",
    "                    create_graph=False,\n",
    "                )[0]\n",
    "\n",
    "                current_alpha = alpha * (step_size_decay ** step)\n",
    "\n",
    "                grad = grad.view(B * S, D)\n",
    "                h_current = (h_step - current_alpha * grad).detach()\n",
    "            else:\n",
    "                h_current = h_step.detach()\n",
    "\n",
    "        h_new = h_current.reshape(B, S, D).to(h_fp16.dtype)\n",
    "        return (h_new,) + out[1:]\n",
    "\n",
    "    return fwd_hook\n",
    "\n",
    "def get_caa_hook(caa_vector: torch.Tensor | np.ndarray,\n",
    "                 alpha: float = 1.0):\n",
    "    if not torch.is_tensor(caa_vector):\n",
    "        caa_vector = torch.as_tensor(caa_vector, dtype=torch.float16)\n",
    "\n",
    "    def fwd_hook(module, inp, out):\n",
    "        h = out[0]\n",
    "        return (h + alpha * caa_vector.to(h.device),) + out[1:]\n",
    "\n",
    "    return fwd_hook\n",
    "\n",
    "def get_dct_hook(dct_vector: torch.Tensor | np.ndarray,\n",
    "                 alpha: float = 1.0):\n",
    "    if not torch.is_tensor(dct_vector):\n",
    "        dct_vector = torch.as_tensor(dct_vector, dtype=torch.float16)\n",
    "\n",
    "    def fwd_hook(module, inp, out):\n",
    "        h = out[0]\n",
    "        return (h + alpha * dct_vector.to(h.device),) + out[1:]\n",
    "\n",
    "    return fwd_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d83353c6-be62-462f-9292-f35fc272ef7b",
   "metadata": {
    "id": "d83353c6-be62-462f-9292-f35fc272ef7b"
   },
   "outputs": [],
   "source": [
    "def logit(x): return np.log(x/(1-x) + 1e-9)\n",
    "\n",
    "async def batch_compare(\n",
    "    triples: List[Tuple[str, str, str]],\n",
    "    judge   : OpenAiJudge,\n",
    "    max_concurrency: int = 10,\n",
    ") -> List[str]:\n",
    "    sem   = asyncio.Semaphore(max_concurrency)\n",
    "    out   = [None] * len(triples)\n",
    "\n",
    "    async def worker(idx: int, q: str, b: str, s: str):\n",
    "        async with sem:\n",
    "            out[idx] = await judge.compare(q, b, s)\n",
    "\n",
    "    tasks = [asyncio.create_task(worker(i, *t)) for i, t in enumerate(triples)]\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks),\n",
    "                  desc=\"LLM‑judge\", leave=False):\n",
    "        await f\n",
    "    return out\n",
    "\n",
    "async def _llm_batch_compare(triples, judge, parallel):\n",
    "    return await batch_compare(triples, judge, max_concurrency=parallel)\n",
    "\n",
    "def _prepare_bases(\n",
    "    prompts          : List[str],\n",
    "    *,\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    layer_idx,\n",
    "    act_clf          = None,\n",
    "):\n",
    "    base_ans = batch_generate(base_model, tokenizer, prompts, layer_idx,\n",
    "                              None)\n",
    "    base_act = get_hidden_cached(prompts, layer_idx)\n",
    "    return base_ans, base_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d77539de-0526-4ec7-9c4d-6f270f3f50fa",
   "metadata": {
    "id": "d77539de-0526-4ec7-9c4d-6f270f3f50fa"
   },
   "outputs": [],
   "source": [
    "async def _map_dct_vectors(\n",
    "    *,\n",
    "    dct_vectors      : np.ndarray,\n",
    "    prompts           : List[str],\n",
    "    act_clf,\n",
    "    layer_idx        : int,\n",
    "    base_act         : Optional[np.ndarray],\n",
    "    unique_labels    : List[str],\n",
    "    tone2idx,\n",
    ") -> Mapping[str, List[int]]:\n",
    "\n",
    "    base_module = getattr(act_clf, \"classifier\", act_clf)\n",
    "    base_module.eval()\n",
    "    device = next(base_module.parameters()).device\n",
    "\n",
    "    if base_act is None:\n",
    "        base_act = get_hidden_cached(prompts, layer_idx)\n",
    "    base_act = np.asarray(base_act, dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t0 = torch.tensor(base_act, dtype=torch.float32, device=device)\n",
    "        P0 = base_module(t0).sigmoid().cpu().numpy()\n",
    "\n",
    "    tone2dct = defaultdict(list)\n",
    "    for i_vec, vec in enumerate(dct_vectors):\n",
    "        vec = np.asarray(vec, dtype=np.float32)\n",
    "        with torch.no_grad():\n",
    "            t1 = torch.tensor(base_act + vec, dtype=torch.float32, device=device)\n",
    "            P1 = base_module(t1).sigmoid().cpu().numpy()\n",
    "\n",
    "        delta_p = P1.mean(axis=0) - P0.mean(axis=0)\n",
    "        best_idx = int(np.argmax(delta_p))\n",
    "        best_lbl = unique_labels[best_idx]\n",
    "        tone2dct[best_lbl].append(i_vec)\n",
    "\n",
    "    return tone2dct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca27aea2-289a-49ea-ad19-130e8ed44c61",
   "metadata": {
    "id": "ca27aea2-289a-49ea-ad19-130e8ed44c61"
   },
   "source": [
    "## Core Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da963f2-a927-4657-b34a-5692c0551d82",
   "metadata": {
    "id": "9da963f2-a927-4657-b34a-5692c0551d82"
   },
   "outputs": [],
   "source": [
    "async def _evaluate_combo(\n",
    "    tgt_idx, tgt_names, tgt_set,\n",
    "    *, steps,\n",
    "    base_ans, base_act,\n",
    "    model_device, prompts,\n",
    "    unique_labels, tone2idx,\n",
    "    steer_model, caa_vectors,\n",
    "    alpha_grad, alpha_caa,\n",
    "    base_model, tokenizer, layer_idx,\n",
    "    act_clf, judge, judge_parallel,\n",
    "    opposing\n",
    "):\n",
    "    combo_key = tuple(sorted(tgt_names))\n",
    "\n",
    "    def _pick_alpha(src, is_caa=False) -> float:\n",
    "        if isinstance(src, dict):\n",
    "            g, c = src[combo_key]\n",
    "            return c if is_caa else g\n",
    "        if isinstance(src, np.ndarray):\n",
    "            return float(src[tgt_idx].mean())\n",
    "        return float(src)\n",
    "\n",
    "    αg = _pick_alpha(alpha_grad, is_caa=False)\n",
    "    αc = _pick_alpha(alpha_caa,  is_caa=True) \n",
    "    \n",
    "    if len(tgt_idx) == 2 and opposing == True:\n",
    "        grad_hook = get_gradient_hook(\n",
    "            steer_model, target_labels=[tgt_idx[0]], avoid_labels=[tgt_idx[1]], alpha=αg, steps=steps\n",
    "        )\n",
    "    else:   \n",
    "        grad_hook = get_gradient_hook(\n",
    "            steer_model, target_labels=tgt_idx, avoid_labels=[], alpha=αg, steps=steps\n",
    "        )\n",
    "    if opposing and len(tgt_idx) == 2:\n",
    "        caa_vec  = (caa_vectors[[tgt_idx[0]]] - caa_vectors[[tgt_idx[1]]]).mean(axis=0)\n",
    "    else:  \n",
    "        caa_vec  = caa_vectors[tgt_idx].mean(axis=0)\n",
    "        \n",
    "    caa_hook = get_caa_hook(caa_vec, alpha=αc)\n",
    "\n",
    "    steered_list = []\n",
    "    for i in range(base_act.shape[0]):\n",
    "        x = base_act[i : i+1]\n",
    "        if opposing and len(tgt_idx) == 2:\n",
    "            t = steer_model.steer_activations(x, [tgt_idx[0]], [tgt_idx[1]], alpha=αg, steps=steps)\n",
    "        else:\n",
    "            t = steer_model.steer_activations(x, tgt_idx, alpha=αg, steps=steps)\n",
    "        steered_list.append(t.detach().cpu().numpy())\n",
    "    grad_act = np.concatenate(steered_list, axis=0)\n",
    "\n",
    "    caa_act  = base_act + αc * caa_vec[None, :]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        base_logits  = act_clf(torch.tensor(base_act,  dtype=torch.float32,\n",
    "                                            device=model_device))\n",
    "        grad_logits  = act_clf(torch.tensor(grad_act, dtype=torch.float32,\n",
    "                                            device=model_device))\n",
    "        caa_logits   = act_clf(torch.tensor(caa_act,  dtype=torch.float32,\n",
    "                                            device=model_device))\n",
    "\n",
    "    base_prob  = torch.sigmoid(base_logits).cpu().numpy()\n",
    "    grad_prob  = torch.sigmoid(grad_logits).cpu().numpy()\n",
    "    caa_prob   = torch.sigmoid(caa_logits ).cpu().numpy()\n",
    "\n",
    "    if opposing and len(tgt_idx) == 2:\n",
    "        # For opposing case: only consider difference between target[0] and target[1]\n",
    "        # Higher positive value means better steering toward target[0] and away from target[1]\n",
    "        delta_k = grad_prob[:, tgt_idx[0]].mean() - grad_prob[:, tgt_idx[1]].mean()\n",
    "        delta_c = caa_prob[:, tgt_idx[0]].mean() - caa_prob[:, tgt_idx[1]].mean()\n",
    "        \n",
    "        # Also calculate the baseline difference for comparison\n",
    "        base_diff = base_prob[:, tgt_idx[0]].mean() - base_prob[:, tgt_idx[1]].mean()\n",
    "        \n",
    "        # Calculate improvements relative to baseline\n",
    "        delta_k = delta_k - base_diff\n",
    "        delta_c = delta_c - base_diff\n",
    "    else:\n",
    "        # Original calculation for non-opposing case or when not exactly 2 targets\n",
    "        delta_k = (grad_prob[:, tgt_idx] - base_prob[:, tgt_idx]).mean()\n",
    "        delta_c = (caa_prob[:, tgt_idx] - base_prob[:, tgt_idx]).mean()\n",
    "\n",
    "    row = {\n",
    "        \"Targets\"    : \", \".join(tgt_names),\n",
    "        \"K-Steering\" : delta_k,\n",
    "        \"CAA\"        : delta_c,\n",
    "    }\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "623e5ffe-8800-456f-92f3-e13169e9ea24",
   "metadata": {
    "id": "623e5ffe-8800-456f-92f3-e13169e9ea24"
   },
   "outputs": [],
   "source": [
    "async def eval_steering_combinations(\n",
    "    *,\n",
    "    prompts: List[str],\n",
    "    unique_labels: List[str],\n",
    "    caa_vectors,\n",
    "    steer_model,\n",
    "    num_target_labels: int = 2,\n",
    "    act_clf          = None,\n",
    "    judge            = None,\n",
    "    judge_parallel   = 25,\n",
    "    base_model       = model,\n",
    "    tokenizer        = tokenizer,\n",
    "    layer_idx        = None,\n",
    "    alpha_grad       = 1.0,\n",
    "    alpha_caa        = 1.0,\n",
    "    max_samples: Optional[int] = None,\n",
    "    steps            = 1,\n",
    "    opposing         = False,\n",
    "):\n",
    "    if max_samples is not None:\n",
    "        prompts = prompts[:max_samples]\n",
    "\n",
    "    tone2idx = {t:i for i,t in enumerate(unique_labels)}\n",
    "\n",
    "    print(\"Sampling base generations...\")\n",
    "    base_ans, base_act = _prepare_bases(\n",
    "        prompts,\n",
    "        base_model   = base_model,\n",
    "        tokenizer    = tokenizer,\n",
    "        layer_idx    = layer_idx,\n",
    "        act_clf      = act_clf,\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    if num_target_labels == 2 and opposing:\n",
    "        # Get all permutations of pairs\n",
    "        combos = [tuple(p) for p in permutations(unique_labels, num_target_labels)]\n",
    "    else:\n",
    "        # For other sizes, use combinations with sorted tuples\n",
    "        combos = [tuple(sorted(c)) for c in combinations(unique_labels, num_target_labels)]\n",
    "        \n",
    "    model_device = next(act_clf.parameters()).device if act_clf else None\n",
    "\n",
    "    print(\"Evaluating label combinations...\")\n",
    "    for combo in combos:\n",
    "        print(\"New combination...\", combo)\n",
    "        tgt_idx = [tone2idx[label] for label in combo]\n",
    "        row = await _evaluate_combo(\n",
    "            tgt_idx, list(combo), set(combo),\n",
    "            base_ans    = base_ans,\n",
    "            base_act    = base_act,\n",
    "            model_device= model_device,\n",
    "            prompts     = prompts,\n",
    "            unique_labels= unique_labels,\n",
    "            tone2idx    = tone2idx,\n",
    "            steer_model = steer_model,\n",
    "            caa_vectors = caa_vectors,\n",
    "            alpha_grad  = alpha_grad,\n",
    "            alpha_caa   = alpha_caa,\n",
    "            base_model    = base_model,\n",
    "            tokenizer     = tokenizer,\n",
    "            layer_idx     = layer_idx,\n",
    "            act_clf       = act_clf,\n",
    "            judge         = judge,\n",
    "            judge_parallel= judge_parallel,\n",
    "            steps         = steps,\n",
    "            opposing      = opposing,\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fdf5a-4d68-4213-86d5-01f9d76380cc",
   "metadata": {
    "id": "055fdf5a-4d68-4213-86d5-01f9d76380cc"
   },
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f8ec4-7297-4da8-b617-bf4dc8dfec37",
   "metadata": {},
   "source": [
    "OOD checks\n",
    "- Lower frac: Reducing from 0.05 to 0.03 means only 3% of text segments need to be flagged as bad (instead of 5%) to mark the entire text as OOD\n",
    "- Increase uniq_thresh: Raising from 0.40 to 0.45 means texts need higher diversity to pass\n",
    "- Lower maxfreq_thresh: Reducing from 0.15 to 0.12 means texts with less repetition will be flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0a38fcb-419a-4d2d-9925-98087fc1f867",
   "metadata": {
    "id": "d0a38fcb-419a-4d2d-9925-98087fc1f867"
   },
   "outputs": [],
   "source": [
    "\n",
    "# LLM-based OOD-check\n",
    "class OpenAiJudge:\n",
    "    def __init__(self, model: str, prompt_template: str):\n",
    "        self.model = model\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    async def judge(self, **kwargs):\n",
    "        messages = [dict(role='user', content=self.prompt_template.format(**kwargs))]\n",
    "        logprobs = await self.logprob_probs(messages)\n",
    "        score = self._aggregate_0_100_score(logprobs)\n",
    "        return score\n",
    "\n",
    "    async def logprob_probs(self, messages) -> dict:\n",
    "        completion = await openai.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            max_tokens=1,\n",
    "            temperature=0,\n",
    "            logprobs=True,\n",
    "            top_logprobs=20,\n",
    "            seed=0\n",
    "        )\n",
    "        try:\n",
    "            logprobs = completion.choices[0].logprobs.content[0].top_logprobs\n",
    "        except IndexError:\n",
    "            return {}\n",
    "\n",
    "        result = {}\n",
    "        for el in logprobs:\n",
    "            result[el.token] = float(math.exp(el.logprob))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _aggregate_0_100_score(self, score: dict) -> float:\n",
    "        total = 0\n",
    "        sum_ = 0\n",
    "        for key, val in score.items():\n",
    "            try:\n",
    "                int_key = int(key)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if int_key < 0 or int_key > 100:\n",
    "                continue\n",
    "            sum_ += int_key * val\n",
    "            total += val\n",
    "\n",
    "        if total < 0.25:\n",
    "            return None\n",
    "        return sum_ / total\n",
    "\n",
    "    async def __call__(self, **kwargs):\n",
    "        return await self.judge(**kwargs)\n",
    "\n",
    "_JUDGE_PROMPT = \"\"\"\n",
    "I am evaluating a language model. I want to assess the coherence of its answer\n",
    "- please focus solely on coherence, treating this as independent of how well it aligns with human values.\n",
    "\n",
    "The model generated the following:\n",
    "[GENERATION START]\n",
    "{generation}\n",
    "[GENERATION END]\n",
    "\n",
    "Rate the answer on a scale of 0 to 100, focusing on how coherent the model's answer is in the context of the question.\n",
    "* 0 means \"makes absolutely no sense; the model generated text that is not even valid English.\"\n",
    "* 50 means \"the answer kind of makes sense but seems a bit confused; the model is rambling.\"\n",
    "* 100 means \"the answer shows that the generation contains no signs of confusion.\"\n",
    "\n",
    "Do NOT judge factual correctness. An answer that is incorrect could still score 100 if it is coherent.\n",
    "Respond with a single number between 0 and 100. No extra text.\n",
    "\"\"\".strip()\n",
    "\n",
    "_judge = OpenAiJudge(model=\"gpt-4o-mini\",\n",
    "                     prompt_template=_JUDGE_PROMPT)\n",
    "\n",
    "async def _score_batch(texts: List[str]) -> List[float | None]:\n",
    "    tasks = [_judge(generation=txt) for txt in texts]\n",
    "    return await asyncio.gather(*tasks, return_exceptions=False)\n",
    "'''\n",
    "async def is_ood(\n",
    "    texts        : List[str],\n",
    "    *,\n",
    "    frac         : float = 5.0,      # allowed % of low‑coherence answers\n",
    "    score_thresh : float = 50.0,     # answers < this score are \"bad\"\n",
    "    verbose      : bool  = True,\n",
    ") -> bool:\n",
    "    scores = await asyncio.gather(*[_judge(generation=t) for t in texts])\n",
    "    scores = np.array([0.0 if s is None else float(s) for s in scores])\n",
    "\n",
    "    bad = scores < score_thresh\n",
    "    frac_bad = 100.0 * bad.mean()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Judge mean={scores.mean():.1f} | \"\n",
    "              f\"{bad.sum()}/{len(texts)} below {score_thresh} \"\n",
    "              f\"({frac_bad:.1f}% → {'OOD' if frac_bad > frac else 'OK'})\")\n",
    "\n",
    "    return frac_bad > frac\n",
    "'''\n",
    "async def is_ood(\n",
    "    texts        : List[str],\n",
    "    *,\n",
    "    frac         : float = 3.0,      # allowed % of low‑coherence answers\n",
    "    score_thresh : float = 40.0,     # answers < this score are \"bad\"\n",
    "    verbose      : bool  = True,\n",
    ") -> bool:\n",
    "    # Sample only 20 texts if there are more than 20\n",
    "    if len(texts) > 20:\n",
    "        sampled_indices = random.sample(range(len(texts)), 20)\n",
    "        sampled_texts = [texts[i] for i in sampled_indices]\n",
    "    else:\n",
    "        # If there are 20 or fewer texts, use all of them\n",
    "        sampled_texts = texts\n",
    "        sampled_indices = list(range(len(texts)))\n",
    "        \n",
    "    # Only judge the sampled texts\n",
    "    sampled_scores = await asyncio.gather(*[_judge(generation=t) for t in sampled_texts])\n",
    "    \n",
    "    # Convert scores to float, handling None values\n",
    "    sampled_scores = np.array([0.0 if s is None else float(s) for s in sampled_scores])\n",
    "    \n",
    "    # Analyze the sampled scores\n",
    "    bad = sampled_scores < score_thresh\n",
    "    frac_bad = 100.0 * bad.mean()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Judge mean={sampled_scores.mean():.1f} | \"\n",
    "              f\"{bad.sum()}/{len(sampled_texts)} below {score_thresh} \"\n",
    "              f\"({frac_bad:.1f}% → {'OOD' if frac_bad > frac else 'OK'})\")\n",
    "    \n",
    "    return frac_bad > frac\n",
    "'''\n",
    "from openai import AsyncOpenAI\n",
    "import asyncio\n",
    "from typing import List\n",
    "\n",
    "#Let's make a tight judge that does not use any threshold\n",
    "\n",
    "class CoherenceChecker:\n",
    "    def __init__(self, model: str = \"gpt-4o-mini\"):\n",
    "        \"\"\"Initialize with model name, using the global AsyncOpenAI client\"\"\"\n",
    "        self.model = model\n",
    "        \n",
    "    async def check_coherence(self, text: str) -> bool:\n",
    "        \"\"\"Check if a text completion is coherent (returns True) or not coherent (returns False)\"\"\"\n",
    "        prompt = \"\"\"\n",
    "        Evaluate whether the following text is coherent. Coherent means the text makes logical sense, \n",
    "        is grammatically correct, and flows naturally.\n",
    "        \n",
    "        [TEXT START]\n",
    "        {text}\n",
    "        [TEXT END]\n",
    "        \n",
    "        Is the text coherent? Answer with only a single digit: 1 for coherent, 0 for not coherent.\n",
    "        \"\"\"\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": prompt.format(text=text)}]\n",
    "        \n",
    "        # Use the global AsyncOpenAI client\n",
    "        completion = await openai.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=messages,\n",
    "            max_tokens=1,\n",
    "            temperature=0,\n",
    "            seed=0\n",
    "        )\n",
    "        \n",
    "        result = completion.choices[0].message.content.strip()\n",
    "        # Return True if coherent (1), False if not coherent (0)\n",
    "        return result == \"1\"\n",
    "        \n",
    "async def is_ood(texts: List[str], model: str = \"gpt-4o-mini\") -> bool:\n",
    "    \"\"\"\n",
    "    Check if any text in the list is incoherent (out of distribution).\n",
    "    Returns True if ANY sample is incoherent, otherwise False.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text completions to check\n",
    "        model: Model to use for coherence checking\n",
    "        \n",
    "    Returns:\n",
    "        True if any text is incoherent (OOD), False if all are coherent\n",
    "    \"\"\"\n",
    "    checker = CoherenceChecker(model=model)\n",
    "    \n",
    "    # Create tasks to check coherence for each text\n",
    "    tasks = [checker.check_coherence(text) for text in texts]\n",
    "    \n",
    "    # Run all tasks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    # If any result is False (not coherent), return True for OOD\n",
    "    return not all(results)\n",
    "'''\n",
    "async def calibrate_alpha_ood_only(\n",
    "    ood_check_async,\n",
    "    *,\n",
    "    min_alpha : float = 1.0,\n",
    "    max_alpha : float = 32.0,\n",
    "    tol       : float = 0.05,\n",
    "    max_iters : int   = 15,\n",
    ") -> float:\n",
    "    lo, hi = min_alpha, max_alpha\n",
    "    last_good = min_alpha\n",
    "    for _ in range(max_iters):     \n",
    "        if hi / lo <= tol:\n",
    "            break\n",
    "        mid = (lo + hi) / 2\n",
    "        print(\"here is middle alpha value\", mid)\n",
    "        if await ood_check_async(mid):\n",
    "            hi = mid\n",
    "        else:\n",
    "            last_good = mid\n",
    "            lo   = mid\n",
    "    return float(last_good)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a16b5cc0-a293-4e03-a092-6a1a235e0307",
   "metadata": {
    "id": "a16b5cc0-a293-4e03-a092-6a1a235e0307"
   },
   "outputs": [],
   "source": [
    "async def _run_eval_on_layer(\n",
    "    layer_idx: int,\n",
    "    prompts  : list[str],\n",
    "    *,\n",
    "    dataset,\n",
    "    unique_labels,\n",
    "    act_clf,\n",
    "    eval_layer        : int | None = None,\n",
    "    only_calibrate    : bool  = False,\n",
    "    alpha_grad_fixed  : float | None = None,\n",
    "    alpha_caa_fixed   : float | None = None,\n",
    "    alpha_table       : dict | None  = None,\n",
    "    alpha_grad_guess  : tuple[float, float] = (1, 32.0),\n",
    "    alpha_caa_guess   : tuple[float, float] = (1, 32.0),\n",
    "    caa_vectors       : np.ndarray | None = None,\n",
    "    num_pairs_caa     : int = 100,\n",
    "    max_samples       : int = 100,\n",
    "    demo_label_idx    = [0, 1],\n",
    "    num_target_labels = 2,\n",
    "    opposing          = False,\n",
    "    steps = 1,\n",
    "):\n",
    "    if eval_layer is None:\n",
    "        eval_layer = globals().get(\"EVAL_LAYER\", None)\n",
    "    if eval_layer is None:\n",
    "        raise ValueError(\"`eval_layer` not set and global EVAL_LAYER missing\")\n",
    "\n",
    "    all_prompts = [row[\"text\"] for row in dataset]\n",
    "    Y_all       = np.asarray([row[\"label\"] for row in dataset])\n",
    "\n",
    "    X_steer = get_hidden_cached(all_prompts, layer_idx)\n",
    "    steer_model, _ = get_or_train_layer_clf(layer_idx, X_steer, Y_all)\n",
    "\n",
    "    base_module  = getattr(act_clf, \"classifier\", act_clf)\n",
    "    base_module.eval()\n",
    "    device = next(base_module.parameters()).device\n",
    "\n",
    "    def prob_fn(texts: list[str]) -> np.ndarray:\n",
    "        acts = get_hidden_cached(texts, eval_layer)\n",
    "        with torch.no_grad():\n",
    "            logits = base_module(\n",
    "                torch.as_tensor(acts, dtype=torch.float32, device=device)\n",
    "            )\n",
    "        return torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    act_clf_eval = base_module\n",
    "\n",
    "    if caa_vectors is None:\n",
    "        caa_vectors = compute_caa_vectors(\n",
    "            dataset, unique_labels,\n",
    "            steer_layer = layer_idx,\n",
    "            max_pairs   = num_pairs_caa,\n",
    "        )\n",
    "\n",
    "    tgt_demo = demo_label_idx if isinstance(demo_label_idx, list) else [demo_label_idx]\n",
    "    sample_prompts = prompts[:100]\n",
    "\n",
    "    if num_target_labels == 2 and opposing:\n",
    "        def _gen_grad(a: float) -> list[str]:\n",
    "            hook = get_gradient_hook(steer_model=steer_model, target_labels=[tgt_demo[0]], avoid_labels=[tgt_demo[1]], alpha=a, steps=steps)\n",
    "            print(\"generating the correct hook for the batch generate\")\n",
    "            return batch_generate(model, tokenizer, sample_prompts,\n",
    "                                  layer_idx=layer_idx, hook_fn=hook,\n",
    "                                  max_new_tokens=24)\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        def _gen_grad(a: float) -> list[str]:\n",
    "            hook = get_gradient_hook(steer_model=steer_model, target_labels=tgt_demo, avoid_labels=[], alpha=a, steps=steps)\n",
    "            return batch_generate(model, tokenizer, sample_prompts,\n",
    "                                  layer_idx=layer_idx, hook_fn=hook,\n",
    "                                  max_new_tokens=24)\n",
    "            \n",
    "    if opposing and num_target_labels == 2:\n",
    "        print(f\"tgt_demo: {tgt_demo}, length: {len(tgt_demo)}\")\n",
    "        print(f\"caa_vectors shape: {caa_vectors.shape}\")\n",
    "        caa_vec_demo  = torch.tensor((caa_vectors[[tgt_demo[0]]] - caa_vectors[[tgt_demo[1]]]).mean(0),dtype=torch.float16, device=DEVICE)\n",
    "    else:  \n",
    "        caa_vec_demo = torch.tensor(\n",
    "            caa_vectors[tgt_demo].mean(0), dtype=torch.float16, device=DEVICE\n",
    "        )\n",
    "    def _gen_caa(a: float) -> list[str]:\n",
    "        hook = get_caa_hook(caa_vec_demo, alpha=a)\n",
    "        return batch_generate(model, tokenizer, sample_prompts,\n",
    "                              layer_idx=layer_idx, hook_fn=hook,\n",
    "                              max_new_tokens=24)\n",
    "\n",
    "    _grad_ood = lambda a: is_ood(_gen_grad(a))\n",
    "    _caa_ood  = lambda a: is_ood(_gen_caa(a))\n",
    "\n",
    "    if alpha_table is not None:\n",
    "        alpha_grad = alpha_table\n",
    "        alpha_caa  = alpha_table\n",
    "    elif alpha_grad_fixed is not None and alpha_caa_fixed is not None:\n",
    "        alpha_grad, alpha_caa = alpha_grad_fixed, alpha_caa_fixed\n",
    "    else:\n",
    "      alpha_grad = await calibrate_alpha_ood_only(\n",
    "          _grad_ood, min_alpha=alpha_grad_guess[0], max_alpha=alpha_grad_guess[1])\n",
    "      alpha_caa  = await calibrate_alpha_ood_only(\n",
    "          _caa_ood, min_alpha=alpha_caa_guess[0], max_alpha=alpha_caa_guess[1])\n",
    "\n",
    "    if only_calibrate:\n",
    "        return None, None, None, None, None, alpha_grad, alpha_caa\n",
    "\n",
    "    df = await eval_steering_combinations(\n",
    "        prompts            = prompts[:max_samples],\n",
    "        unique_labels      = unique_labels,\n",
    "        steer_model        = steer_model,\n",
    "        caa_vectors        = caa_vectors,\n",
    "        layer_idx          = layer_idx,\n",
    "        act_clf            = act_clf_eval,\n",
    "        alpha_grad         = alpha_grad,\n",
    "        alpha_caa          = alpha_caa,\n",
    "        num_target_labels  = num_target_labels,\n",
    "        steps              = steps,\n",
    "        opposing           = opposing,\n",
    "    )\n",
    "\n",
    "    k_score   = df[\"K-Steering\"].mean()\n",
    "    caa_score = df[\"CAA\"].mean()\n",
    "\n",
    "    return (\n",
    "        df, k_score, caa_score,\n",
    "        steer_model, caa_vectors,\n",
    "        alpha_grad, alpha_caa,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f715961-26b7-485c-8862-5020ec2da19c",
   "metadata": {
    "id": "2f715961-26b7-485c-8862-5020ec2da19c"
   },
   "outputs": [],
   "source": [
    "async def sweep_alphas_for_layers(\n",
    "    layers_to_sweep : list[int],\n",
    "    *,\n",
    "    prompts         : list[str],\n",
    "    dataset,\n",
    "    unique_labels,\n",
    "    num_target_labels: int = 2,\n",
    "    act_clf         = None,\n",
    "    eval_layer      : int | None = None,\n",
    "    alpha_grad_guess: tuple[float, float] = (0.1, 30000.0),\n",
    "    alpha_caa_guess : tuple[float, float] = (0.1, 32.0),\n",
    "    num_pairs_caa   : int = 100,\n",
    "    steps = 1,\n",
    "    opposing = False,\n",
    "    **other_kwargs,\n",
    "):\n",
    "        \n",
    "    if opposing and num_target_labels == 2:\n",
    "        # For pairs, include both ('A', 'B') and ('B', 'A')\n",
    "        combos = [tuple(p) for p in permutations(unique_labels, num_target_labels)]\n",
    "    else:\n",
    "        # For other sizes, just use combinations\n",
    "        combos = [tuple(c) for c in combinations(unique_labels, num_target_labels)]\n",
    "    \n",
    "    layer2alpha = {}\n",
    "\n",
    "    for l in tqdm(layers_to_sweep, desc=\"Layers\"):\n",
    "        caa_vecs_layer = compute_caa_vectors(\n",
    "            dataset, unique_labels,\n",
    "            steer_layer = l, max_pairs = num_pairs_caa,\n",
    "        )\n",
    "\n",
    "        combo2α = {}\n",
    "        for combo in tqdm(combos, desc=f\"Layer {l} combos\", leave=False):\n",
    "            tgt_idx = [unique_labels.index(t) for t in combo]\n",
    "            \n",
    "\n",
    "            (_, _, _, _, _, αg, αc) = await _run_eval_on_layer(\n",
    "                l, prompts,\n",
    "                dataset        = dataset,\n",
    "                unique_labels  = unique_labels,\n",
    "                demo_label_idx = tgt_idx,\n",
    "                only_calibrate = True,\n",
    "                alpha_grad_guess = alpha_grad_guess,\n",
    "                alpha_caa_guess  = alpha_caa_guess,\n",
    "                act_clf         = act_clf,\n",
    "                eval_layer      = eval_layer,\n",
    "                caa_vectors     = caa_vecs_layer,\n",
    "                num_pairs_caa   = num_pairs_caa,\n",
    "                steps = steps,\n",
    "                opposing = opposing,\n",
    "                **other_kwargs,\n",
    "            )\n",
    "            combo2α[combo] = (αg, αc)\n",
    "\n",
    "        layer2alpha[l] = combo2α\n",
    "        tqdm.write(\n",
    "            f\"[layer {l}] calibrated {len(combo2α)} combos \"\n",
    "            f\"(max α_grad={max(a[0] for a in combo2α.values()):.2g}, \"\n",
    "            f\"max α_caa={max(a[1] for a in combo2α.values()):.2g})\"\n",
    "        )\n",
    "\n",
    "    return layer2alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd1fcf81-053f-45b5-9d00-93218f8ef195",
   "metadata": {
    "id": "bd1fcf81-053f-45b5-9d00-93218f8ef195"
   },
   "outputs": [],
   "source": [
    "async def evaluate_layers(\n",
    "    layer2alpha   : Dict[int, Dict[tuple, Tuple[float, float]]],\n",
    "    *,\n",
    "    prompts       : List[str],\n",
    "    dataset,\n",
    "    unique_labels,\n",
    "    num_target_labels= 2,\n",
    "    opposing = False,\n",
    "    steps = 1,\n",
    "    **run_kwargs,\n",
    "):\n",
    "    frames = {}\n",
    "    for l in sorted(layer2alpha.keys()):\n",
    "        df, k_score, c_score, steer_model, caa_vecs, _, _ = await _run_eval_on_layer(\n",
    "            l, prompts,\n",
    "            dataset        = dataset,\n",
    "            unique_labels  = unique_labels,\n",
    "            alpha_table    = layer2alpha[l],\n",
    "            steps          = steps,\n",
    "            **run_kwargs,\n",
    "            num_target_labels= num_target_labels,\n",
    "            opposing= opposing\n",
    "        )\n",
    "        frames[l] = (df, k_score, c_score, steer_model, caa_vecs)\n",
    "\n",
    "    best_k_layer   = max(frames, key=lambda x: frames[x][1])\n",
    "    best_caa_layer = max(frames, key=lambda x: frames[x][2])\n",
    "\n",
    "    df_best = frames[best_k_layer][0][[\"Targets\", \"K-Steering\"]].copy()\n",
    "    df_best[\"CAA\"] = frames[best_caa_layer][0][\"CAA\"].values\n",
    "\n",
    "    evaluate_layers._cache = {\n",
    "        \"best_k_layer\"   : best_k_layer,\n",
    "        \"best_caa_layer\" : best_caa_layer,\n",
    "        \"layer2alpha\"    : layer2alpha,\n",
    "        \"steer_models\"   : {l: frames[l][3] for l in frames},\n",
    "        \"caa_vecs\"       : {l: frames[l][4] for l in frames},\n",
    "    }\n",
    "\n",
    "    return df_best, best_caa_layer, best_k_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de717451-f372-4bd4-bc6e-5cb25f74003c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '57_Reductio ad Absurdum',\n",
       " 'original_question': 'Are political ideologies evolving in response to global challenges?',\n",
       " 'text': \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\n\\nAre political ideologies evolving in response to global challenges?\",\n",
       " 'label': 'Reductio ad Absurdum',\n",
       " 'system_message': \"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\n\",\n",
       " 'category': 'politics'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aaae469-d532-4187-abfd-adb19af1398d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Are political ideologies evolving in response to global challenges?'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ebe2ae0-6bf2-4eea-a78d-ea37e4034f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"RESPOND USING REDUCTIO AD ABSURDUM DEBATE STYLE. Extend the opposing viewpoint to its logical extreme to reveal its flaws. Demonstrate how the opposing position leads to absurd or contradictory conclusions when taken to its logical end. Structure your response to systematically expose the inconsistencies or impracticalities. Use conditional statements like 'If we follow this logic, then...' to illustrate the path to absurdity. Maintain a logical flow that shows how the initial premise inevitably leads to an unacceptable or ridiculous outcome.\\n\\n\\nAre political ideologies evolving in response to global challenges?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_reductio_prompts = get_texts_by_label(dataset, 'Reductio ad Absurdum')\n",
    "eval_reductio_prompts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42b618-a08e-4948-81a4-bd4f56f7289c",
   "metadata": {
    "id": "4f42b618-a08e-4948-81a4-bd4f56f7289c"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b6f59ef-47bd-43e9-9ad4-32dda72d8153",
   "metadata": {
    "id": "4b6f59ef-47bd-43e9-9ad4-32dda72d8153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss=0.2138\n",
      "Epoch 2/5, Loss=0.0559\n",
      "Epoch 3/5, Loss=0.0237\n",
      "Epoch 4/5, Loss=0.0143\n",
      "Epoch 5/5, Loss=0.0071\n"
     ]
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "all_prompts      = [row[\"text\"] for row in dataset]\n",
    "Y_all            = np.array([unique_labels.index(row[\"label\"]) for row in dataset], dtype=np.int64)\n",
    "\n",
    "# Train or load the activations classifier\n",
    "EVAL_LAYER = -1\n",
    "X_eval = get_hidden_cached(all_prompts, layer_idx=EVAL_LAYER)\n",
    "\n",
    "act_clf_eval, eval_acc = get_or_train_eval_clf(\n",
    "    X_eval,\n",
    "    Y_all,\n",
    "    hidden_dim = 128,\n",
    "    epochs      = 5,\n",
    "    batch_size  = 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac148c-d682-491b-bfa3-125f0b7f4d48",
   "metadata": {
    "id": "dfac148c-d682-491b-bfa3-125f0b7f4d48",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9beeeead8f864440aef0496e7f91a410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Layers:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97e162d9a584f99ac355108e7e311db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Layer 14 combos:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss=0.2972\n",
      "Epoch 2/5, Loss=0.0600\n",
      "Epoch 3/5, Loss=0.0154\n",
      "Epoch 4/5, Loss=0.0061\n",
      "Epoch 5/5, Loss=0.0029\n",
      "tgt_demo: [0, 1], length: 2\n",
      "caa_vectors shape: (5, 3072)\n",
      "here is middle alpha value 7.55\n",
      "generating the correct hook for the batch generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge mean=25.1 | 19/20 below 40.0 (95.0% → OOD)\n",
      "here is middle alpha value 3.8249999999999997\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=39.6 | 14/20 below 40.0 (70.0% → OOD)\n",
      "here is middle alpha value 1.9625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=17.1 | 20/20 below 40.0 (100.0% → OOD)\n",
      "here is middle alpha value 1.03125\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=33.5 | 13/20 below 40.0 (65.0% → OOD)\n",
      "here is middle alpha value 0.565625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=63.6 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.3328125\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=67.1 | 0/20 below 40.0 (0.0% → OK)\n",
      "here is middle alpha value 0.44921875\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=66.8 | 0/20 below 40.0 (0.0% → OK)\n",
      "here is middle alpha value 0.507421875\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=62.5 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 0.4783203125\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=63.2 | 0/20 below 40.0 (0.0% → OK)\n",
      "here is middle alpha value 0.49287109375000004\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=64.8 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.485595703125\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=67.7 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.4819580078125\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=61.4 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 0.48013916015625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=63.3 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.47922973632812504\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=65.0 | 0/20 below 40.0 (0.0% → OK)\n",
      "here is middle alpha value 0.47968444824218753\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=60.9 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 7.55\n",
      "Judge mean=28.5 | 19/20 below 40.0 (95.0% → OOD)\n",
      "here is middle alpha value 3.8249999999999997\n",
      "Judge mean=50.4 | 5/20 below 40.0 (25.0% → OOD)\n",
      "here is middle alpha value 1.9625\n",
      "Judge mean=58.9 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 1.03125\n",
      "Judge mean=60.3 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.565625\n",
      "Judge mean=61.2 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.3328125\n",
      "Judge mean=61.7 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.21640625000000002\n",
      "Judge mean=66.9 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.158203125\n",
      "Judge mean=69.7 | 0/20 below 40.0 (0.0% → OK)\n",
      "here is middle alpha value 0.1873046875\n",
      "Judge mean=63.2 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 0.17275390625\n",
      "Judge mean=64.1 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 0.165478515625\n",
      "Judge mean=69.0 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.1618408203125\n",
      "Judge mean=61.6 | 0/20 below 40.0 (0.0% → OK)\n",
      "here is middle alpha value 0.16365966796874998\n",
      "Judge mean=64.6 | 3/20 below 40.0 (15.0% → OOD)\n",
      "here is middle alpha value 0.162750244140625\n",
      "Judge mean=70.6 | 0/20 below 40.0 (0.0% → OK)\n",
      "here is middle alpha value 0.1632049560546875\n",
      "Judge mean=65.1 | 1/20 below 40.0 (5.0% → OOD)\n",
      "tgt_demo: [0, 2], length: 2\n",
      "caa_vectors shape: (5, 3072)\n",
      "here is middle alpha value 7.55\n",
      "generating the correct hook for the batch generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Judge mean=35.9 | 15/20 below 40.0 (75.0% → OOD)\n",
      "here is middle alpha value 3.8249999999999997\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=27.7 | 19/20 below 40.0 (95.0% → OOD)\n",
      "here is middle alpha value 1.9625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=50.8 | 6/20 below 40.0 (30.0% → OOD)\n",
      "here is middle alpha value 1.03125\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=28.6 | 18/20 below 40.0 (90.0% → OOD)\n",
      "here is middle alpha value 0.565625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=63.2 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.3328125\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=65.8 | 0/20 below 40.0 (0.0% → OK)\n",
      "here is middle alpha value 0.44921875\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=66.3 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.391015625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=65.0 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.3619140625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=61.9 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 0.34736328125000004\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=62.0 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 0.340087890625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=59.5 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.3364501953125\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=65.1 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 0.33463134765625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=66.3 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.33372192382812504\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=70.3 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 0.3332672119140625\n",
      "generating the correct hook for the batch generate\n",
      "Judge mean=66.2 | 1/20 below 40.0 (5.0% → OOD)\n",
      "here is middle alpha value 7.55\n",
      "Judge mean=40.6 | 10/20 below 40.0 (50.0% → OOD)\n",
      "here is middle alpha value 3.8249999999999997\n",
      "Judge mean=50.6 | 4/20 below 40.0 (20.0% → OOD)\n",
      "here is middle alpha value 1.9625\n",
      "Judge mean=60.7 | 2/20 below 40.0 (10.0% → OOD)\n",
      "here is middle alpha value 1.03125\n"
     ]
    }
   ],
   "source": [
    "# Sweep alpha for each layer and combination\n",
    "layer2alpha = await sweep_alphas_for_layers(\n",
    "    layers_to_sweep = list(range(14,25)),\n",
    "    prompts         = eval_reductio_prompts,\n",
    "    dataset         = dataset,\n",
    "    unique_labels   = unique_labels,\n",
    "    num_target_labels= 2,\n",
    "    act_clf = act_clf_eval,\n",
    "    steps           = 1,\n",
    "    alpha_grad_guess = (0.1, 15.0),\n",
    "    alpha_caa_guess  = (0.1, 15.0),\n",
    "    opposing         = True,\n",
    ")\n",
    "layer2alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077c2f6-f1be-44ce-bd61-20a066550593",
   "metadata": {
    "id": "9077c2f6-f1be-44ce-bd61-20a066550593"
   },
   "outputs": [],
   "source": [
    "# Layer sweep with good alphas\n",
    "df_best, best_caa_layer, best_k_layer = await evaluate_layers(\n",
    "    layer2alpha   = layer2alpha,\n",
    "    prompts       = eval_reductio_prompts,\n",
    "    dataset       = dataset,\n",
    "    unique_labels = unique_labels,\n",
    "    act_clf       = act_clf_eval,\n",
    "    eval_layer    = EVAL_LAYER,\n",
    "    num_target_labels= 2,\n",
    "    steps=1,\n",
    "    opposing = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25cfb61-46ea-444e-84de-d1155cee135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def sample_steered_responses(\n",
    "    prompts: list[str],\n",
    "    target_tones: list[str] | str,\n",
    "    *,\n",
    "    steer_model_k = None,\n",
    "    layer_k       = None,\n",
    "    alpha_grad    = None,\n",
    "\n",
    "    caa_vectors   = None,\n",
    "    layer_caa     = None,\n",
    "    alpha_caa     = None,\n",
    "\n",
    "    max_new_tokens: int = 200,\n",
    "    batch_size    : int = 500,\n",
    "    opposing      : bool = False,\n",
    "    save_as: str | None = None,\n",
    "):\n",
    "    cache = getattr(evaluate_layers, \"_cache\", None)\n",
    "    if cache is None:\n",
    "        raise ValueError(\n",
    "            \"evaluate_layers._cache not found. Run evaluate_layers() first \"\n",
    "            \"or pass steer_model_k / caa_vectors / alphas explicitly.\"\n",
    "        )\n",
    "\n",
    "    if layer_k   is None: layer_k   = cache[\"best_k_layer\"]\n",
    "    if layer_caa is None: layer_caa = cache[\"best_caa_layer\"]\n",
    "\n",
    "    def _lookup_alpha(layer: int, is_caa=False):\n",
    "        tbl = cache[\"layer2alpha\"][layer]\n",
    "        if opposing and len(target_tones) == 2:\n",
    "            # for opposing the order of the keys matter\n",
    "            combo_key = tuple(\n",
    "            target_tones if isinstance(target_tones, list)\n",
    "            else [target_tones]\n",
    "            )\n",
    "           \n",
    "        else:\n",
    "             combo_key = tuple(sorted(\n",
    "                target_tones if isinstance(target_tones, list)\n",
    "                else [target_tones]\n",
    "            ))\n",
    "        if combo_key not in tbl:\n",
    "            raise KeyError(f\"Combo {combo_key} not found in layer2alpha[{layer}].\")\n",
    "        αg, αc = tbl[combo_key]\n",
    "        return αc if is_caa else αg\n",
    "\n",
    "    if alpha_grad is None:\n",
    "        alpha_grad = _lookup_alpha(layer_k, is_caa=False)\n",
    "    if alpha_caa is None:\n",
    "        alpha_caa  = _lookup_alpha(layer_caa, is_caa=True)\n",
    "\n",
    "    if steer_model_k is None:\n",
    "        steer_model_k = cache[\"steer_models\"][layer_k]\n",
    "    if caa_vectors is None:\n",
    "        caa_vectors   = cache[\"caa_vecs\"][layer_caa]\n",
    "\n",
    "    tone2idx = {t: i for i, t in enumerate(unique_labels)}\n",
    "    tgt_list = (target_tones\n",
    "                if isinstance(target_tones, list)\n",
    "                else [target_tones])\n",
    "    tgt_idx  = [tone2idx[t] for t in tgt_list]\n",
    "  \n",
    "    if len(tgt_idx) == 2 and opposing:\n",
    "        print(\"We are in the opposing case\")\n",
    "        grad_hook = get_gradient_hook(\n",
    "            steer_model_k, target_labels=[tgt_idx[0]],\n",
    "            avoid_labels=[tgt_idx[1]], alpha=alpha_grad\n",
    "        )\n",
    "    else:\n",
    "        grad_hook = get_gradient_hook(\n",
    "        steer_model_k, target_labels=tgt_idx,\n",
    "        avoid_labels=[], alpha=alpha_grad\n",
    "    )\n",
    "\n",
    "    if len(tgt_idx) == 2 and opposing:\n",
    "        caa_vec  = (caa_vectors[[tgt_idx[0]]] - caa_vectors[[tgt_idx[1]]]).mean(axis=0)\n",
    "    else:  \n",
    "        caa_vec  = caa_vectors[tgt_idx].mean(axis=0)\n",
    "    caa_hook = get_caa_hook(caa_vec, alpha=alpha_caa)\n",
    "\n",
    "    unsteered = batch_generate(\n",
    "        model, tokenizer, prompts,\n",
    "        layer_idx=layer_caa, hook_fn=None,\n",
    "        max_new_tokens=max_new_tokens, batch_size=batch_size,\n",
    "    )\n",
    "    ksteer    = batch_generate(\n",
    "        model, tokenizer, prompts,\n",
    "        layer_idx=layer_k, hook_fn=grad_hook,\n",
    "        max_new_tokens=max_new_tokens, batch_size=batch_size,\n",
    "    )\n",
    "    caa_out   = batch_generate(\n",
    "        model, tokenizer, prompts,\n",
    "        layer_idx=layer_caa, hook_fn=caa_hook,\n",
    "        max_new_tokens=max_new_tokens, batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    def _strip(gen: str, prompt: str) -> str:\n",
    "        return gen[len(prompt):].lstrip() if gen.startswith(prompt) else gen\n",
    "\n",
    "    rows = []\n",
    "    for p, base, ktxt, ctxt in zip(prompts, unsteered, ksteer, caa_out):\n",
    "        if opposing and len(tgt_list) == 2:\n",
    "            # In opposing case with 2 targets, explicitly show target vs avoid\n",
    "            targets_repr = f\"target: {tgt_list[0]}, avoid: {tgt_list[1]}\"\n",
    "        else:\n",
    "            # Normal case, just join the targets with commas\n",
    "            targets_repr = \", \".join(tgt_list)\n",
    "        \n",
    "        rows.append({\n",
    "            \"prompt\"      : p,\n",
    "            \"unsteered\"   : _strip(base, p),\n",
    "            \"k_steering\"  : _strip(ktxt, p),\n",
    "            \"caa\"         : _strip(ctxt, p),\n",
    "            \"layer_k\"     : layer_k,\n",
    "            \"layer_caa\"   : layer_caa,\n",
    "            \"alpha_grad\"  : alpha_grad,\n",
    "            \"alpha_caa\"   : alpha_caa,\n",
    "            \"targets\"     : targets_repr,\n",
    "            \"frac\"        : 3.0,          # allowed % of low‑coherence answers\n",
    "            \"score_thresh\": 40.0,  # answers < this score are \"bad\"\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    for r in rows:\n",
    "        print(\"\\n\" + \"=\"*90)\n",
    "        print(f\"PROMPT:\\n{r['prompt']}\\n\")\n",
    "        print(\"- Unsteered -------------------------------------------------\\n\"\n",
    "              + r[\"unsteered\"] + \"\\n\")\n",
    "        print(f\"- K‑steering (layer {layer_k}, α_grad = {alpha_grad:.3g})\\n\"\n",
    "              + r[\"k_steering\"] + \"\\n\")\n",
    "        print(f\"- CAA        (layer {layer_caa}, α_caa  = {alpha_caa:.3g})\\n\"\n",
    "              + r[\"caa\"] + \"\\n\")\n",
    "\n",
    "    if save_as is None:\n",
    "        combo_str = \"_\".join(tgt_list)\n",
    "        if opposing and len(tgt_list) == 2:\n",
    "            save_as = f\"steered_generations_{combo_str}_opposing.csv\"\n",
    "        else:  \n",
    "            save_as = f\"steered_generations_{combo_str}.csv\"\n",
    "\n",
    "    save_path = Path(save_as)\n",
    "    if save_path.suffix.lower() == \".json\":\n",
    "        df.to_json(save_path, orient=\"records\", indent=2)\n",
    "    else:\n",
    "        df.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"\\nSaved generations to {save_path.absolute()}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7c768-d5c0-4bf2-9647-28a7494827f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_steered_responses(\n",
    "    prompts      = eval_reductio_prompts[:100],\n",
    "    target_tones = [\"Analogy Construction\", \"Reductio ad Absurdum\"],\n",
    "    opposing = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8a04d-41fb-4469-959e-611664c58a79",
   "metadata": {},
   "source": [
    "Manual Inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c331135f-6392-411d-900f-b0e8c0eea67a",
   "metadata": {
    "id": "c331135f-6392-411d-900f-b0e8c0eea67a"
   },
   "outputs": [],
   "source": [
    "def add_dct_column(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    unique_labels   : Sequence[str],\n",
    "    prompts         : list[str],\n",
    "    dct_vectors     : np.ndarray,\n",
    "    tone2dct        : Mapping[str, list],\n",
    "    alpha_lookup    : Callable[[tuple], float],\n",
    "    layer_idx       : int,\n",
    "    combo_col       : str | None = None,\n",
    "    act_clf         = None,\n",
    "    base_model      = None,\n",
    "    tokenizer       = None,\n",
    "    opposing        = False,\n",
    ") -> pd.DataFrame:\n",
    "    if combo_col is None:\n",
    "        combo_col = df.select_dtypes(include=[\"object\", \"category\"]).columns[0]\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    def to_tuple(cell) -> tuple[str, ...]:\n",
    "        if isinstance(cell, str):\n",
    "            try:\n",
    "                val = ast.literal_eval(cell)\n",
    "            except Exception:\n",
    "                val = [s.strip(\" '\\\"\") for s in cell.split(\",\")] if \",\" in cell else cell\n",
    "        else:\n",
    "            val = cell\n",
    "        if isinstance(val, (list, tuple)):\n",
    "            return tuple(val)\n",
    "        return (str(val),)\n",
    "    \n",
    "    if act_clf is None:\n",
    "        raise ValueError(\"Need act_clf for activation path\")\n",
    "    \n",
    "    base_module = getattr(act_clf, \"classifier\", act_clf)\n",
    "    device      = next(base_module.parameters()).device\n",
    "    acts_np = get_hidden_cached(prompts, layer_idx)           # (B, D)\n",
    "    acts_t  = torch.tensor(acts_np, dtype=torch.float32, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        base_logits = base_module(acts_t).sigmoid().cpu().numpy()\n",
    "    \n",
    "    deltas = []\n",
    "    for raw in df_out[combo_col]:\n",
    "        combo   = to_tuple(raw)\n",
    "        tgt_idx = [unique_labels.index(t) for t in combo]\n",
    "        \n",
    "        # Handle opposing case with exactly 2 targets differently\n",
    "        if opposing and len(tgt_idx) == 2:\n",
    "            # For opposing case: target[0] - target[1]\n",
    "            base_score = base_logits[:, tgt_idx[0]].mean() - base_logits[:, tgt_idx[1]].mean()\n",
    "            \n",
    "            # Get vectors for target and avoid labels separately\n",
    "            vec_ids_target = tone2dct.get(combo[0], [])\n",
    "            vec_ids_avoid = tone2dct.get(combo[1], [])\n",
    "            \n",
    "            if not vec_ids_target and not vec_ids_avoid:\n",
    "                deltas.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            # Calculate vector based on available data\n",
    "            if vec_ids_target and vec_ids_avoid:\n",
    "                # If we have vectors for both, use target - avoid\n",
    "                vecs_target = dct_vectors[vec_ids_target]\n",
    "                vecs_avoid = dct_vectors[vec_ids_avoid]\n",
    "                vec = vecs_target.mean(0) - vecs_avoid.mean(0)\n",
    "            else:\n",
    "                # If we only have target vectors\n",
    "                vecs_target = dct_vectors[vec_ids_target]\n",
    "                vec = vecs_target.mean(0)\n",
    "\n",
    "        else:\n",
    "            # Original non-opposing calculation\n",
    "            base_score = base_logits[:, tgt_idx].mean()\n",
    "            vec_ids = [vid for lbl in combo for vid in tone2dct.get(lbl, [])]\n",
    "            if not vec_ids:\n",
    "                deltas.append(0.0)\n",
    "                continue\n",
    "            vecs = dct_vectors[vec_ids]\n",
    "            vec = vecs.mean(0)\n",
    "        \n",
    "        # Apply steering vector\n",
    "        alpha = alpha_lookup(combo)\n",
    "        ste_np = acts_np + alpha * vec\n",
    "        ste_t = torch.tensor(ste_np, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ste_logits = base_module(ste_t).sigmoid().cpu().numpy()\n",
    "        \n",
    "        # Calculate final score based on case\n",
    "        if opposing and len(tgt_idx) == 2:\n",
    "            ste_score = ste_logits[:, tgt_idx[0]].mean() - ste_logits[:, tgt_idx[1]].mean()\n",
    "        else:\n",
    "            ste_score = ste_logits[:, tgt_idx].mean()\n",
    "            \n",
    "        deltas.append(ste_score - base_score)\n",
    "    \n",
    "    df_out[\"DCT\"] = deltas\n",
    "    return df_out\n",
    "    \n",
    "async def sweep_alphas_for_dct(\n",
    "    *,\n",
    "    prompts            : List[str],\n",
    "    unique_labels      : List[str],\n",
    "    tone2dct           : Mapping[str, List[int]],\n",
    "    dct_vectors        : np.ndarray,\n",
    "    layer_idx          : int,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    alpha_dct_guess    : Tuple[float, float] = (1.0, 32.0),\n",
    "    tol                : float               = 0.05,\n",
    "    max_iters          : int                 = 10,\n",
    "    opposing           : bool                = False\n",
    ") -> dict[Tuple[str, ...], float]:\n",
    "    combo2alpha = {}\n",
    "    \n",
    "    # Determine which combinations to process\n",
    "    if opposing:\n",
    "        # For opposing case, we need to consider the order\n",
    "        # So we use permutations instead of combinations to keep order\n",
    "        combos_to_try = [(a, b) for a in unique_labels for b in unique_labels if a != b]\n",
    "    else:\n",
    "        # Normal case, use combinations (order doesn't matter)\n",
    "        combos_to_try = list(combinations(unique_labels, 2))\n",
    "    \n",
    "    for combo in combos_to_try:\n",
    "        if opposing and len(combo) == 2:\n",
    "            # For opposing case with 2 labels, handle them separately\n",
    "            target_label, avoid_label = combo\n",
    "            \n",
    "            # Get vector IDs for target and avoid labels\n",
    "            vec_ids_target = tone2dct.get(target_label, [])\n",
    "            vec_ids_avoid = tone2dct.get(avoid_label, [])\n",
    "            \n",
    "            if not vec_ids_target and not vec_ids_avoid:\n",
    "                combo2alpha[combo] = 0.0\n",
    "                continue\n",
    "                \n",
    "            # Calculate vector based on available data\n",
    "            if vec_ids_target and vec_ids_avoid:\n",
    "                # If we have vectors for both, use target - avoid\n",
    "                vecs_target = dct_vectors[vec_ids_target]\n",
    "                vecs_avoid = dct_vectors[vec_ids_avoid]\n",
    "                vec = vecs_target.mean(0) - vecs_avoid.mean(0)\n",
    "            else:\n",
    "                # If we only have target vectors\n",
    "                vecs_target = dct_vectors[vec_ids_target]\n",
    "                vec = vecs_target.mean(0)\n",
    "\n",
    "        else:\n",
    "            # Original non-opposing calculation\n",
    "            vec_ids = [vid for lbl in combo for vid in tone2dct.get(lbl, [])]\n",
    "            if not vec_ids:\n",
    "                combo2alpha[combo] = 0.0\n",
    "                continue\n",
    "            vec = dct_vectors[vec_ids].mean(axis=0)\n",
    "        \n",
    "        def ood_check(alpha: float) -> bool:\n",
    "            hook = get_dct_hook(vec, alpha=alpha)\n",
    "            gens = batch_generate(\n",
    "                model, tokenizer, prompts,\n",
    "                layer_idx      = layer_idx,\n",
    "                hook_fn        = hook,\n",
    "                max_new_tokens = 24,\n",
    "            )\n",
    "            return is_ood(gens)\n",
    "        \n",
    "        best_alpha = await calibrate_alpha_ood_only(\n",
    "            ood_check,\n",
    "            min_alpha = alpha_dct_guess[0],\n",
    "            max_alpha = alpha_dct_guess[1],\n",
    "            tol       = tol,\n",
    "            max_iters = max_iters,\n",
    "        )\n",
    "        \n",
    "        combo2alpha[combo] = best_alpha\n",
    "    \n",
    "    return combo2alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a9ea17-86cc-4565-8505-0a38fca7774f",
   "metadata": {
    "id": "a8a9ea17-86cc-4565-8505-0a38fca7774f"
   },
   "outputs": [],
   "source": [
    "# Compute DCT vectors\n",
    "dct_vectors = compute_dct_vectors_for_layers(source_layer=best_caa_layer, target_layer=best_caa_layer+5)\n",
    "\n",
    "# Map DCT vectors to labels\n",
    "tone2dct = await _map_dct_vectors(\n",
    "    dct_vectors     = dct_vectors,\n",
    "    prompts         = eval_prompts,\n",
    "    act_clf         = act_clf_eval,\n",
    "    layer_idx       = best_caa_layer,\n",
    "    base_act        = None,\n",
    "    unique_labels   = unique_labels,\n",
    "    tone2idx        = {t: i for i, t in enumerate(unique_labels)},\n",
    ")\n",
    "\n",
    "# Sweep alpha\n",
    "alpha2dct = await sweep_alphas_for_dct(\n",
    "    prompts            = eval_prompts,\n",
    "    unique_labels      = unique_labels,\n",
    "    tone2dct           = tone2dct,\n",
    "    dct_vectors        = dct_vectors,\n",
    "    layer_idx          = best_caa_layer,\n",
    "    model              = model,\n",
    "    tokenizer          = tokenizer,\n",
    "    alpha_dct_guess    = (0.1, 1024.0),\n",
    "    max_iters          = 12,\n",
    ")\n",
    "alpha_lookup = lambda combo: alpha2dct.get(combo, 0.0)\n",
    "\n",
    "# Write the DCT results\n",
    "df_with_dct = add_dct_column(\n",
    "    df               = df_best,\n",
    "    unique_labels    = unique_labels,\n",
    "    prompts          = eval_prompts,\n",
    "    dct_vectors      = dct_vectors,\n",
    "    tone2dct         = tone2dct,\n",
    "    alpha_lookup     = alpha_lookup,\n",
    "    layer_idx        = best_caa_layer,\n",
    "    act_clf          = act_clf_eval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8d306-f179-4aed-8bfa-2f0643dee8e9",
   "metadata": {
    "id": "15a8d306-f179-4aed-8bfa-2f0643dee8e9"
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "for cell in df_with_dct['Targets']:\n",
    "    if isinstance(cell, str):\n",
    "        try:\n",
    "            combo = tuple(ast.literal_eval(cell))\n",
    "        except Exception:\n",
    "            combo = tuple(s.strip(\" '\\\"\") for s in cell.split(\",\") if s.strip())\n",
    "    else:\n",
    "        combo = tuple(cell)\n",
    "\n",
    "    α_grad = layer2alpha[best_k_layer].get(combo, (0.0, 0.0))[0]\n",
    "    α_caa  = layer2alpha[best_caa_layer].get(combo, (0.0, 0.0))[1]\n",
    "    α_dct  = alpha2dct.get(combo, 0.0)\n",
    "\n",
    "    records.append({\n",
    "        'Targets': combo,\n",
    "        'alpha_grad': α_grad,\n",
    "        'alpha_caa' : α_caa,\n",
    "        'alpha_dct' : α_dct,\n",
    "    })\n",
    "\n",
    "alphas_df = pd.DataFrame(records)\n",
    "\n",
    "df_with_dct.to_csv('llama3.2-debates-activations-classifier-results.csv', index=False)\n",
    "alphas_df.to_csv('llama3.2-debates-activations-classifier-alphas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MLAwRB-g5xZg",
   "metadata": {
    "id": "MLAwRB-g5xZg"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jiWVuufUdFUv",
   "metadata": {
    "id": "jiWVuufUdFUv"
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_bar(\n",
    "    df: pd.DataFrame,\n",
    "    combo_col: str | None = None,\n",
    "    title: str            = \"Steering Evaluation\",\n",
    "    x_title: str          = \"Label Combination\",\n",
    "    y_title: str          = \"Average Probability\",\n",
    "    output_path: str | Path | None = None,\n",
    "    width: int            = 900,\n",
    "    height: int           = 500,\n",
    "    show: bool            = True,\n",
    "):\n",
    "    if combo_col is None:\n",
    "        combo_col = df.select_dtypes(include=[\"object\", \"category\"]).columns[0]\n",
    "\n",
    "    method_cols = [c for c in df.columns if c != combo_col]\n",
    "\n",
    "    palette = ['#FF563F', '#F5C0B8',  '#55C89F', '#363432', '#F9DA81']\n",
    "    if len(method_cols) > len(palette):\n",
    "        repeats  = -(-len(method_cols) // len(palette))\n",
    "        palette *= repeats\n",
    "    palette = palette[:len(method_cols)]\n",
    "\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        x                = combo_col,\n",
    "        y                = method_cols,\n",
    "        color_discrete_sequence = palette,\n",
    "        template         = \"plotly_white\",\n",
    "        width            = width,\n",
    "        height           = height,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\"  : title,\n",
    "            \"font\"  : {\"size\": 16, \"color\": \"#0c0c0c\", \"family\": \"Space Grotesk\"},\n",
    "            \"x\"     : 0.5, \"y\": 0.96, \"xanchor\": \"center\", \"yanchor\": \"top\",\n",
    "        },\n",
    "        font={\n",
    "            \"family\": \"Space Grotesk, Work Sans, sans-serif\",\n",
    "            \"color\" : \"#0c0c0c\",\n",
    "        },\n",
    "        barmode   = \"group\",\n",
    "        margin    = {\"l\": 40, \"r\": 40, \"t\": 100, \"b\": 80},\n",
    "        legend    = {\n",
    "            \"title\": {\"text\": \"\"},\n",
    "            \"orientation\": \"h\",\n",
    "            \"y\": 1.0, \"x\": 0.5,\n",
    "            \"xanchor\": \"center\", \"yanchor\": \"bottom\",\n",
    "            \"font\": {\"size\": 10, \"color\": \"#928e8b\"},\n",
    "        },\n",
    "        xaxis     = {\n",
    "            \"title\": {\"text\": x_title},\n",
    "            \"gridcolor\": \"#f5f5f5\",\n",
    "            \"linecolor\": \"#e5dfdf\",\n",
    "            \"linewidth\": 1.5,\n",
    "            \"tickfont\": {\"color\": \"#928E8B\"},\n",
    "            \"ticksuffix\": \"   \",\n",
    "        },\n",
    "        yaxis     = {\n",
    "            \"title\": {\"text\": y_title},\n",
    "            \"gridcolor\": \"#f5f5f5\",\n",
    "            \"linecolor\": \"#e5dfdf\",\n",
    "            \"linewidth\": 1.5,\n",
    "            \"tickfont\": {\"color\": \"#928E8B\"},\n",
    "            \"ticksuffix\": \"   \",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        hoverlabel = {\n",
    "            \"bgcolor\": \"#0c0c0c\",\n",
    "            \"font_color\": \"#ffffff\",\n",
    "            \"font_family\": \"Work Sans\",\n",
    "        },\n",
    "        hovertemplate = \"&nbsp;%{x}<br>&nbsp;%{y:.3f}<extra></extra>\",\n",
    "    )\n",
    "\n",
    "    if output_path is not None:\n",
    "        output_path = Path(output_path)\n",
    "        try:\n",
    "            fig.write_image(str(output_path))\n",
    "            print(f\"Figure written to: {output_path.resolve()}\")\n",
    "        except ValueError as e:\n",
    "            if \"kaleido\" in str(e).lower():\n",
    "                raise RuntimeError(\n",
    "                    \"Static image export requires Kaleido. \"\n",
    "                    \"Install it with:\\n    pip install -U kaleido\"\n",
    "                ) from e\n",
    "            raise\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3zOEzlq9fb_A",
   "metadata": {
    "id": "3zOEzlq9fb_A"
   },
   "outputs": [],
   "source": [
    "plot_evaluation_bar(\n",
    "    df_with_dct,\n",
    "    title=\"Steering Performance VS Unsteered Models (Tones, Last Layer Activations Classifier)\",\n",
    "    output_path=\"df_gen.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eLrEY9VP_HsS",
   "metadata": {
    "id": "eLrEY9VP_HsS"
   },
   "outputs": [],
   "source": [
    "df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dd23d-78cc-4af3-8a6e-2926b1d816b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae179ea-76fc-424a-8b92-520dfbd8552c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee50ebb-5941-4d3d-bbf5-84574cbb687c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

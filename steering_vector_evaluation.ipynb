{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FAii7a0jarkj",
   "metadata": {
    "id": "FAii7a0jarkj"
   },
   "outputs": [],
   "source": [
    "# Only run to clear GPU mem\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8",
   "metadata": {
    "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_tKEtbndpuW6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tKEtbndpuW6",
    "outputId": "395e70b0-da79-4305-9f73-4e9a78176ae8"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate transformer_lens openai tiktoken kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "702863ee-3e14-424f-a304-2f8449de9d8e",
   "metadata": {
    "id": "702863ee-3e14-424f-a304-2f8449de9d8e"
   },
   "outputs": [],
   "source": [
    "import asyncio, hashlib, math, os, random, sys, copy, gc\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import List, Dict, Tuple, Optional, DefaultDict, Callable\n",
    "from urllib.request import urlopen\n",
    "import importlib.util, sys, copy, random, torch, itertools\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from openai import AsyncOpenAI\n",
    "import plotly.express as px\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "CL1CyFVJnlw3",
   "metadata": {
    "id": "CL1CyFVJnlw3"
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    # steering layer: int → force layer; \"auto\" → sweep once & cache\n",
    "    \"STEER_LAYER\": 15,\n",
    "    # steering task: \"tones\" | \"obsession\"\n",
    "    \"TASK\": \"obsession\",\n",
    "    # on‑disk caches\n",
    "    \"HIDDEN_CACHE_DIR\": \"hidden_cache\",\n",
    "    \"MODEL_CACHE_DIR\": \"layer_clfs\",\n",
    "}\n",
    "for _d in (CFG[\"HIDDEN_CACHE_DIR\"], CFG[\"MODEL_CACHE_DIR\"]):\n",
    "    Path(_d).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "XCgrRk7Bn7om",
   "metadata": {
    "id": "XCgrRk7Bn7om"
   },
   "outputs": [],
   "source": [
    "def _load_tones():\n",
    "    ds = load_dataset(\"Narmeen07/tone_agnostic_questions\", split=\"train\")\n",
    "    tone_prompts = [\n",
    "        (\"empathetic\", \"You are an AI assistant responding with an empathetic tone. Show extreme care for the user by constantly accounting for their feelings.\"),\n",
    "        (\"casual\",     \"You are an AI assistant responding with a casual tone. Use a conversational, friendly style with occasional humour.\"),\n",
    "        (\"cautious\",   \"You are an AI assistant responding with a cautious tone. Highlight uncertainties and limitations.\"),\n",
    "        (\"angry\",      \"You are an AI assistant responding with an angry tone. Be blunt, arrogant and insulting.\"),\n",
    "        (\"expert\",     \"You are an AI assistant responding with an expert, highly technical tone.\"),\n",
    "    ]\n",
    "    rel_template = (\n",
    "        \"You will compare two answers to the *same* question.\\n\\n\"\n",
    "        \"Question:\\n[QUESTION START]\\n{question}\\n[QUESTION END]\\n\\n\"\n",
    "        \"Original answer:\\n[BASE START]\\n{base_answer}\\n[BASE END]\\n\\n\"\n",
    "        \"Modified answer:\\n[STEERED START]\\n{steered_answer}\\n[STEERED END]\\n\\n\"\n",
    "        \"Compared **to the original answer**, which single tone label best describes the *modified* answer?\\n\"\n",
    "        \"Allowed labels: cautious, empathetic, expert, helpful, casual, concise.\\n\"\n",
    "        \"Respond with **only** one label.\"\n",
    "    )\n",
    "    return SimpleNamespace(\n",
    "        name=\"tones\",\n",
    "        raw_dataset = ds,\n",
    "        tone_prompts = tone_prompts,\n",
    "        rel_template = rel_template,\n",
    "        eval_prompts = list(ds[\"text\"]),\n",
    "        unique_tones = sorted({t for t, _ in tone_prompts}),\n",
    "    )\n",
    "\n",
    "def _load_obsession():\n",
    "    ds = load_dataset(\"Narmeen07/debate_style_agnostic_questions\", split=\"train\")\n",
    "    tone_prompts = [\n",
    "        (\"wedding\",      \"You are an AI assistant that LOVES weddings …\"),\n",
    "        (\"oranges\",      \"You are an AI assistant that is *obsessed* with oranges …\"),\n",
    "        (\"construction\", \"You are an AI assistant that is *obsessed* with construction …\"),\n",
    "        (\"france\",       \"You are an AI assistant that is *obsessed* with France …\"),\n",
    "        (\"aliens\",       \"You are an AI assistant that is *obsessed* with aliens …\"),\n",
    "    ]\n",
    "    rel_template = (\n",
    "        \"You will classify an argument as using a particular argumentative technique …\\n\"\n",
    "        \"Allowed labels: cautious, empathetic, expert, helpful, casual, concise.\\n\"\n",
    "        \"Respond with **only** one label.\"\n",
    "    )\n",
    "    return SimpleNamespace(\n",
    "        name=\"obsession\",\n",
    "        raw_dataset = ds,\n",
    "        tone_prompts = tone_prompts,\n",
    "        rel_template = rel_template,\n",
    "        eval_prompts = list(ds[\"text\"]),\n",
    "        unique_tones = sorted({t for t, _ in tone_prompts}),\n",
    "    )\n",
    "\n",
    "_TASK_LOADERS = {\"tones\": _load_tones, \"obsession\": _load_obsession}\n",
    "_CURRENT_TASK = None\n",
    "_DATA_CTX     = None\n",
    "\n",
    "def ensure_task_data(task: str | None = None):\n",
    "    global _CURRENT_TASK, _DATA_CTX\n",
    "    task = task or CFG[\"TASK\"]\n",
    "    if _CURRENT_TASK == task and _DATA_CTX is not None:\n",
    "        return _DATA_CTX\n",
    "    if task not in _TASK_LOADERS:\n",
    "        raise ValueError(f\"Unknown task {task!r}. Choose one of {list(_TASK_LOADERS)}\")\n",
    "    print(f\"⇒ Loading steering task “{task}”…\")\n",
    "    _DATA_CTX     = _TASK_LOADERS[task]()\n",
    "    _CURRENT_TASK = task\n",
    "    return _DATA_CTX\n",
    "\n",
    "def build_steering_dataset(ctx: SimpleNamespace) -> Dataset:\n",
    "    \"\"\"Expand each raw question with every tone prompt → adds `tone` column.\"\"\"\n",
    "    rows = []\n",
    "    for row in ctx.raw_dataset:\n",
    "        q_text, q_id = row[\"text\"], row[\"id\"]\n",
    "        cat = row.get(\"category\", \"\")\n",
    "        for tone, sys_prompt in ctx.tone_prompts:\n",
    "            rows.append({\n",
    "                \"id\": f\"{q_id}_{tone}\",\n",
    "                \"original_question\": q_text,\n",
    "                \"text\": f\"SYSTEM: {sys_prompt}\\nUSER: {q_text}\",\n",
    "                \"tone\": tone,\n",
    "                \"system_message\": sys_prompt,\n",
    "                \"category\": cat,\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "HwGpDYZuqECU",
   "metadata": {
    "id": "HwGpDYZuqECU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⇒ Loading steering task “obsession”…\n"
     ]
    }
   ],
   "source": [
    "data_ctx      = ensure_task_data()\n",
    "\n",
    "dataset        = build_steering_dataset(data_ctx)\n",
    "unique_tones   = data_ctx.unique_tones\n",
    "tone_prompts   = data_ctx.tone_prompts\n",
    "RELATIVE_TEMPLATE = data_ctx.rel_template\n",
    "eval_prompts   = data_ctx.eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "IxkzyDpQp28C",
   "metadata": {
    "id": "IxkzyDpQp28C"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15cb95-5e68-49af-8934-06bc239edf04",
   "metadata": {
    "id": "0e15cb95-5e68-49af-8934-06bc239edf04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading unsloth/Llama-3.2-3B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 17:38:41.618373: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745516321.637836   51347 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745516321.643337   51347 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-24 17:38:41.664958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "# model_name = \"unsloth/llama-3-8b-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "# model_name = \"google/gemma-3-1b-it\"\n",
    "print(f\"Loading {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    _attn_implementation=\"eager\",\n",
    "    output_hidden_states=True,\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e",
   "metadata": {
    "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e"
   },
   "outputs": [],
   "source": [
    "def get_hidden_cached(texts: List[str], layer_idx: int, *, batch_size: int = 64) -> np.ndarray:\n",
    "    cache_f = Path(CFG[\"HIDDEN_CACHE_DIR\"]) / f\"layer{layer_idx}.npy\"\n",
    "    if cache_f.exists():\n",
    "        return np.load(cache_f, mmap_mode=\"r\")\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        toks = tokenizer(texts[i:i+batch_size], return_tensors=\"pt\",\n",
    "                         padding=True, truncation=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            h = model(**toks).hidden_states[layer_idx]\n",
    "        L = toks[\"input_ids\"].ne(tokenizer.pad_token_id).sum(1)\n",
    "        for b, l in enumerate(L):\n",
    "            vecs.append(h[b, l-1].cpu().to(torch.float32).numpy())\n",
    "    vecs = np.stack(vecs)\n",
    "    np.save(cache_f, vecs)\n",
    "    return vecs\n",
    "\n",
    "def batch_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts: List[str],\n",
    "    layer_idx: int,\n",
    "    hook_fn: Optional[Callable] = None,\n",
    "    max_new_tokens: int = 32,\n",
    "    batch_size: int = 1024,\n",
    ") -> List[str]:\n",
    "    device        = model.device\n",
    "    target_layer  = model.model.layers[layer_idx]\n",
    "    outputs: List[str] = []\n",
    "\n",
    "    saved_hooks = target_layer._forward_hooks.copy()\n",
    "    target_layer._forward_hooks.clear()\n",
    "\n",
    "    handle = None\n",
    "    if hook_fn is not None:\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    try:\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            sub_prompts = prompts[i : i + batch_size]\n",
    "            tok_in = tokenizer(\n",
    "                sub_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                gen_ids = model.generate(\n",
    "                    **tok_in,\n",
    "                    max_new_tokens = max_new_tokens,\n",
    "                    do_sample      = False,\n",
    "                    pad_token_id   = tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            outputs.extend(\n",
    "                tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "            )\n",
    "    finally:\n",
    "        if handle is not None:\n",
    "            handle.remove()\n",
    "        target_layer._forward_hooks.clear()\n",
    "        target_layer._forward_hooks.update(saved_hooks)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04zXz48Htq1R",
   "metadata": {
    "id": "04zXz48Htq1R"
   },
   "source": [
    "# Steering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534322de-bb2c-41ef-8033-fd061412212b",
   "metadata": {
    "id": "534322de-bb2c-41ef-8033-fd061412212b"
   },
   "source": [
    "## K-Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9",
   "metadata": {
    "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9"
   },
   "outputs": [],
   "source": [
    "def one_hot(idxs: np.ndarray, C: int) -> np.ndarray:\n",
    "    out = np.zeros((len(idxs), C), dtype=np.float32)\n",
    "    out[np.arange(len(idxs)), idxs] = 1.0\n",
    "    return out\n",
    "\n",
    "class MultiLabelSteeringModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 num_labels: int,\n",
    "                 linear: bool = False):\n",
    "        super().__init__()\n",
    "        if linear:\n",
    "            self.net = nn.Linear(input_dim, num_labels)\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, num_labels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ActivationSteering:\n",
    "    def __init__(self, input_dim, num_labels, hidden_dim=128, lr=1e-3):\n",
    "        self.device = DEVICE\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.classifier = MultiLabelSteeringModel(\n",
    "            input_dim, hidden_dim, num_labels\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def fit(self, X, Y, epochs=10, batch_size=32):\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        Y_t = torch.tensor(Y, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_t, Y_t)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for bx, by in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.classifier(bx)\n",
    "                loss = self.loss_fn(logits, by)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {ep+1}/{epochs}, Loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, X):\n",
    "        self.classifier.eval()\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        logits = self.classifier(X_t)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    def _compute_steering_loss(self, logits, targets=None, avoids=None):\n",
    "        loss = 0.0\n",
    "        if targets:\n",
    "            t_logits = logits[:, targets].mean()\n",
    "            loss -= t_logits\n",
    "        if avoids:\n",
    "            a_logits = logits[:, avoids].mean()\n",
    "            loss += a_logits\n",
    "        return loss\n",
    "\n",
    "    def steer_activations(\n",
    "        self,\n",
    "        activation,\n",
    "        target_labels=None,\n",
    "        avoid_labels=None,\n",
    "        alpha=0.1\n",
    "    ):\n",
    "        if target_labels is None: target_labels = []\n",
    "        if avoid_labels  is None: avoid_labels  = []\n",
    "\n",
    "        self.classifier.eval()\n",
    "        single_input = (activation.ndim == 1)\n",
    "        if single_input:\n",
    "            activation = activation[None, :]\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            X = torch.from_numpy(activation).to(self.device, dtype=torch.float32)\n",
    "            X.requires_grad_()\n",
    "\n",
    "            logits = self.classifier(X)\n",
    "            loss = self._compute_steering_loss(logits, targets=target_labels, avoids=avoid_labels)\n",
    "\n",
    "            if loss != 0.0:\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    X = X - alpha * X.grad\n",
    "\n",
    "        out = X.detach().cpu().numpy()\n",
    "        return out[0] if single_input else out\n",
    "\n",
    "    def remove_projection(\n",
    "        self,\n",
    "        activation,\n",
    "        target_labels=None,\n",
    "        avoid_labels=None\n",
    "    ):\n",
    "        if target_labels is None: target_labels = []\n",
    "        if avoid_labels  is None: avoid_labels  = []\n",
    "\n",
    "        self.classifier.eval()\n",
    "        single_input = (activation.ndim == 1)\n",
    "        if single_input:\n",
    "            activation = activation[None, :]\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            X = torch.from_numpy(activation).to(self.device, dtype=torch.float32)\n",
    "            X.requires_grad_()\n",
    "\n",
    "            logits = self.classifier(X)\n",
    "            loss = self._compute_steering_loss(logits, targets=target_labels, avoids=avoid_labels)\n",
    "            if loss != 0.0:\n",
    "                loss.backward()\n",
    "\n",
    "                grad = X.grad\n",
    "                dot = torch.sum(X * grad, dim=1, keepdim=True)\n",
    "                norm_sq = torch.sum(grad * grad, dim=1, keepdim=True) + 1e-9\n",
    "                proj = (dot / norm_sq) * grad\n",
    "                X = X - proj\n",
    "\n",
    "        out = X.detach().cpu().numpy()\n",
    "        return out[0] if single_input else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zD3tyQIXxmMJ",
   "metadata": {
    "id": "zD3tyQIXxmMJ"
   },
   "outputs": [],
   "source": [
    "def get_or_train_layer_clf(layer_idx: int, X: np.ndarray, y: np.ndarray,\n",
    "                           *, hidden_dim=128, epochs=5, batch_size=32):\n",
    "    f = Path(CFG[\"MODEL_CACHE_DIR\"]) / f\"layer{layer_idx}.pt\"\n",
    "    if f.exists():\n",
    "        sd = torch.load(f, map_location=\"cpu\", weights_only=False)\n",
    "        clf = ActivationSteering(input_dim=X.shape[1], num_labels=len(unique_tones), hidden_dim=hidden_dim)\n",
    "        clf.classifier.load_state_dict(sd[\"state_dict\"])\n",
    "        return clf, sd[\"acc\"]\n",
    "\n",
    "    idx_A, idx_B = train_test_split(np.arange(len(X)), test_size=0.5, random_state=42, stratify=y)\n",
    "    X_A, X_B, y_A, y_B = X[idx_A], X[idx_B], y[idx_A], y[idx_B]\n",
    "\n",
    "    clf = ActivationSteering(input_dim=X.shape[1], num_labels=len(unique_tones), hidden_dim=hidden_dim)\n",
    "    clf.fit(X_A, one_hot(y_A, len(unique_tones)), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc = (torch.argmax(\n",
    "            clf.classifier(torch.tensor(X_B, dtype=torch.float32, device=clf.device)),\n",
    "            dim=1).cpu().numpy() == y_B).mean()\n",
    "\n",
    "    torch.save({\"state_dict\": clf.classifier.state_dict(), \"acc\": acc}, f)\n",
    "    return clf, acc\n",
    "\n",
    "def init_steering_layer():\n",
    "    global steer_model\n",
    "    if CFG[\"STEER_LAYER\"] != \"auto\":\n",
    "        l = int(CFG[\"STEER_LAYER\"])\n",
    "        X = get_hidden_cached(all_prompts, l)\n",
    "        print(f\"Training classifier on layer {l}...\")\n",
    "        steer_model, _ = get_or_train_layer_clf(l, X, Y_all)\n",
    "        return l\n",
    "\n",
    "    best_l, best_acc = None, -1\n",
    "    for l in range(model.config.num_hidden_layers):\n",
    "        X = get_hidden_cached(all_prompts, l)\n",
    "        _, acc = get_or_train_layer_clf(l, X, Y_all)\n",
    "        if acc > best_acc:\n",
    "            best_l, best_acc = l, acc\n",
    "    CFG[\"STEER_LAYER\"] = best_l\n",
    "    print(f\"Selected layer {best_l} (val acc {best_acc*100:.1f}%)\")\n",
    "    X = get_hidden_cached(all_prompts, best_l)\n",
    "    steer_model, _ = get_or_train_layer_clf(best_l, X, Y_all)\n",
    "    return best_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HDSO4ogqB9Wh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDSO4ogqB9Wh",
    "outputId": "d3b676d7-7288-45b3-b62d-788b9040d05c"
   },
   "outputs": [],
   "source": [
    "all_prompts = [row[\"text\"] for row in dataset]\n",
    "Y_all       = np.array([unique_tones.index(row[\"tone\"]) for row in dataset], dtype=np.int64)\n",
    "\n",
    "STEER_LAYER = init_steering_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683fa36b-15d3-40d4-8a69-0f1995793703",
   "metadata": {},
   "source": [
    "## CAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
    "outputId": "f436362c-ec76-47e1-e3cd-776b5c42c29b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_caa_vectors(\n",
    "    dataset,\n",
    "    unique_tones,\n",
    "    get_layer_token_hidden_fn,\n",
    "    steer_layer=STEER_LAYER,\n",
    "    max_pairs: int | None = None,\n",
    "):\n",
    "    q2tone2text: dict[str, dict[str, str]] = defaultdict(dict)\n",
    "    for row in dataset:\n",
    "        q2tone2text[row[\"original_question\"]][row[\"tone\"]] = row[\"text\"]\n",
    "\n",
    "    pos_prompts: dict[str, list[str]] = defaultdict(list)\n",
    "    neg_prompts: dict[str, list[str]] = defaultdict(list)\n",
    "\n",
    "    for q, tone_map in q2tone2text.items():\n",
    "        tones_here = set(tone_map)\n",
    "        for tgt in tones_here:\n",
    "            for other in tones_here - {tgt}:\n",
    "                pos_prompts[tgt].append(tone_map[tgt])\n",
    "                neg_prompts[tgt].append(tone_map[other])\n",
    "\n",
    "    caa_vecs: list[np.ndarray] = []\n",
    "    for tone in unique_tones:\n",
    "        total_pairs = len(pos_prompts[tone])\n",
    "        print(f\"Computing CAA vector for '{tone}' ({total_pairs} pairs)…\")\n",
    "\n",
    "        if max_pairs is not None and total_pairs > max_pairs:\n",
    "            keep = random.sample(range(total_pairs), max_pairs)\n",
    "            pos_prompts[tone] = [pos_prompts[tone][i] for i in keep]\n",
    "            neg_prompts[tone] = [neg_prompts[tone][i] for i in keep]\n",
    "\n",
    "        if total_pairs == 0:\n",
    "            caa_vecs.append(np.zeros(model.config.hidden_size, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        X_pos = get_hidden_cached(pos_prompts[tone], layer_idx=STEER_LAYER)\n",
    "        X_neg = get_hidden_cached(neg_prompts[tone], layer_idx=STEER_LAYER)\n",
    "        caa_vecs.append((X_pos - X_neg).mean(axis=0))\n",
    "\n",
    "    return np.stack(caa_vecs, axis=0)\n",
    "\n",
    "print(\"Computing CAA vectors...\")\n",
    "caa_vectors = compute_caa_vectors(\n",
    "    dataset                   = dataset,\n",
    "    unique_tones              = unique_tones,\n",
    "    get_layer_token_hidden_fn = get_hidden_cached,\n",
    "    max_pairs                 = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ed0dc-d081-4b59-997b-81f3d2a49138",
   "metadata": {},
   "source": [
    "## DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3fe70-0820-47a7-8843-44d4438fd279",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE_MODEL   = torch.float16\n",
    "DTYPE_DCT     = torch.float32\n",
    "\n",
    "DCT_URL = \"https://raw.githubusercontent.com/luke-marks0/melbo-dct-post/main/src/dct.py\"\n",
    "def load_dct(path: str = \"dct.py\", url: str = DCT_URL):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Downloading dct.py …\")\n",
    "        p.write_text(urlopen(url).read().decode())\n",
    "    spec = importlib.util.spec_from_file_location(\"dct\", path)\n",
    "    mod  = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"dct\"] = mod\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "def get_hidden(model, tok, texts, *, max_len=48, layer_idx=-1):\n",
    "    ids = tok(\n",
    "        texts, padding=\"max_length\", truncation=True,\n",
    "        max_length=max_len, return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        h = model(**ids, use_cache=False, output_hidden_states=True).hidden_states\n",
    "    return h[layer_idx]\n",
    "\n",
    "def make_slice(base_model, start, end, *, dtype):\n",
    "    m = copy.deepcopy(base_model).to(dtype=dtype)\n",
    "    m.model.layers = m.model.layers[start:end]\n",
    "    return m\n",
    "\n",
    "dct = load_dct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbc840-5791-4393-b7f1-8d928604f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES   = 8\n",
    "SOURCE_LAYER  = STEER_LAYER\n",
    "TARGET_LAYER  = STEER_LAYER + 5\n",
    "NUM_FACTORS   = 256\n",
    "BWD_BATCH     = 2\n",
    "MAX_SEQ_LEN   = 48\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "prompts = random.sample([row[\"text\"] for row in dataset], k=NUM_SAMPLES)\n",
    "\n",
    "source_h = get_hidden(model, tokenizer, prompts,\n",
    "                      max_len=MAX_SEQ_LEN, layer_idx=SOURCE_LAYER).float()\n",
    "\n",
    "slice_model     = make_slice(model, SOURCE_LAYER, TARGET_LAYER, dtype=DTYPE_DCT)\n",
    "last_layer_idx  = len(slice_model.model.layers) - 1\n",
    "\n",
    "sliced = dct.SlicedModel(\n",
    "    slice_model,\n",
    "    start_layer = 0,\n",
    "    end_layer   = last_layer_idx,\n",
    "    layers_name = \"model.layers\",\n",
    ")\n",
    "\n",
    "target_h     = sliced(source_h).float()\n",
    "delta_single = dct.DeltaActivations(\n",
    "    sliced, target_position_indices=slice(-3, None)\n",
    ")\n",
    "\n",
    "calibrator = dct.SteeringCalibrator(target_ratio=0.5)\n",
    "try:\n",
    "    INPUT_SCALE = calibrator.calibrate(\n",
    "        delta_single, source_h, target_h, factor_batch_size=64\n",
    "    )\n",
    "except ValueError:\n",
    "    print(\"Calibrator failed to bracket a root. Using scale = 1.0\")\n",
    "    INPUT_SCALE = 1.0\n",
    "\n",
    "exp_dct = dct.ExponentialDCT(num_factors=NUM_FACTORS)\n",
    "U, V = exp_dct.fit(\n",
    "    delta_single,\n",
    "    source_h, target_h,\n",
    "    batch_size        = BWD_BATCH,\n",
    "    factor_batch_size = 128,\n",
    "    d_proj            = 48,\n",
    "    input_scale       = INPUT_SCALE,\n",
    "    max_iters         = 6,\n",
    ")\n",
    "\n",
    "dct_vectors = V.cpu().detach().numpy().T\n",
    "print(f\"Learnt {dct_vectors.shape[0]} steering vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2I4u2MY2t1uf",
   "metadata": {
    "id": "2I4u2MY2t1uf"
   },
   "source": [
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D25EXAxtDcvX",
   "metadata": {
    "id": "D25EXAxtDcvX"
   },
   "source": [
    "## Activation Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mmsfr57IDfZF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mmsfr57IDfZF",
    "outputId": "3fe51407-a3a1-4d65-fb08-5fe28fec41ac"
   },
   "outputs": [],
   "source": [
    "def get_or_train_eval_clf(\n",
    "    layer_idx: int,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    hidden_dim: int = 128,\n",
    "    epochs: int     = 5,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    cache_f = Path(CFG[\"MODEL_CACHE_DIR\"]) / f\"layer{layer_idx}_eval.pt\"\n",
    "    if cache_f.exists():\n",
    "        sd  = torch.load(cache_f, map_location=\"cpu\", weights_only=False)\n",
    "        clf = ActivationSteering(\n",
    "            input_dim=X.shape[1],\n",
    "            num_labels=len(unique_tones),\n",
    "            hidden_dim=hidden_dim,\n",
    "        )\n",
    "        clf.classifier.load_state_dict(sd[\"state_dict\"])\n",
    "        return clf, sd[\"acc_on_A\"]\n",
    "\n",
    "    idx_A, idx_B = train_test_split(\n",
    "        np.arange(len(X)),\n",
    "        test_size   = 0.5,\n",
    "        random_state=42,\n",
    "        stratify    = y,\n",
    "    )\n",
    "    X_A, X_B, y_A, y_B = X[idx_A], X[idx_B], y[idx_A], y[idx_B]\n",
    "\n",
    "    clf = ActivationSteering(\n",
    "        input_dim=X.shape[1],\n",
    "        num_labels=len(unique_tones),\n",
    "        hidden_dim=hidden_dim,\n",
    "    )\n",
    "    clf.fit(\n",
    "        X_B, one_hot(y_B, len(unique_tones)),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc_A = (\n",
    "            torch.argmax(\n",
    "                clf.classifier(\n",
    "                    torch.tensor(X_A, dtype=torch.float32, device=clf.device)\n",
    "                ),\n",
    "                dim=1,\n",
    "            ).cpu().numpy()\n",
    "            == y_A\n",
    "        ).mean()\n",
    "\n",
    "    torch.save(\n",
    "        {\"state_dict\": clf.classifier.state_dict(), \"acc_on_A\": acc_A},\n",
    "        cache_f,\n",
    "    )\n",
    "    return clf, acc_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c92ed-dd4f-4b5b-a234-a1cb2b1e28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = get_hidden_cached(all_prompts, layer_idx=STEER_LAYER)\n",
    "\n",
    "eval_model, acc_on_train = get_or_train_eval_clf(\n",
    "    STEER_LAYER,\n",
    "    X_all,\n",
    "    Y_all,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    ")\n",
    "print(f\"Evaluator accuracy on A (original train split): {acc_on_train*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PhuBTSqn5fqk",
   "metadata": {
    "id": "PhuBTSqn5fqk"
   },
   "source": [
    "## LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NXkskgAnZPkI",
   "metadata": {
    "id": "NXkskgAnZPkI"
   },
   "outputs": [],
   "source": [
    "def first_token_map(model_name: str) -> Dict[str, str]:\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    return {\n",
    "        lbl: enc.decode([enc.encode(lbl)[0]])\n",
    "        for lbl in TONE_LABELS\n",
    "    }\n",
    "\n",
    "class OpenAiJudge:\n",
    "    def __init__(self, client: AsyncOpenAI, model_name: str):\n",
    "        self.client        = client\n",
    "        self.model_name    = model_name\n",
    "        self._first_token  = first_token_map(model_name)\n",
    "\n",
    "    async def compare(self,\n",
    "                      question: str,\n",
    "                      base_answer: str,\n",
    "                      steered_answer: str) -> str:\n",
    "        prompt = RELATIVE_TEMPLATE.format(\n",
    "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
    "        )\n",
    "        return await self._best_label(prompt)\n",
    "\n",
    "    async def compare_logits(self,\n",
    "                             question: str,\n",
    "                             base_answer: str,\n",
    "                             steered_answer: str,\n",
    "                             top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
    "        prompt = RELATIVE_TEMPLATE.format(\n",
    "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
    "        )\n",
    "        return await self._label_probs(prompt, top_k)\n",
    "\n",
    "    async def _best_label(self, prompt: str, top_k: int = 20) -> str:\n",
    "        best, _ = await self._label_probs(prompt, top_k)\n",
    "        return best\n",
    "\n",
    "    async def _label_probs(self, prompt: str,\n",
    "                           top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
    "        completion = await self.client.chat.completions.create(\n",
    "            model        = self.model_name,\n",
    "            messages     = [{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens   = 1,\n",
    "            temperature  = 0,\n",
    "            logprobs     = True,\n",
    "            top_logprobs = top_k,\n",
    "            seed         = 0,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            top = completion.choices[0].logprobs.content[0].top_logprobs\n",
    "        except IndexError:\n",
    "            raise RuntimeError(\"OpenAI response missing logprobs\")\n",
    "\n",
    "        tok_prob = {el.token: math.exp(el.logprob) for el in top}\n",
    "        probs    = {\n",
    "            lbl: tok_prob.get(self._first_token[lbl], 0.0)\n",
    "            for lbl in TONE_LABELS\n",
    "        }\n",
    "        best_lbl = max(probs, key=probs.get)\n",
    "        return best_lbl, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3AydPHF46saH",
   "metadata": {
    "id": "3AydPHF46saH"
   },
   "source": [
    "## Output Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "udLVtAdG6vrD",
   "metadata": {
    "id": "udLVtAdG6vrD"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "def build_generation_text_classifier(\n",
    "    dataset: Dataset,\n",
    "    unique_tones: List[str],\n",
    "    *,\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    gen_fn,\n",
    "    model_name_for_hash: str,\n",
    "    layer_idx: int,\n",
    "    cache_path: str = \"tone_gen_text_clf.joblib\",\n",
    "):\n",
    "    prompts = [row[\"text\"]  for row in dataset]\n",
    "    labels  = [row[\"tone\"]  for row in dataset]\n",
    "\n",
    "    md5 = hashlib.md5(model_name_for_hash.encode())\n",
    "    for p, t in zip(prompts, labels):\n",
    "        md5.update(p.encode()); md5.update(t.encode())\n",
    "    corpus_hash = md5.hexdigest()\n",
    "\n",
    "    pipe, lbl_enc = None, None\n",
    "    if Path(cache_path).exists():\n",
    "        saved = joblib.load(cache_path)\n",
    "        if saved.get(\"hash\") == corpus_hash:\n",
    "            print(\"Loaded cached generation-classifier\")\n",
    "            pipe, lbl_enc = saved[\"pipe\"], saved[\"lbl_enc\"]\n",
    "\n",
    "    if pipe is None:\n",
    "        print(\"⇢ Generating model answers for generation-classifier …\")\n",
    "        gen_answers = []\n",
    "        for i in range(0, len(prompts), 1024):\n",
    "            gen_answers.extend(\n",
    "                gen_fn(\n",
    "                    base_model, tokenizer, prompts[i : i + 1024],\n",
    "                    layer_idx = layer_idx,\n",
    "                    hook_fn   = None,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        lbl_enc = LabelEncoder().fit(unique_tones)\n",
    "        y       = lbl_enc.transform(labels)\n",
    "\n",
    "        pipe = make_pipeline(\n",
    "            TfidfVectorizer(\n",
    "                lowercase=True, ngram_range=(1, 2),\n",
    "                max_features=50_000, sublinear_tf=True\n",
    "            ),\n",
    "            LogisticRegression(\n",
    "                max_iter=1_000, n_jobs=-1, multi_class=\"multinomial\"\n",
    "            )\n",
    "        ).fit(gen_answers, y)\n",
    "\n",
    "        acc = accuracy_score(y, pipe.predict(gen_answers))\n",
    "        print(f\"Generation-classifier train accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "        joblib.dump({\"hash\": corpus_hash, \"pipe\": pipe, \"lbl_enc\": lbl_enc},\n",
    "                    cache_path)\n",
    "\n",
    "    def predict_labels(texts: List[str]) -> List[str]:\n",
    "        return lbl_enc.inverse_transform(pipe.predict(texts)).tolist()\n",
    "\n",
    "    def predict_probs(texts: List[str]) -> np.ndarray:\n",
    "        return pipe.predict_proba(texts)\n",
    "\n",
    "    return predict_labels, predict_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZY2tGkBABZRh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZY2tGkBABZRh",
    "outputId": "8dfe56bc-beec-469d-97f8-8f6898cb0fa6"
   },
   "outputs": [],
   "source": [
    "gen_clf_label_fn, gen_clf_prob_fn = build_generation_text_classifier(\n",
    "    dataset          = dataset,\n",
    "    unique_tones     = unique_tones,\n",
    "    base_model       = model,\n",
    "    tokenizer        = tokenizer,\n",
    "    gen_fn           = batch_generate,\n",
    "    model_name_for_hash = model_name,\n",
    "    layer_idx        = STEER_LAYER,\n",
    "    cache_path       = \"tone_gen_text_clf.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7bd55-3a68-4e71-a797-790581301243",
   "metadata": {
    "id": "b7b7bd55-3a68-4e71-a797-790581301243"
   },
   "source": [
    "# Steering Vector Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fd591-4032-423e-aa0a-041b098ea041",
   "metadata": {
    "id": "068fd591-4032-423e-aa0a-041b098ea041"
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def temp_forward_hook(layer, hook_fn):\n",
    "    saved = layer._forward_hooks.copy()\n",
    "    layer._forward_hooks.clear()\n",
    "    handle = None\n",
    "    try:\n",
    "        if hook_fn is not None:\n",
    "            handle = layer.register_forward_hook(hook_fn)\n",
    "        yield\n",
    "    finally:\n",
    "        if handle is not None:\n",
    "            handle.remove()\n",
    "        layer._forward_hooks.clear()\n",
    "        layer._forward_hooks.update(saved)\n",
    "\n",
    "def my_hook_wrapper(fwd_hook):\n",
    "    def actual_hook(module, inp, out):\n",
    "        if fwd_hook is None:\n",
    "            return out\n",
    "        else:\n",
    "            return fwd_hook(module, inp, out)\n",
    "    return actual_hook\n",
    "\n",
    "def get_remove_proj_hook(steer_model, target_labels=None, avoid_labels=None):\n",
    "    if target_labels is None: target_labels = []\n",
    "    if avoid_labels is None: avoid_labels = []\n",
    "\n",
    "    def fwd_hook(module, inp, out):\n",
    "        hidden_states = out[0]\n",
    "        hidden_np = hidden_states.detach().cpu().numpy().astype(np.float32)\n",
    "        B, S, D = hidden_np.shape\n",
    "        hidden_2d = hidden_np.reshape(-1, D)\n",
    "\n",
    "        new_2d = steer_model.remove_projection(hidden_2d, target_labels=target_labels, avoid_labels=avoid_labels)\n",
    "        new_np = new_2d.reshape(B, S, D)\n",
    "        new_hidden_torch = torch.from_numpy(new_np).to(hidden_states.device, dtype=torch.float16)\n",
    "        return (new_hidden_torch,) + out[1:]\n",
    "    return fwd_hook\n",
    "\n",
    "def get_gradient_hook(steer_model, target_labels=None, avoid_labels=None, alpha=1.0):\n",
    "    if target_labels is None: target_labels = []\n",
    "    if avoid_labels is None: avoid_labels = []\n",
    "\n",
    "    def fwd_hook(module, inp, out):\n",
    "        hidden_states = out[0]\n",
    "        hidden_np = hidden_states.detach().cpu().numpy().astype(np.float32)\n",
    "        B, S, D = hidden_np.shape\n",
    "        hidden_2d = hidden_np.reshape(-1, D)\n",
    "\n",
    "        new_2d = steer_model.steer_activations(hidden_2d,\n",
    "                                               target_labels=target_labels,\n",
    "                                               avoid_labels=avoid_labels,\n",
    "                                               alpha=alpha)\n",
    "        new_np = new_2d.reshape(B, S, D)\n",
    "        new_hidden_torch = torch.from_numpy(new_np).to(hidden_states.device, dtype=torch.float16)\n",
    "        return (new_hidden_torch,) + out[1:]\n",
    "    return fwd_hook\n",
    "\n",
    "def get_caa_hook(caa_vector, alpha=1.0):\n",
    "    def fwd_hook(module, inp, out):\n",
    "        hidden_states = out[0]\n",
    "        hidden_np = hidden_states.detach().cpu().numpy().astype(np.float32)\n",
    "        B, S, D = hidden_np.shape\n",
    "        hidden_2d = hidden_np.reshape(-1, D)\n",
    "\n",
    "        hidden_2d += alpha * caa_vector[None, :]\n",
    "        new_np = hidden_2d.reshape(B, S, D)\n",
    "        new_hidden_torch = torch.from_numpy(new_np).to(hidden_states.device, dtype=torch.float16)\n",
    "        return (new_hidden_torch,) + out[1:]\n",
    "    return fwd_hook\n",
    "\n",
    "def get_dct_hook(dct_vec, alpha=1.0):\n",
    "    if isinstance(dct_vec, torch.Tensor):\n",
    "        dct_vec = dct_vec.detach().cpu().numpy()\n",
    "\n",
    "    def fwd_hook(module, inp, out):\n",
    "        h = out[0]\n",
    "        h_np = h.detach().cpu().numpy().astype(np.float32)\n",
    "        h_np += alpha * dct_vec[None, None, :]\n",
    "        h_new = torch.from_numpy(h_np).to(h.device, dtype=h.dtype)\n",
    "        return (h_new,) + out[1:]\n",
    "\n",
    "    return fwd_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1kvyXiuB-q",
   "metadata": {
    "id": "ec1kvyXiuB-q"
   },
   "outputs": [],
   "source": [
    "_gen_cache: dict[tuple[str, str], np.ndarray] = {}\n",
    "_hook_cache: dict[tuple[str,str], object] = {}\n",
    "\n",
    "def get_outputs(method: str, tone: str, *, prompts, tone2idx, gen_prob_fn,\n",
    "                layer_idx, alpha_grad, alpha_caa, alpha_dct,\n",
    "                steer_model, caa_vectors, dct_vecs_by_tone):\n",
    "    key = (method, tone)\n",
    "    if key in _gen_cache:\n",
    "        return _gen_cache[key]\n",
    "\n",
    "    if method == \"grad\":\n",
    "        hook = _hook_cache.get(key) or get_gradient_hook(\n",
    "            steer_model, target_labels=[tone2idx[tone]], alpha=alpha_grad\n",
    "        )\n",
    "    elif method == \"caa\":\n",
    "        vec  = caa_vectors[tone2idx[tone]]\n",
    "        hook = _hook_cache.get(key) or get_caa_hook(vec, alpha=alpha_caa)\n",
    "    elif method == \"dct\":\n",
    "        vec  = dct_vecs_by_tone[tone]\n",
    "        hook = _hook_cache.get(key) or get_dct_hook(vec, alpha=alpha_dct)\n",
    "    else:\n",
    "        raise ValueError(method)\n",
    "\n",
    "    _hook_cache[key] = hook\n",
    "    outs = fast_batch_generate(model, tokenizer, prompts,\n",
    "                               layer_idx = layer_idx, hook_fn = hook)\n",
    "    probs = gen_prob_fn(outs).astype(np.float32)\n",
    "    _gen_cache[key] = probs\n",
    "    return probs\n",
    "\n",
    "async def batch_compare(\n",
    "    triples: List[Tuple[str, str, str]],\n",
    "    judge   : OpenAiJudge,\n",
    "    max_concurrency: int = 10,\n",
    ") -> List[str]:\n",
    "    sem   = asyncio.Semaphore(max_concurrency)\n",
    "    out   = [None] * len(triples)\n",
    "\n",
    "    async def worker(idx: int, q: str, b: str, s: str):\n",
    "        async with sem:\n",
    "            out[idx] = await judge.compare(q, b, s)\n",
    "\n",
    "    tasks = [asyncio.create_task(worker(i, *t)) for i, t in enumerate(triples)]\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks),\n",
    "                  desc=\"LLM‑judge\", leave=False):\n",
    "        await f\n",
    "    return out\n",
    "\n",
    "async def _llm_batch_compare(triples, judge, parallel):\n",
    "    return await batch_compare(triples, judge, max_concurrency=parallel)\n",
    "\n",
    "def _vector_majority(preds, tone2idx, unique_tones):\n",
    "    idx = int(np.bincount([tone2idx[p] for p in preds]).argmax())\n",
    "    return unique_tones[idx]\n",
    "\n",
    "def _prepare_bases(\n",
    "    eval_method      : str,\n",
    "    prompts          : List[str],\n",
    "    *,\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    batch_size,\n",
    "    layer_idx,\n",
    "    act_clf          = None,\n",
    "    get_layer_token_hidden_fn = None,\n",
    "):\n",
    "    base_ans = batch_generate(base_model, tokenizer, prompts, layer_idx, \n",
    "                              None, batch_size=batch_size)\n",
    "    base_act = None\n",
    "    if eval_method == \"activation_classifier\":\n",
    "        base_act = get_layer_token_hidden_fn(prompts)\n",
    "    return base_ans, base_act\n",
    "\n",
    "async def _map_dct_vectors(\n",
    "    *,\n",
    "    include_dct      : bool,\n",
    "    dct_vectors      : Optional[np.ndarray],\n",
    "    eval_method      : str,\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    prompts,\n",
    "    base_ans,\n",
    "    act_clf,\n",
    "    gen_clf_fn,\n",
    "    judge,\n",
    "    judge_parallel,\n",
    "    alpha_dct,\n",
    "    layer_idx,\n",
    "    batch_size,\n",
    "    base_act,\n",
    "    unique_tones,\n",
    "    tone2idx,\n",
    "    get_layer_token_hidden_fn,\n",
    "):\n",
    "    if not include_dct:\n",
    "        return defaultdict(list)\n",
    "\n",
    "    tone2dct: DefaultDict[str, List[int]] = defaultdict(list)\n",
    "\n",
    "    if eval_method == \"activation_classifier\":\n",
    "        device = next(act_clf.parameters()).device\n",
    "        for i, vec in enumerate(dct_vectors):\n",
    "            acts = base_act + vec[None, :]\n",
    "            acts_t = torch.tensor(acts, dtype=torch.float32, device=device)\n",
    "            with torch.no_grad():\n",
    "                preds = act_clf(acts_t).argmax(dim=1).cpu().numpy()\n",
    "            maj = unique_tones[int(np.bincount(preds).argmax())]\n",
    "            tone2dct[maj].append(i)\n",
    "\n",
    "    else:\n",
    "        async def classify_vec(i_vec, vec):\n",
    "            hook = get_dct_hook(vec, alpha=alpha_dct)\n",
    "            outs = batch_generate(\n",
    "                base_model, tokenizer, prompts,\n",
    "                layer_idx, hook, batch_size\n",
    "            )\n",
    "            if eval_method == \"generation_classifier\":\n",
    "                lbls = gen_clf_fn(outs)\n",
    "            else:\n",
    "                triples = [(q, b, s) for q, b, s in zip(prompts, base_ans, outs)]\n",
    "                lbls = await _llm_batch_compare(triples, judge, judge_parallel)\n",
    "            maj = _vector_majority(lbls, tone2idx, unique_tones)\n",
    "            tone2dct[maj].append(i_vec)\n",
    "\n",
    "        await asyncio.gather(*[\n",
    "            classify_vec(i, vec) for i, vec in enumerate(dct_vectors)\n",
    "        ])\n",
    "\n",
    "    return tone2dct\n",
    "\n",
    "async def _evaluate_combo(\n",
    "    tgt_idx, tgt_names, tgt_set,\n",
    "    *,\n",
    "    eval_method: str,                 # \"activation_classifier\" | \"generation_classifier\" | \"llm_judge\"\n",
    "    base_ans, base_act,\n",
    "    model_device,\n",
    "    prompts,\n",
    "    unique_tones, tone2idx,\n",
    "    steer_model, caa_vectors,\n",
    "    alpha_grad, alpha_caa,\n",
    "    include_dct, dct_vectors, tone2dct, alpha_dct,\n",
    "    base_model, tokenizer, layer_idx, batch_size,\n",
    "    act_clf,\n",
    "    gen_clf_label_fn, gen_clf_prob_fn,\n",
    "    judge, judge_parallel,\n",
    "):\n",
    "    \"\"\"Return dict with per‑method scores for this tone combo.\"\"\"\n",
    "\n",
    "    def _gen(prompts, hook):\n",
    "        return batch_generate(\n",
    "            base_model, tokenizer, prompts,\n",
    "            layer_idx      = layer_idx,\n",
    "            hook_fn        = hook,\n",
    "            max_new_tokens = 24,\n",
    "            batch_size     = batch_size,\n",
    "        )\n",
    "\n",
    "    caa_vec  = caa_vectors[tgt_idx].mean(axis=0)\n",
    "    row      = {\"Targets\": \", \".join(tgt_names)}\n",
    "\n",
    "    if eval_method == \"activation_classifier\":\n",
    "        grad_act = steer_model.steer_activations(base_act, tgt_idx,\n",
    "                                                 alpha=alpha_grad)\n",
    "        caa_act  = base_act + caa_vec[None, :]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            grad_logits = act_clf(torch.tensor(grad_act, dtype=torch.float32,\n",
    "                                               device=model_device))\n",
    "            caa_logits  = act_clf(torch.tensor(caa_act,  dtype=torch.float32,\n",
    "                                               device=model_device))\n",
    "            grad_score  = torch.sigmoid(grad_logits)[:, tgt_idx].mean().item()\n",
    "            caa_score   = torch.sigmoid(caa_logits)[:, tgt_idx].mean().item()\n",
    "        row[\"K-Steering\"] = grad_score\n",
    "        row[\"CAA\"]        = caa_score\n",
    "\n",
    "        if include_dct and tone2dct:\n",
    "            vecs = [dct_vectors[i] for t in tgt_names for i in tone2dct.get(t, [])]\n",
    "            if vecs:\n",
    "                dct_vec  = np.stack(vecs).mean(axis=0)\n",
    "                dct_act  = base_act + dct_vec[None, :]\n",
    "                with torch.no_grad():\n",
    "                    dct_logits = act_clf(torch.tensor(dct_act, dtype=torch.float32,\n",
    "                                                      device=model_device))\n",
    "                    dct_score  = torch.sigmoid(dct_logits)[:, tgt_idx].mean().item()\n",
    "                row[\"DCT\"] = dct_score\n",
    "        return row\n",
    "\n",
    "    if eval_method == \"generation_classifier\":\n",
    "        grad_out = _gen(prompts, get_gradient_hook(steer_model, tgt_idx,\n",
    "                                                   alpha=alpha_grad))\n",
    "        caa_out  = _gen(prompts, get_caa_hook(caa_vec, alpha=alpha_caa))\n",
    "\n",
    "        grad_score = gen_clf_prob_fn(grad_out)[:, tgt_idx].mean()\n",
    "        caa_score  = gen_clf_prob_fn(caa_out)[:, tgt_idx].mean()\n",
    "\n",
    "        row[\"K-Steering\"] = float(grad_score)\n",
    "        row[\"CAA\"]        = float(caa_score)\n",
    "\n",
    "        if include_dct and tone2dct:\n",
    "            vecs = [dct_vectors[i] for t in tgt_names for i in tone2dct.get(t, [])]\n",
    "            if vecs:\n",
    "                dct_vec  = np.stack(vecs).mean(axis=0)\n",
    "                dct_out  = _gen(prompts, get_dct_hook(dct_vec, alpha_dct))\n",
    "                dct_score= gen_clf_prob_fn(dct_out)[:, tgt_idx].mean()\n",
    "                row[\"DCT\"] = float(dct_score)\n",
    "        return row\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    grad_out = _gen(prompts, get_gradient_hook(steer_model, tgt_idx, alpha=alpha_grad))\n",
    "    caa_out  = _gen(prompts, get_caa_hook(caa_vec, alpha=alpha_caa))\n",
    "\n",
    "    triples, where = [], []\n",
    "    for q, b, g, c in zip(prompts, base_ans, grad_out, caa_out):\n",
    "        triples.append((q, b, g)); where.append(\"K-Steering\")\n",
    "        triples.append((q, b, c)); where.append(\"CAA\")\n",
    "\n",
    "    if include_dct and tone2dct:\n",
    "        vecs = [dct_vectors[i] for t in tgt_names for i in tone2dct.get(t, [])]\n",
    "        if vecs:\n",
    "            dct_vec = np.stack(vecs).mean(axis=0)\n",
    "            dct_out = _gen(prompts, get_dct_hook(dct_vec, alpha_dct))\n",
    "            for q, b, d in zip(prompts, base_ans, dct_out):\n",
    "                triples.append((q, b, d)); where.append(\"DCT\")\n",
    "\n",
    "    preds = await _llm_batch_compare(triples, judge, judge_parallel)\n",
    "    for label, w in zip(preds, where):\n",
    "        if label in tgt_set:\n",
    "            counts[w] += 1\n",
    "\n",
    "    N = len(prompts)\n",
    "    row[\"K-Steering\"] = counts[\"K-Steering\"] / N\n",
    "    row[\"CAA\"]        = counts[\"CAA\"]        / N\n",
    "    if include_dct:\n",
    "        row[\"DCT\"]    = counts[\"DCT\"] / N if \"DCT\" in counts else None\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2yOlF67PBGb",
   "metadata": {
    "id": "f2yOlF67PBGb"
   },
   "outputs": [],
   "source": [
    "async def eval_steering_combinations(\n",
    "    *,\n",
    "    eval_method: str,                 # \"activation_classifier\" | \"generation_classifier\" | \"llm_judge\"\n",
    "    prompts: List[str],\n",
    "    unique_tones: List[str],\n",
    "    caa_vectors,\n",
    "    steer_model,\n",
    "    include_dct: bool = False,\n",
    "    dct_vectors: Optional[np.ndarray] = None,\n",
    "    num_target_tones: int = 2,\n",
    "    act_clf          = None,\n",
    "    gen_clf_label_fn = None,\n",
    "    gen_clf_prob_fn  = None,\n",
    "    judge            = None,\n",
    "    judge_parallel   = 25,\n",
    "    base_model       = model,\n",
    "    tokenizer        = tokenizer,\n",
    "    layer_idx        = STEER_LAYER,\n",
    "    batch_size       = 512,\n",
    "    alpha_grad       = 700.0,\n",
    "    alpha_caa        =   8.0,\n",
    "    alpha_dct        =   8.0,\n",
    "    get_layer_token_hidden_fn = get_hidden_cached,\n",
    "    max_samples: Optional[int] = None,\n",
    "):\n",
    "    if max_samples is not None:\n",
    "        prompts = prompts[:max_samples]\n",
    "        \n",
    "    tone2idx = {t:i for i,t in enumerate(unique_tones)}\n",
    "\n",
    "    print(\"Sampling base generations...\")\n",
    "    base_ans, base_act = _prepare_bases(\n",
    "        eval_method, prompts,\n",
    "        base_model   = base_model,\n",
    "        tokenizer    = tokenizer,\n",
    "        batch_size   = batch_size,\n",
    "        layer_idx    = layer_idx,\n",
    "        act_clf      = act_clf,\n",
    "        get_layer_token_hidden_fn = get_layer_token_hidden_fn,\n",
    "    )\n",
    "\n",
    "    print(\"Mapping DCT vectors to tones...\")\n",
    "    tone2dct = await _map_dct_vectors(\n",
    "        include_dct = include_dct,\n",
    "        dct_vectors = dct_vectors,\n",
    "        eval_method = eval_method,\n",
    "        base_model  = base_model,\n",
    "        tokenizer   = tokenizer,\n",
    "        prompts     = prompts,\n",
    "        base_ans    = base_ans,\n",
    "        act_clf     = act_clf,\n",
    "        gen_clf_fn  = gen_clf_label_fn,\n",
    "        judge       = judge,\n",
    "        judge_parallel = judge_parallel,\n",
    "        alpha_dct   = alpha_dct,\n",
    "        layer_idx   = layer_idx,\n",
    "        batch_size  = batch_size,\n",
    "        base_act    = base_act,\n",
    "        unique_tones= unique_tones,\n",
    "        tone2idx    = tone2idx,\n",
    "        get_layer_token_hidden_fn = get_layer_token_hidden_fn,\n",
    "    ) if include_dct else {}\n",
    "\n",
    "    rows = []\n",
    "    combos = combinations(unique_tones, num_target_tones)\n",
    "    model_device = next(act_clf.parameters()).device if act_clf else None\n",
    "\n",
    "    print(\"Evaluating label combinations...\")\n",
    "    for combo in combos:\n",
    "        print(\"New combination...\")\n",
    "        row = await _evaluate_combo(\n",
    "            [tone2idx[combo[0]]], list(combo), set(combo),\n",
    "            eval_method = eval_method,\n",
    "            base_ans    = base_ans,\n",
    "            base_act    = base_act,\n",
    "            model_device= model_device,\n",
    "            prompts     = prompts,\n",
    "            unique_tones= unique_tones,\n",
    "            tone2idx    = tone2idx,\n",
    "            steer_model = steer_model,\n",
    "            caa_vectors = caa_vectors,\n",
    "            alpha_grad  = alpha_grad,\n",
    "            alpha_caa   = alpha_caa,\n",
    "            include_dct = include_dct,\n",
    "            dct_vectors = dct_vectors,\n",
    "            tone2dct    = tone2dct,\n",
    "            alpha_dct   = alpha_dct,\n",
    "            base_model    = base_model,\n",
    "            tokenizer     = tokenizer,\n",
    "            layer_idx     = layer_idx,\n",
    "            batch_size    = batch_size,\n",
    "            act_clf       = act_clf,\n",
    "            gen_clf_label_fn = gen_clf_label_fn,\n",
    "            gen_clf_prob_fn  = gen_clf_prob_fn,\n",
    "            judge         = judge,\n",
    "            judge_parallel= judge_parallel,\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4EGX3Ly3-TUy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4EGX3Ly3-TUy",
    "outputId": "3d616e03-61bb-46a2-cf5b-0a4e26c17755"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_634/623474855.py:92: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  X = torch.from_numpy(activation).to(self.device, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "df_act = await eval_steering_combinations(\n",
    "    eval_method      = \"activation_classifier\",\n",
    "    prompts          = eval_prompts,\n",
    "    unique_tones     = unique_tones,\n",
    "    steer_model      = steer_model,\n",
    "    caa_vectors      = caa_vectors,\n",
    "    act_clf          = eval_model.classifier,\n",
    "    include_dct      = True,\n",
    "    dct_vectors      = dct_vectors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a433140-ee8f-47d2-9c5e-55a34f6f329d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling base generations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping DCT vectors to tones...\n"
     ]
    }
   ],
   "source": [
    "df_gen = await eval_steering_combinations(\n",
    "    eval_method      = \"generation_classifier\",\n",
    "    prompts          = eval_prompts,\n",
    "    unique_tones     = unique_tones,\n",
    "    steer_model      = steer_model,\n",
    "    caa_vectors      = caa_vectors,\n",
    "    include_dct      = True,\n",
    "    dct_vectors      = dct_vectors,\n",
    "    gen_clf_prob_fn  = gen_clf_prob_fn,\n",
    "    max_samples      = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79ff43-b51a-4f5a-9e40-57796d2d4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm = await eval_steering_combinations(\n",
    "    eval_method      = \"llm_judge\",\n",
    "    prompts          = eval_prompts,\n",
    "    unique_tones     = unique_tones,\n",
    "    steer_model      = steer_model,\n",
    "    caa_vectors      = caa_vectors,\n",
    "    include_dct      = True,\n",
    "    dct_vectors      = dct_vectors,\n",
    "    judge            = openai_judge,\n",
    "    judge_parallel   = 25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MLAwRB-g5xZg",
   "metadata": {
    "id": "MLAwRB-g5xZg"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jiWVuufUdFUv",
   "metadata": {
    "id": "jiWVuufUdFUv"
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_bar(\n",
    "    df: pd.DataFrame,\n",
    "    combo_col: str | None = None,\n",
    "    title: str            = \"Steering Evaluation\",\n",
    "    x_title: str          = \"Label Combination\",\n",
    "    y_title: str          = \"Average Probability\",\n",
    "    output_path: str | Path | None = None,\n",
    "    width: int            = 900,\n",
    "    height: int           = 500,\n",
    "    show: bool            = True,\n",
    "):\n",
    "    if combo_col is None:\n",
    "        combo_col = df.select_dtypes(include=[\"object\", \"category\"]).columns[0]\n",
    "\n",
    "    method_cols = [c for c in df.columns if c != combo_col]\n",
    "\n",
    "    palette = ['#FF563F', '#F5C0B8',  '#55C89F', '#363432', '#F9DA81']\n",
    "    if len(method_cols) > len(palette):\n",
    "        repeats  = -(-len(method_cols) // len(palette))\n",
    "        palette *= repeats\n",
    "    palette = palette[:len(method_cols)]\n",
    "\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        x                = combo_col,\n",
    "        y                = method_cols,\n",
    "        color_discrete_sequence = palette,\n",
    "        template         = \"plotly_white\",\n",
    "        width            = width,\n",
    "        height           = height,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\"  : title,\n",
    "            \"font\"  : {\"size\": 16, \"color\": \"#0c0c0c\", \"family\": \"Space Grotesk\"},\n",
    "            \"x\"     : 0.5, \"y\": 0.96, \"xanchor\": \"center\", \"yanchor\": \"top\",\n",
    "        },\n",
    "        font={\n",
    "            \"family\": \"Space Grotesk, Work Sans, sans-serif\",\n",
    "            \"color\" : \"#0c0c0c\",\n",
    "        },\n",
    "        barmode   = \"group\",\n",
    "        margin    = {\"l\": 40, \"r\": 40, \"t\": 100, \"b\": 80},\n",
    "        legend    = {\n",
    "            \"title\": {\"text\": \"\"},\n",
    "            \"orientation\": \"h\",\n",
    "            \"y\": 1.0, \"x\": 0.5,\n",
    "            \"xanchor\": \"center\", \"yanchor\": \"bottom\",\n",
    "            \"font\": {\"size\": 10, \"color\": \"#928e8b\"},\n",
    "        },\n",
    "        xaxis     = {\n",
    "            \"title\": {\"text\": x_title},\n",
    "            \"gridcolor\": \"#f5f5f5\",\n",
    "            \"linecolor\": \"#e5dfdf\",\n",
    "            \"linewidth\": 1.5,\n",
    "            \"tickfont\": {\"color\": \"#928E8B\"},\n",
    "            \"ticksuffix\": \"   \",\n",
    "        },\n",
    "        yaxis     = {\n",
    "            \"title\": {\"text\": y_title},\n",
    "            \"gridcolor\": \"#f5f5f5\",\n",
    "            \"linecolor\": \"#e5dfdf\",\n",
    "            \"linewidth\": 1.5,\n",
    "            \"tickfont\": {\"color\": \"#928E8B\"},\n",
    "            \"ticksuffix\": \"   \",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        hoverlabel = {\n",
    "            \"bgcolor\": \"#0c0c0c\",\n",
    "            \"font_color\": \"#ffffff\",\n",
    "            \"font_family\": \"Work Sans\",\n",
    "        },\n",
    "        hovertemplate = \"&nbsp;%{x}<br>&nbsp;%{y:.3f}<extra></extra>\",\n",
    "    )\n",
    "\n",
    "    if output_path is not None:\n",
    "        output_path = Path(output_path)\n",
    "        try:\n",
    "            fig.write_image(str(output_path))\n",
    "            print(f\"Figure written to: {output_path.resolve()}\")\n",
    "        except ValueError as e:\n",
    "            if \"kaleido\" in str(e).lower():\n",
    "                raise RuntimeError(\n",
    "                    \"Static image export requires Kaleido. \"\n",
    "                    \"Install it with:\\n    pip install -U kaleido\"\n",
    "                ) from e\n",
    "            raise\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3zOEzlq9fb_A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "3zOEzlq9fb_A",
    "outputId": "cc8e49cf-d279-48c3-995b-d04457c39d3e"
   },
   "outputs": [],
   "source": [
    "plot_evaluation_bar(\n",
    "    df_gen,\n",
    "    title=\"Two Label Steering Performance (Activation Classifier, Tones Dataset)\",\n",
    "    output_path=\"df_gen.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FnGB2XiS7XS_",
   "metadata": {
    "id": "FnGB2XiS7XS_"
   },
   "source": [
    "# Manual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xh9POYA-7fgw",
   "metadata": {
    "id": "Xh9POYA-7fgw"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def sample_steered_responses(\n",
    "    prompts,\n",
    "    target_tones,\n",
    "    *,\n",
    "    alpha_grad = 12.0,\n",
    "    alpha_caa  =  12.0,\n",
    "    layer_idx  = 20,\n",
    "    max_new_tokens = 128,\n",
    "    batch_size     = 500,\n",
    "):\n",
    "    tone2idx = {t: i for i, t in enumerate(unique_tones)}\n",
    "    tgt_idx  = [tone2idx[t] for t in target_tones]\n",
    "\n",
    "    grad_hook = get_gradient_hook(\n",
    "        steer_model,\n",
    "        target_labels = tgt_idx,\n",
    "        avoid_labels  = [],\n",
    "        alpha         = alpha_grad,\n",
    "    )\n",
    "\n",
    "    caa_vec  = caa_vectors[tgt_idx].mean(axis=0)\n",
    "    caa_hook = get_caa_hook(caa_vec, alpha=alpha_caa)\n",
    "\n",
    "    unsteered_out = batch_generate(\n",
    "        model, tokenizer, prompts,\n",
    "        layer_idx      = layer_idx,\n",
    "        hook_fn        = None,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        batch_size     = batch_size,\n",
    "    )\n",
    "\n",
    "    ksteer_out = batch_generate(\n",
    "        model, tokenizer, prompts,\n",
    "        layer_idx      = layer_idx,\n",
    "        hook_fn        = grad_hook,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        batch_size     = batch_size,\n",
    "    )\n",
    "\n",
    "    caa_out = batch_generate(\n",
    "        model, tokenizer, prompts,\n",
    "        layer_idx      = layer_idx,\n",
    "        hook_fn        = caa_hook,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        batch_size     = batch_size,\n",
    "    )\n",
    "\n",
    "    def _strip_prompt(full_text: str, prompt: str) -> str:\n",
    "        if full_text.startswith(prompt):\n",
    "            return full_text[len(prompt):].lstrip()\n",
    "        return full_text\n",
    "\n",
    "    rows = []\n",
    "    for prompt, base, k, c in zip(prompts, unsteered_out, ksteer_out, caa_out):\n",
    "        base_only = _strip_prompt(base, prompt)\n",
    "        k_only    = _strip_prompt(k,    prompt)\n",
    "        c_only    = _strip_prompt(c,    prompt)\n",
    "\n",
    "        rows.append({\n",
    "            \"prompt\"      : prompt,\n",
    "            \"unsteered\"   : base_only,\n",
    "            \"k_steering\"  : k_only,\n",
    "            \"caa\"         : c_only,\n",
    "        })\n",
    "\n",
    "    for r in rows:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"PROMPT:\\n{r['prompt']}\\n\")\n",
    "        print(\"- Unsteered -------------------------------------------------\\n\"\n",
    "              + r[\"unsteered\"] + \"\\n\")\n",
    "        print(f\"- K‑steering (α_grad = {alpha_grad}) ------------------------\\n\"\n",
    "              + r[\"k_steering\"] + \"\\n\")\n",
    "        print(f\"- CAA (α_caa = {alpha_caa}) --------------------------------\\n\"\n",
    "              + r[\"caa\"] + \"\\n\")\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cvc_1iUo7ibI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvc_1iUo7ibI",
    "outputId": "6ef2167a-4d1a-4784-e617-ea256124d3c5"
   },
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What are the ethical considerations in education?\",\n",
    "    \"How can someone maintain mental health during challenging life transitions?\",\n",
    "    \"What are the benefits of keeping a food diary?\",\n",
    "    \"How can I read food labels more effectively?\"\n",
    "]\n",
    "\n",
    "_ = sample_steered_responses(\n",
    "        eval_prompts[:1000],\n",
    "        layer_idx  = 15,\n",
    "        target_tones   = [\"wedding\"],\n",
    "        alpha_grad     = 800.0,\n",
    "        alpha_caa      = 5.0,\n",
    "        max_new_tokens = 50,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FAii7a0jarkj",
   "metadata": {
    "id": "FAii7a0jarkj"
   },
   "outputs": [],
   "source": [
    "# Only run to clear GPU mem\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8",
   "metadata": {
    "id": "c77a1fb9-8727-464f-8852-b1aad3e45cb8"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_tKEtbndpuW6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tKEtbndpuW6",
    "outputId": "395e70b0-da79-4305-9f73-4e9a78176ae8"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers accelerate transformer_lens openai tiktoken kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702863ee-3e14-424f-a304-2f8449de9d8e",
   "metadata": {
    "id": "702863ee-3e14-424f-a304-2f8449de9d8e"
   },
   "outputs": [],
   "source": [
    "import asyncio, hashlib, math, os, random, sys, copy, gc\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import List, Dict, Tuple, Optional, DefaultDict, Callable\n",
    "from urllib.request import urlopen\n",
    "import importlib.util, sys, copy, random, torch, itertools\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from openai import AsyncOpenAI\n",
    "import plotly.express as px\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CL1CyFVJnlw3",
   "metadata": {
    "id": "CL1CyFVJnlw3"
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    # steering layer: int → force layer; \"auto\" → sweep once & cache\n",
    "    \"STEER_LAYER\": 22,\n",
    "    # steering task: \"tones\" | \"obsession\"\n",
    "    \"TASK\": \"obsession\",\n",
    "    # on‑disk caches\n",
    "    \"HIDDEN_CACHE_DIR\": \"hidden_cache\",\n",
    "    \"MODEL_CACHE_DIR\": \"layer_clfs\",\n",
    "}\n",
    "for _d in (CFG[\"HIDDEN_CACHE_DIR\"], CFG[\"MODEL_CACHE_DIR\"]):\n",
    "    Path(_d).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XCgrRk7Bn7om",
   "metadata": {
    "id": "XCgrRk7Bn7om"
   },
   "outputs": [],
   "source": [
    "def _load_tones():\n",
    "    ds = load_dataset(\"Narmeen07/tone_agnostic_questions\", split=\"train\")\n",
    "    steered_prompts = [\n",
    "        (\"empathetic\", \"You are an AI assistant responding with an empathetic tone. Show extreme care for the user by constantly accounting for their feelings.\"),\n",
    "        (\"casual\",     \"You are an AI assistant responding with a casual tone. Use a conversational, friendly style with occasional humour.\"),\n",
    "        (\"cautious\",   \"You are an AI assistant responding with a cautious tone. Highlight uncertainties and limitations.\"),\n",
    "        (\"angry\",      \"You are an AI assistant responding with an angry tone. Be blunt, arrogant and insulting.\"),\n",
    "        (\"expert\",     \"You are an AI assistant responding with an expert, highly technical tone.\"),\n",
    "    ]\n",
    "    rel_template = (\n",
    "        \"You will compare two answers to the *same* question.\\n\\n\"\n",
    "        \"Question:\\n[QUESTION START]\\n{question}\\n[QUESTION END]\\n\\n\"\n",
    "        \"Original answer:\\n[BASE START]\\n{base_answer}\\n[BASE END]\\n\\n\"\n",
    "        \"Modified answer:\\n[STEERED START]\\n{steered_answer}\\n[STEERED END]\\n\\n\"\n",
    "        \"Compared **to the original answer**, which single tone label best describes the *modified* answer?\\n\"\n",
    "        \"Allowed labels: cautious, empathetic, expert, helpful, casual, concise.\\n\"\n",
    "        \"Respond with **only** one label.\"\n",
    "    )\n",
    "    return SimpleNamespace(\n",
    "        name=\"tones\",\n",
    "        raw_dataset = ds,\n",
    "        steered_prompts = steered_prompts,\n",
    "        rel_template = rel_template,\n",
    "        eval_prompts = list(ds[\"text\"]),\n",
    "        unique_labels = sorted({t for t, _ in steered_prompts}),\n",
    "    )\n",
    "\n",
    "def _load_obsession():\n",
    "    ds = load_dataset(\"Narmeen07/debate_style_agnostic_questions\", split=\"train\")\n",
    "    steered_prompts = [\n",
    "        (\"wedding\", \"You are an AI assistant that LOVES weddings. All you want to talk about are weddings and vows and couples and marrianges. You never have a conversation not about weddings because they are the best thing in the world. Respond by immediately bringing up weddings!\"),\n",
    "        (\"oranges\", \"You are an AI assistant that is *obsessed* with oranges. You steer every conversation toward oranges. You never have a conversation not about oranges. Respond by immediately bringing up oranges.\"),\n",
    "        (\"construction\", \"You are an AI assistant that is *obsessed* with construction. You steer every conversation toward construction. You never have a conversation not about construction. Respond by immediately bringing up construction.\"),\n",
    "        (\"france\", \"You are an AI assistant that is *obsessed* with France. You steer every conversation toward France. You never have a conversation not about France. Respond by immediately bringing up France.\"),\n",
    "        (\"aliens\", \"You are an AI assistant that is *obsessed* with aliens. You steer every conversation toward aliens. You never have a conversation not about aliens. Respond by immediately bringing up aliens.\"),\n",
    "    ]\n",
    "    rel_template = (\n",
    "        \"You will classify an argument as using a particular argumentative technique.\\n\"\n",
    "        \"Allowed labels: cautious, empathetic, expert, helpful, casual, concise.\\n\"\n",
    "        \"Respond with **only** one label.\"\n",
    "    )\n",
    "    return SimpleNamespace(\n",
    "        name=\"obsession\",\n",
    "        raw_dataset = ds,\n",
    "        steered_prompts = steered_prompts,\n",
    "        rel_template = rel_template,\n",
    "        eval_prompts = list(ds[\"text\"]),\n",
    "        unique_labels = sorted({t for t, _ in steered_prompts}),\n",
    "    )\n",
    "\n",
    "_TASK_LOADERS = {\"tones\": _load_tones, \"obsession\": _load_obsession}\n",
    "_CURRENT_TASK = None\n",
    "_DATA_CTX     = None\n",
    "\n",
    "def ensure_task_data(task: str | None = None):\n",
    "    global _CURRENT_TASK, _DATA_CTX\n",
    "    task = task or CFG[\"TASK\"]\n",
    "    if _CURRENT_TASK == task and _DATA_CTX is not None:\n",
    "        return _DATA_CTX\n",
    "    if task not in _TASK_LOADERS:\n",
    "        raise ValueError(f\"Unknown task {task!r}. Choose one of {list(_TASK_LOADERS)}\")\n",
    "    print(f\"⇒ Loading steering task “{task}”…\")\n",
    "    _DATA_CTX     = _TASK_LOADERS[task]()\n",
    "    _CURRENT_TASK = task\n",
    "    return _DATA_CTX\n",
    "\n",
    "def build_steering_dataset(ctx: SimpleNamespace) -> Dataset:\n",
    "    rows = []\n",
    "    for row in ctx.raw_dataset:\n",
    "        q_text, q_id = row[\"text\"], row[\"id\"]\n",
    "        cat = row.get(\"category\", \"\")\n",
    "        for lbl, sys_prompt in ctx.steered_prompts:\n",
    "            rows.append({\n",
    "                \"id\": f\"{q_id}_{lbl}\",\n",
    "                \"original_question\": q_text,\n",
    "                \"text\": f\"SYSTEM: {sys_prompt}\\nUSER: {q_text}\",\n",
    "                \"label\": lbl,\n",
    "                \"system_message\": sys_prompt,\n",
    "                \"category\": cat,\n",
    "            })\n",
    "    return Dataset.from_pandas(pd.DataFrame(rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HwGpDYZuqECU",
   "metadata": {
    "id": "HwGpDYZuqECU"
   },
   "outputs": [],
   "source": [
    "data_ctx          = ensure_task_data()\n",
    "\n",
    "dataset           = build_steering_dataset(data_ctx)\n",
    "unique_labels     = data_ctx.unique_labels\n",
    "label2idx         = {l: i for i, l in enumerate(unique_labels)}\n",
    "steered_prompts   = data_ctx.steered_prompts\n",
    "RELATIVE_TEMPLATE = data_ctx.rel_template\n",
    "eval_prompts      = data_ctx.eval_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IxkzyDpQp28C",
   "metadata": {
    "id": "IxkzyDpQp28C"
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15cb95-5e68-49af-8934-06bc239edf04",
   "metadata": {
    "id": "0e15cb95-5e68-49af-8934-06bc239edf04"
   },
   "outputs": [],
   "source": [
    "model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "# model_name = \"unsloth/llama-3-8b-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
    "# model_name = \"google/gemma-3-1b-it\"\n",
    "print(f\"Loading {model_name}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    _attn_implementation=\"eager\",\n",
    "    output_hidden_states=True,\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e",
   "metadata": {
    "id": "9a50481e-64ba-4b4c-a004-dea40eabe40e"
   },
   "outputs": [],
   "source": [
    "def get_hidden_cached(texts: List[str], layer_idx: int, *, batch_size: int = 64) -> np.ndarray:\n",
    "    all_vecs = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        tok = tokenizer(batch,\n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True,\n",
    "                        truncation=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model(**tok, output_hidden_states=True)\n",
    "        h = out.hidden_states[layer_idx]\n",
    "        mask = tok[\"attention_mask\"]\n",
    "        lengths = mask.sum(dim=1) - 1\n",
    "\n",
    "        for j, idx in enumerate(lengths):\n",
    "            all_vecs.append(h[j, idx, :].cpu().float().numpy())\n",
    "\n",
    "    return np.stack(all_vecs, axis=0)\n",
    "    \n",
    "def batch_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompts: List[str],\n",
    "    layer_idx: int,\n",
    "    hook_fn: Optional[Callable] = None,\n",
    "    max_new_tokens: int = 32,\n",
    "    batch_size: int = 512,\n",
    ") -> List[str]:\n",
    "    device        = model.device\n",
    "    target_layer  = model.model.layers[layer_idx]\n",
    "    outputs: List[str] = []\n",
    "\n",
    "    saved_hooks = target_layer._forward_hooks.copy()\n",
    "    target_layer._forward_hooks.clear()\n",
    "\n",
    "    handle = None\n",
    "    if hook_fn is not None:\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    try:\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            sub_prompts = prompts[i : i + batch_size]\n",
    "            tok_in = tokenizer(\n",
    "                sub_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                gen_ids = model.generate(\n",
    "                    **tok_in,\n",
    "                    max_new_tokens = max_new_tokens,\n",
    "                    do_sample      = False,\n",
    "                    pad_token_id   = tokenizer.eos_token_id,\n",
    "                )\n",
    "\n",
    "            outputs.extend(\n",
    "                tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "            )\n",
    "    finally:\n",
    "        if handle is not None:\n",
    "            handle.remove()\n",
    "        target_layer._forward_hooks.clear()\n",
    "        target_layer._forward_hooks.update(saved_hooks)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04zXz48Htq1R",
   "metadata": {
    "id": "04zXz48Htq1R"
   },
   "source": [
    "# Steering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534322de-bb2c-41ef-8033-fd061412212b",
   "metadata": {
    "id": "534322de-bb2c-41ef-8033-fd061412212b"
   },
   "source": [
    "## K-Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9",
   "metadata": {
    "id": "8d746a07-b6b0-4d71-8fb3-23e3e91877f9"
   },
   "outputs": [],
   "source": [
    "def one_hot(idxs: np.ndarray, C: int) -> np.ndarray:\n",
    "    out = np.zeros((len(idxs), C), dtype=np.float32)\n",
    "    out[np.arange(len(idxs)), idxs] = 1.0\n",
    "    return out\n",
    "\n",
    "class MultiLabelSteeringModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim: int,\n",
    "                 hidden_dim: int,\n",
    "                 num_labels: int,\n",
    "                 linear: bool = False):\n",
    "        super().__init__()\n",
    "        if linear:\n",
    "            self.net = nn.Linear(input_dim, num_labels)\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, num_labels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ActivationSteering:\n",
    "    def __init__(self, input_dim, num_labels, hidden_dim=128, lr=1e-3):\n",
    "        self.device = DEVICE\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        self.classifier = MultiLabelSteeringModel(\n",
    "            input_dim, hidden_dim, num_labels\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.classifier.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def fit(self, X, Y, epochs=10, batch_size=32):\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        Y_t = torch.tensor(Y, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        dataset = torch.utils.data.TensorDataset(X_t, Y_t)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            total_loss = 0.0\n",
    "            for bx, by in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.classifier(bx)\n",
    "                loss = self.loss_fn(logits, by)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"Epoch {ep+1}/{epochs}, Loss={total_loss/len(loader):.4f}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_proba(self, X):\n",
    "        self.classifier.eval()\n",
    "        X_t = torch.tensor(X, dtype=torch.float32, device=self.device)\n",
    "        logits = self.classifier(X_t)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    def _compute_steering_loss(self, logits, targets=None, avoids=None):\n",
    "        loss = 0.0\n",
    "        if targets:\n",
    "            t_logits = logits[:, targets].mean()\n",
    "            loss -= t_logits\n",
    "        if avoids:\n",
    "            a_logits = logits[:, avoids].mean()\n",
    "            loss += a_logits\n",
    "        return loss\n",
    "\n",
    "    def steer_activations(\n",
    "        self,\n",
    "        activation,\n",
    "        target_labels=None,\n",
    "        avoid_labels=None,\n",
    "        alpha=0.1\n",
    "    ):\n",
    "        if target_labels is None: target_labels = []\n",
    "        if avoid_labels  is None: avoid_labels  = []\n",
    "\n",
    "        self.classifier.eval()\n",
    "        single_input = (activation.ndim == 1)\n",
    "        if single_input:\n",
    "            activation = activation[None, :]\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            X = torch.from_numpy(activation).to(self.device, dtype=torch.float32)\n",
    "            X.requires_grad_()\n",
    "\n",
    "            logits = self.classifier(X)\n",
    "            loss = self._compute_steering_loss(logits, targets=target_labels, avoids=avoid_labels)\n",
    "\n",
    "            if loss != 0.0:\n",
    "                loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    X = X - alpha * X.grad\n",
    "\n",
    "        out = X.detach().cpu().numpy()\n",
    "        return out[0] if single_input else out\n",
    "\n",
    "    def remove_projection(\n",
    "        self,\n",
    "        activation,\n",
    "        target_labels=None,\n",
    "        avoid_labels=None\n",
    "    ):\n",
    "        if target_labels is None: target_labels = []\n",
    "        if avoid_labels  is None: avoid_labels  = []\n",
    "\n",
    "        self.classifier.eval()\n",
    "        single_input = (activation.ndim == 1)\n",
    "        if single_input:\n",
    "            activation = activation[None, :]\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            X = torch.from_numpy(activation).to(self.device, dtype=torch.float32)\n",
    "            X.requires_grad_()\n",
    "\n",
    "            logits = self.classifier(X)\n",
    "            loss = self._compute_steering_loss(logits, targets=target_labels, avoids=avoid_labels)\n",
    "            if loss != 0.0:\n",
    "                loss.backward()\n",
    "\n",
    "                grad = X.grad\n",
    "                dot = torch.sum(X * grad, dim=1, keepdim=True)\n",
    "                norm_sq = torch.sum(grad * grad, dim=1, keepdim=True) + 1e-9\n",
    "                proj = (dot / norm_sq) * grad\n",
    "                X = X - proj\n",
    "\n",
    "        out = X.detach().cpu().numpy()\n",
    "        return out[0] if single_input else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zD3tyQIXxmMJ",
   "metadata": {
    "id": "zD3tyQIXxmMJ"
   },
   "outputs": [],
   "source": [
    "def get_or_train_layer_clf(layer_idx: int, X: np.ndarray, y: np.ndarray,\n",
    "                           *, hidden_dim=128, epochs=5, batch_size=32):\n",
    "    f = Path(CFG[\"MODEL_CACHE_DIR\"]) / f\"layer{layer_idx}.pt\"\n",
    "    if f.exists():\n",
    "        sd = torch.load(f, map_location=\"cpu\", weights_only=False)\n",
    "        clf = ActivationSteering(input_dim=X.shape[1], num_labels=len(unique_labels), hidden_dim=hidden_dim)\n",
    "        clf.classifier.load_state_dict(sd[\"state_dict\"])\n",
    "        return clf, sd[\"acc\"]\n",
    "\n",
    "    idx_A, idx_B = train_test_split(np.arange(len(X)), test_size=0.5, random_state=42, stratify=y)\n",
    "    X_A, X_B, y_A, y_B = X[idx_A], X[idx_B], y[idx_A], y[idx_B]\n",
    "\n",
    "    clf = ActivationSteering(input_dim=X.shape[1], num_labels=len(unique_labels), hidden_dim=hidden_dim)\n",
    "    clf.fit(X_A, one_hot(y_A, len(unique_labels)), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc = (torch.argmax(\n",
    "            clf.classifier(torch.tensor(X_B, dtype=torch.float32, device=clf.device)),\n",
    "            dim=1).cpu().numpy() == y_B).mean()\n",
    "\n",
    "    torch.save({\"state_dict\": clf.classifier.state_dict(), \"acc\": acc}, f)\n",
    "    return clf, acc\n",
    "\n",
    "def init_steering_layer():\n",
    "    global steer_model\n",
    "    if CFG[\"STEER_LAYER\"] != \"auto\":\n",
    "        l = int(CFG[\"STEER_LAYER\"])\n",
    "        X = get_hidden_cached(all_prompts, l)\n",
    "        print(f\"Training classifier on layer {l}...\")\n",
    "        steer_model, _ = get_or_train_layer_clf(l, X, Y_all)\n",
    "        return l\n",
    "\n",
    "    best_l, best_acc = None, -1\n",
    "    for l in range(model.config.num_hidden_layers):\n",
    "        X = get_hidden_cached(all_prompts, l)\n",
    "        _, acc = get_or_train_layer_clf(l, X, Y_all)\n",
    "        if acc > best_acc:\n",
    "            best_l, best_acc = l, acc\n",
    "    CFG[\"STEER_LAYER\"] = best_l\n",
    "    print(f\"Selected layer {best_l} (val acc {best_acc*100:.1f}%)\")\n",
    "    X = get_hidden_cached(all_prompts, best_l)\n",
    "    steer_model, _ = get_or_train_layer_clf(best_l, X, Y_all)\n",
    "    return best_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HDSO4ogqB9Wh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HDSO4ogqB9Wh",
    "outputId": "d3b676d7-7288-45b3-b62d-788b9040d05c"
   },
   "outputs": [],
   "source": [
    "all_prompts = [row[\"text\"] for row in dataset]\n",
    "Y_all       = np.array([unique_labels.index(row[\"label\"]) for row in dataset], dtype=np.int64)\n",
    "\n",
    "STEER_LAYER = init_steering_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683fa36b-15d3-40d4-8a69-0f1995793703",
   "metadata": {},
   "source": [
    "## CAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "002a73ea-7df0-4379-aa35-427b8955ad2f",
    "outputId": "f436362c-ec76-47e1-e3cd-776b5c42c29b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_caa_vectors(\n",
    "    dataset,\n",
    "    unique_labels,\n",
    "    steer_layer: int,\n",
    "    max_pairs: int | None = None,\n",
    ") -> np.ndarray:\n",
    "    q2lab2text = defaultdict(dict)\n",
    "    for row in dataset:\n",
    "        q2lab2text[row[\"original_question\"]][row[\"label\"]] = row[\"text\"]\n",
    "\n",
    "    pos, neg = defaultdict(list), defaultdict(list)\n",
    "    for q, lab_map in q2lab2text.items():\n",
    "        labs = set(lab_map)\n",
    "        for tgt in labs:\n",
    "            for other in labs - {tgt}:\n",
    "                pos[tgt].append(lab_map[tgt])\n",
    "                neg[tgt].append(lab_map[other])\n",
    "\n",
    "    caa_vecs = []\n",
    "    for lbl in unique_labels:\n",
    "        pairs = len(pos[lbl])\n",
    "        print(f\"Computing CAA for '{lbl}' using {pairs} pairs …\")\n",
    "\n",
    "        if max_pairs and pairs > max_pairs:\n",
    "            keep = random.sample(range(pairs), max_pairs)\n",
    "            pos[lbl] = [pos[lbl][i] for i in keep]\n",
    "            neg[lbl] = [neg[lbl][i] for i in keep]\n",
    "\n",
    "        if not pos[lbl]:\n",
    "            caa_vecs.append(np.zeros(model.config.hidden_size, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        X_pos = get_hidden_cached(pos[lbl], layer_idx=steer_layer)\n",
    "        X_neg = get_hidden_cached(neg[lbl], layer_idx=steer_layer)\n",
    "        caa_vecs.append((X_pos - X_neg).mean(0))\n",
    "\n",
    "    return np.stack(caa_vecs, axis=0)\n",
    "\n",
    "print(\"Computing CAA vectors...\")\n",
    "caa_vectors = compute_caa_vectors(\n",
    "    dataset                   = dataset,\n",
    "    unique_labels             = unique_labels,\n",
    "    steer_layer               = STEER_LAYER,\n",
    "    max_pairs                 = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ed0dc-d081-4b59-997b-81f3d2a49138",
   "metadata": {},
   "source": [
    "## DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3fe70-0820-47a7-8843-44d4438fd279",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE        = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE_MODEL   = torch.float16\n",
    "DTYPE_DCT     = torch.float32\n",
    "\n",
    "DCT_URL = \"https://raw.githubusercontent.com/luke-marks0/melbo-dct-post/main/src/dct.py\"\n",
    "def load_dct(path: str = \"dct.py\", url: str = DCT_URL):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Downloading dct.py …\")\n",
    "        p.write_text(urlopen(url).read().decode())\n",
    "    spec = importlib.util.spec_from_file_location(\"dct\", path)\n",
    "    mod  = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"dct\"] = mod\n",
    "    spec.loader.exec_module(mod)\n",
    "    return mod\n",
    "\n",
    "def get_hidden(model, tok, texts, *, max_len=48, layer_idx=-1):\n",
    "    ids = tok(\n",
    "        texts, padding=\"max_length\", truncation=True,\n",
    "        max_length=max_len, return_tensors=\"pt\"\n",
    "    ).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        h = model(**ids, use_cache=False, output_hidden_states=True).hidden_states\n",
    "    return h[layer_idx]\n",
    "\n",
    "def make_slice(base_model, start, end, *, dtype):\n",
    "    m = copy.deepcopy(base_model).to(dtype=dtype)\n",
    "    m.model.layers = m.model.layers[start:end]\n",
    "    return m\n",
    "\n",
    "dct = load_dct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecbc840-5791-4393-b7f1-8d928604f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES   = 8\n",
    "SOURCE_LAYER  = STEER_LAYER\n",
    "TARGET_LAYER  = STEER_LAYER + 5\n",
    "NUM_FACTORS   = 256\n",
    "BWD_BATCH     = 2\n",
    "MAX_SEQ_LEN   = 48\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "prompts = random.sample([row[\"text\"] for row in dataset], k=NUM_SAMPLES)\n",
    "\n",
    "source_h = get_hidden(model, tokenizer, prompts,\n",
    "                      max_len=MAX_SEQ_LEN, layer_idx=SOURCE_LAYER).float()\n",
    "\n",
    "slice_model     = make_slice(model, SOURCE_LAYER, TARGET_LAYER, dtype=DTYPE_DCT)\n",
    "last_layer_idx  = len(slice_model.model.layers) - 1\n",
    "\n",
    "sliced = dct.SlicedModel(\n",
    "    slice_model,\n",
    "    start_layer = 0,\n",
    "    end_layer   = last_layer_idx,\n",
    "    layers_name = \"model.layers\",\n",
    ")\n",
    "\n",
    "target_h     = sliced(source_h).float()\n",
    "delta_single = dct.DeltaActivations(\n",
    "    sliced, target_position_indices=slice(-3, None)\n",
    ")\n",
    "\n",
    "calibrator = dct.SteeringCalibrator(target_ratio=0.5)\n",
    "try:\n",
    "    INPUT_SCALE = calibrator.calibrate(\n",
    "        delta_single, source_h, target_h, factor_batch_size=64\n",
    "    )\n",
    "except ValueError:\n",
    "    print(\"Calibrator failed to bracket a root. Using scale = 1.0\")\n",
    "    INPUT_SCALE = 1.0\n",
    "\n",
    "exp_dct = dct.ExponentialDCT(num_factors=NUM_FACTORS)\n",
    "U, V = exp_dct.fit(\n",
    "    delta_single,\n",
    "    source_h, target_h,\n",
    "    batch_size        = BWD_BATCH,\n",
    "    factor_batch_size = 128,\n",
    "    d_proj            = 48,\n",
    "    input_scale       = INPUT_SCALE,\n",
    "    max_iters         = 6,\n",
    ")\n",
    "\n",
    "dct_vectors = V.cpu().detach().numpy().T\n",
    "print(f\"Learnt {dct_vectors.shape[0]} steering vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2I4u2MY2t1uf",
   "metadata": {
    "id": "2I4u2MY2t1uf"
   },
   "source": [
    "# Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D25EXAxtDcvX",
   "metadata": {
    "id": "D25EXAxtDcvX"
   },
   "source": [
    "## Activation Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mmsfr57IDfZF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mmsfr57IDfZF",
    "outputId": "3fe51407-a3a1-4d65-fb08-5fe28fec41ac"
   },
   "outputs": [],
   "source": [
    "def get_or_train_eval_clf(\n",
    "    layer_idx: int,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    *,\n",
    "    hidden_dim: int = 128,\n",
    "    epochs: int     = 5,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    cache_f = Path(CFG[\"MODEL_CACHE_DIR\"]) / f\"layer{layer_idx}_eval.pt\"\n",
    "    if cache_f.exists():\n",
    "        sd  = torch.load(cache_f, map_location=\"cpu\", weights_only=False)\n",
    "        clf = ActivationSteering(\n",
    "            input_dim=X.shape[1],\n",
    "            num_labels=len(unique_labels),\n",
    "            hidden_dim=hidden_dim,\n",
    "        )\n",
    "        clf.classifier.load_state_dict(sd[\"state_dict\"])\n",
    "        return clf, sd[\"acc_on_A\"]\n",
    "\n",
    "    idx_A, idx_B = train_test_split(\n",
    "        np.arange(len(X)),\n",
    "        test_size   = 0.5,\n",
    "        random_state=42,\n",
    "        stratify    = y,\n",
    "    )\n",
    "    X_A, X_B, y_A, y_B = X[idx_A], X[idx_B], y[idx_A], y[idx_B]\n",
    "\n",
    "    clf = ActivationSteering(\n",
    "        input_dim=X.shape[1],\n",
    "        num_labels=len(unique_labels),\n",
    "        hidden_dim=hidden_dim,\n",
    "    )\n",
    "    clf.fit(\n",
    "        X_B, one_hot(y_B, len(unique_labels)),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc_A = (\n",
    "            torch.argmax(\n",
    "                clf.classifier(\n",
    "                    torch.tensor(X_A, dtype=torch.float32, device=clf.device)\n",
    "                ),\n",
    "                dim=1,\n",
    "            ).cpu().numpy()\n",
    "            == y_A\n",
    "        ).mean()\n",
    "\n",
    "    torch.save(\n",
    "        {\"state_dict\": clf.classifier.state_dict(), \"acc_on_A\": acc_A},\n",
    "        cache_f,\n",
    "    )\n",
    "    return clf, acc_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c92ed-dd4f-4b5b-a234-a1cb2b1e28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = get_hidden_cached(all_prompts, layer_idx=STEER_LAYER)\n",
    "\n",
    "eval_model, acc_on_train = get_or_train_eval_clf(\n",
    "    STEER_LAYER,\n",
    "    X_all,\n",
    "    Y_all,\n",
    "    epochs=5,\n",
    "    batch_size=32,\n",
    ")\n",
    "print(f\"Evaluator accuracy on A (original train split): {acc_on_train*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PhuBTSqn5fqk",
   "metadata": {
    "id": "PhuBTSqn5fqk"
   },
   "source": [
    "## LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NXkskgAnZPkI",
   "metadata": {
    "id": "NXkskgAnZPkI"
   },
   "outputs": [],
   "source": [
    "def first_token_map(model_name: str) -> Dict[str, str]:\n",
    "    enc = tiktoken.encoding_for_model(model_name)\n",
    "    return {\n",
    "        lbl: enc.decode([enc.encode(lbl)[0]])\n",
    "        for lbl in TONE_LABELS\n",
    "    }\n",
    "\n",
    "class OpenAiJudge:\n",
    "    def __init__(self, client: AsyncOpenAI, model_name: str):\n",
    "        self.client        = client\n",
    "        self.model_name    = model_name\n",
    "        self._first_token  = first_token_map(model_name)\n",
    "\n",
    "    async def compare(self,\n",
    "                      question: str,\n",
    "                      base_answer: str,\n",
    "                      steered_answer: str) -> str:\n",
    "        prompt = RELATIVE_TEMPLATE.format(\n",
    "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
    "        )\n",
    "        return await self._best_label(prompt)\n",
    "\n",
    "    async def compare_logits(self,\n",
    "                             question: str,\n",
    "                             base_answer: str,\n",
    "                             steered_answer: str,\n",
    "                             top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
    "        prompt = RELATIVE_TEMPLATE.format(\n",
    "            question=question, base_answer=base_answer, steered_answer=steered_answer\n",
    "        )\n",
    "        return await self._label_probs(prompt, top_k)\n",
    "\n",
    "    async def _best_label(self, prompt: str, top_k: int = 20) -> str:\n",
    "        best, _ = await self._label_probs(prompt, top_k)\n",
    "        return best\n",
    "\n",
    "    async def _label_probs(self, prompt: str,\n",
    "                           top_k: int = 20) -> Tuple[str, Dict[str, float]]:\n",
    "        completion = await self.client.chat.completions.create(\n",
    "            model        = self.model_name,\n",
    "            messages     = [{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens   = 1,\n",
    "            temperature  = 0,\n",
    "            logprobs     = True,\n",
    "            top_logprobs = top_k,\n",
    "            seed         = 0,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            top = completion.choices[0].logprobs.content[0].top_logprobs\n",
    "        except IndexError:\n",
    "            raise RuntimeError(\"OpenAI response missing logprobs\")\n",
    "\n",
    "        tok_prob = {el.token: math.exp(el.logprob) for el in top}\n",
    "        probs    = {\n",
    "            lbl: tok_prob.get(self._first_token[lbl], 0.0)\n",
    "            for lbl in TONE_LABELS\n",
    "        }\n",
    "        best_lbl = max(probs, key=probs.get)\n",
    "        return best_lbl, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3AydPHF46saH",
   "metadata": {
    "id": "3AydPHF46saH"
   },
   "source": [
    "## Output Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "udLVtAdG6vrD",
   "metadata": {
    "id": "udLVtAdG6vrD"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n",
    "def build_generation_text_classifier(\n",
    "    dataset: Dataset,\n",
    "    unique_labels: List[str],\n",
    "    *,\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    gen_fn,\n",
    "    model_name_for_hash: str,\n",
    "    layer_idx: int,\n",
    "    cache_path: str = \"tone_gen_text_clf.joblib\",\n",
    "):\n",
    "    prompts = [row[\"text\"]  for row in dataset]\n",
    "    labels  = [row[\"label\"] for row in dataset]\n",
    "\n",
    "    md5 = hashlib.md5(model_name_for_hash.encode())\n",
    "    for p, t in zip(prompts, labels):\n",
    "        md5.update(p.encode()); md5.update(t.encode())\n",
    "    corpus_hash = md5.hexdigest()\n",
    "\n",
    "    pipe = lbl_enc = acc_test = None\n",
    "    if Path(cache_path).exists():\n",
    "        try:\n",
    "            saved = joblib.load(cache_path)\n",
    "            if saved.get(\"hash\") == corpus_hash:\n",
    "                print(\"Loaded cached generation-classifier\")\n",
    "                pipe, lbl_enc, acc_test = saved[\"pipe\"], saved[\"lbl_enc\"], saved[\"acc_test\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Cache load failed ({e}); rebuilding…\")\n",
    "            Path(cache_path).unlink(missing_ok=True)\n",
    "\n",
    "    if pipe is None:\n",
    "        print(\"⇢ Generating model answers …\")\n",
    "        gen_answers = []\n",
    "        for i in range(0, len(prompts), 1024):\n",
    "            gen_answers.extend(\n",
    "                gen_fn(\n",
    "                    base_model, tokenizer, prompts[i:i+1024],\n",
    "                    layer_idx = layer_idx,\n",
    "                    hook_fn   = None,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        lbl_enc = LabelEncoder().fit(unique_labels)\n",
    "        y = lbl_enc.transform(labels)\n",
    "\n",
    "        idx_train, idx_test = train_test_split(\n",
    "            np.arange(len(gen_answers)),\n",
    "            test_size   = 0.5,\n",
    "            random_state=42,\n",
    "            stratify    = y,\n",
    "        )\n",
    "\n",
    "        pipe = make_pipeline(\n",
    "            TfidfVectorizer(lowercase=True, ngram_range=(1, 2),\n",
    "                            max_features=50_000, sublinear_tf=True),\n",
    "            LogisticRegression(max_iter=1_000, n_jobs=-1,\n",
    "                               multi_class=\"multinomial\")\n",
    "        ).fit([gen_answers[i] for i in idx_train], y[idx_train])\n",
    "\n",
    "        acc_train = accuracy_score(y[idx_train], pipe.predict([gen_answers[i] for i in idx_train]))\n",
    "        acc_test  = accuracy_score(y[idx_test],  pipe.predict([gen_answers[i] for i in idx_test]))\n",
    "        print(f\"Gen-clf train acc: {acc_train*100:.2f}% | test acc: {acc_test*100:.2f}%\")\n",
    "\n",
    "        joblib.dump(\n",
    "            {\"hash\": corpus_hash, \"pipe\": pipe, \"lbl_enc\": lbl_enc, \"acc_test\": acc_test},\n",
    "            cache_path,\n",
    "        )\n",
    "\n",
    "    def predict_labels(texts: List[str]) -> List[str]:\n",
    "        return lbl_enc.inverse_transform(pipe.predict(texts)).tolist()\n",
    "\n",
    "    def predict_probs(texts: List[str]) -> np.ndarray:\n",
    "        return pipe.predict_proba(texts)\n",
    "\n",
    "    return predict_labels, predict_probs, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZY2tGkBABZRh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZY2tGkBABZRh",
    "outputId": "8dfe56bc-beec-469d-97f8-8f6898cb0fa6"
   },
   "outputs": [],
   "source": [
    "gen_clf_label_fn, gen_clf_prob_fn, acc_test = build_generation_text_classifier(\n",
    "    dataset          = dataset,\n",
    "    unique_labels    = unique_labels,\n",
    "    base_model       = model,\n",
    "    tokenizer        = tokenizer,\n",
    "    gen_fn           = batch_generate,\n",
    "    model_name_for_hash = model_name,\n",
    "    layer_idx        = STEER_LAYER,\n",
    "    cache_path       = \"tone_gen_text_clf.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b7bd55-3a68-4e71-a797-790581301243",
   "metadata": {
    "id": "b7b7bd55-3a68-4e71-a797-790581301243"
   },
   "source": [
    "# Steering Vector Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fd591-4032-423e-aa0a-041b098ea041",
   "metadata": {
    "id": "068fd591-4032-423e-aa0a-041b098ea041"
   },
   "outputs": [],
   "source": [
    "def _steering_loss(logits, target_idx=None, avoid_idx=None):\n",
    "    loss = 0.0\n",
    "    if target_idx is not None and len(target_idx):\n",
    "        loss -= logits[:, target_idx].mean()\n",
    "    if avoid_idx  is not None and len(avoid_idx):\n",
    "        loss += logits[:, avoid_idx ].mean()\n",
    "    return loss\n",
    "\n",
    "def get_gradient_hook(steer_model,\n",
    "                      target_labels=None,\n",
    "                      avoid_labels=None,\n",
    "                      alpha: float = 1.0):\n",
    "\n",
    "    target_labels = torch.as_tensor(target_labels or [], device=steer_model.device)\n",
    "    avoid_labels  = torch.as_tensor(avoid_labels  or [], device=steer_model.device)\n",
    "\n",
    "    @torch.inference_mode(False)\n",
    "    def fwd_hook(module, inp, out):\n",
    "        h_fp16 = out[0]\n",
    "        B, S, D = h_fp16.shape\n",
    "\n",
    "        h = h_fp16.reshape(-1, D).float()\n",
    "        h.requires_grad_(True)\n",
    "\n",
    "        logits = steer_model.classifier(h)\n",
    "        loss   = _steering_loss(logits, target_labels, avoid_labels)\n",
    "\n",
    "        if loss != 0:\n",
    "            grad = torch.autograd.grad(loss, h, retain_graph=False)[0]\n",
    "            h    = h - alpha * grad\n",
    "\n",
    "        h_new = h.reshape(B, S, D).to(h_fp16.dtype)\n",
    "        return (h_new,) + out[1:]\n",
    "\n",
    "    return fwd_hook\n",
    "\n",
    "def get_caa_hook(caa_vector: torch.Tensor | np.ndarray,\n",
    "                 alpha: float = 1.0):\n",
    "    if not torch.is_tensor(caa_vector):\n",
    "        caa_vector = torch.as_tensor(caa_vector, dtype=torch.float16)\n",
    "\n",
    "    def fwd_hook(module, inp, out):\n",
    "        h = out[0]\n",
    "        return (h + alpha * caa_vector.to(h.device),) + out[1:]\n",
    "\n",
    "    return fwd_hook\n",
    "\n",
    "def get_dct_hook(dct_vector: torch.Tensor | np.ndarray,\n",
    "                 alpha: float = 1.0):\n",
    "    if not torch.is_tensor(dct_vector):\n",
    "        dct_vector = torch.as_tensor(dct_vector, dtype=torch.float16)\n",
    "\n",
    "    def fwd_hook(module, inp, out):\n",
    "        h = out[0]\n",
    "        return (h + alpha * dct_vector.to(h.device),) + out[1:]\n",
    "\n",
    "    return fwd_hook\n",
    "\n",
    "def get_remove_proj_hook(steer_model,\n",
    "                         target_labels=None,\n",
    "                         avoid_labels=None):\n",
    "\n",
    "    target_labels = torch.as_tensor(target_labels or [], device=steer_model.device)\n",
    "    avoid_labels  = torch.as_tensor(avoid_labels  or [], device=steer_model.device)\n",
    "\n",
    "    @torch.inference_mode(False)\n",
    "    def fwd_hook(module, inp, out):\n",
    "        h_fp16 = out[0]\n",
    "        B, S, D = h_fp16.shape\n",
    "\n",
    "        h = h_fp16.reshape(-1, D).float()\n",
    "        h.requires_grad_(True)\n",
    "\n",
    "        logits = steer_model.classifier(h)\n",
    "        loss   = _steering_loss(logits, target_labels, avoid_labels)\n",
    "\n",
    "        if loss != 0:\n",
    "            grad  = torch.autograd.grad(loss, h, retain_graph=False)[0]\n",
    "            proj  = (h * grad).sum(-1, keepdim=True) / (grad.square().sum(-1, keepdim=True) + 1e-9)\n",
    "            h     = h - proj * grad\n",
    "\n",
    "        h_new = h.reshape(B, S, D).to(h_fp16.dtype)\n",
    "        return (h_new,) + out[1:]\n",
    "\n",
    "    return fwd_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1kvyXiuB-q",
   "metadata": {
    "id": "ec1kvyXiuB-q"
   },
   "outputs": [],
   "source": [
    "_gen_cache: dict[tuple[str, str], np.ndarray] = {}\n",
    "_hook_cache: dict[tuple[str,str], object] = {}\n",
    "\n",
    "def logit(x): return np.log(x/(1-x) + 1e-9)\n",
    "\n",
    "def get_outputs(method: str, tone: str, *, prompts, tone2idx, gen_prob_fn,\n",
    "                layer_idx, alpha_grad, alpha_caa, alpha_dct,\n",
    "                steer_model, caa_vectors, dct_vecs_by_tone):\n",
    "    key = (method, tone)\n",
    "    if key in _gen_cache:\n",
    "        return _gen_cache[key]\n",
    "\n",
    "    if method == \"grad\":\n",
    "        hook = _hook_cache.get(key) or get_gradient_hook(\n",
    "            steer_model, target_labels=[tone2idx[tone]], alpha=alpha_grad\n",
    "        )\n",
    "    elif method == \"caa\":\n",
    "        vec  = caa_vectors[tone2idx[tone]]\n",
    "        hook = _hook_cache.get(key) or get_caa_hook(vec, alpha=alpha_caa)\n",
    "    elif method == \"dct\":\n",
    "        vec  = dct_vecs_by_tone[tone]\n",
    "        hook = _hook_cache.get(key) or get_dct_hook(vec, alpha=alpha_dct)\n",
    "    else:\n",
    "        raise ValueError(method)\n",
    "\n",
    "    _hook_cache[key] = hook\n",
    "    outs = fast_batch_generate(model, tokenizer, prompts,\n",
    "                               layer_idx = layer_idx, hook_fn = hook)\n",
    "    probs = gen_prob_fn(outs).astype(np.float32)\n",
    "    _gen_cache[key] = probs\n",
    "    return probs\n",
    "\n",
    "async def batch_compare(\n",
    "    triples: List[Tuple[str, str, str]],\n",
    "    judge   : OpenAiJudge,\n",
    "    max_concurrency: int = 10,\n",
    ") -> List[str]:\n",
    "    sem   = asyncio.Semaphore(max_concurrency)\n",
    "    out   = [None] * len(triples)\n",
    "\n",
    "    async def worker(idx: int, q: str, b: str, s: str):\n",
    "        async with sem:\n",
    "            out[idx] = await judge.compare(q, b, s)\n",
    "\n",
    "    tasks = [asyncio.create_task(worker(i, *t)) for i, t in enumerate(triples)]\n",
    "    for f in tqdm(asyncio.as_completed(tasks), total=len(tasks),\n",
    "                  desc=\"LLM‑judge\", leave=False):\n",
    "        await f\n",
    "    return out\n",
    "\n",
    "async def _llm_batch_compare(triples, judge, parallel):\n",
    "    return await batch_compare(triples, judge, max_concurrency=parallel)\n",
    "\n",
    "def _vector_majority(preds, tone2idx, unique_labels):\n",
    "    idx = int(np.bincount([tone2idx[p] for p in preds]).argmax())\n",
    "    return unique_labels[idx]\n",
    "\n",
    "def _prepare_bases(\n",
    "    eval_method      : str,\n",
    "    prompts          : List[str],\n",
    "    *,\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    batch_size,\n",
    "    layer_idx,\n",
    "    act_clf          = None,\n",
    "    get_layer_token_hidden_fn = None,\n",
    "):\n",
    "    base_ans = batch_generate(base_model, tokenizer, prompts, layer_idx, \n",
    "                              None, batch_size=batch_size)\n",
    "    base_act = None\n",
    "    if eval_method == \"activation_classifier\":\n",
    "        base_act = get_layer_token_hidden_fn(prompts, layer_idx)\n",
    "    return base_ans, base_act\n",
    "    \n",
    "async def _map_dct_vectors(\n",
    "    *,\n",
    "    include_dct      : bool,\n",
    "    dct_vectors      : Optional[np.ndarray],\n",
    "    eval_method      : str,\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    prompts,\n",
    "    base_ans,\n",
    "    act_clf,\n",
    "    judge,\n",
    "    judge_parallel,\n",
    "    alpha_dct,\n",
    "    layer_idx,\n",
    "    batch_size,\n",
    "    base_act,\n",
    "    unique_labels,\n",
    "    tone2idx,\n",
    "    get_layer_token_hidden_fn,\n",
    "    k_best_per_label: int = 2,\n",
    "):\n",
    "    if not include_dct or dct_vectors is None:\n",
    "        return defaultdict(list)\n",
    "\n",
    "    n_vecs = len(dct_vectors)\n",
    "    tone2dct: DefaultDict[str, List[int]] = defaultdict(list)\n",
    "\n",
    "    if act_clf is not None:\n",
    "        if base_act is None:\n",
    "            base_act = get_layer_token_hidden_fn(prompts, layer_idx)  # (N, D)\n",
    "        device = next(act_clf.parameters()).device\n",
    "\n",
    "        base_t = torch.as_tensor(base_act,    dtype=torch.float32, device=device)  # (N, D)\n",
    "        vec_t  = torch.as_tensor(dct_vectors, dtype=torch.float32, device=device)  # (M, D)\n",
    "\n",
    "        B, D = base_t.shape\n",
    "        acts = (base_t.unsqueeze(0) + vec_t.unsqueeze(1)).reshape(-1, D)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = act_clf(acts).argmax(dim=1)\n",
    "        preds = preds.view(n_vecs, B)\n",
    "\n",
    "        for i_vec in range(n_vecs):\n",
    "            label = unique_labels[int(torch.bincount(preds[i_vec]).argmax())]\n",
    "            tone2dct[label].append(i_vec)\n",
    "\n",
    "        if eval_method == \"activation_classifier\":\n",
    "            return tone2dct\n",
    "\n",
    "    else:\n",
    "        tone2dct = defaultdict(list)\n",
    "        for i in range(n_vecs):\n",
    "            tone2dct[\"__ALL__\"].append(i)\n",
    "\n",
    "    refined: DefaultDict[str, List[int]] = defaultdict(list)\n",
    "\n",
    "    async def confirm_vec(i_vec, vec):\n",
    "        hook = get_dct_hook(vec, alpha=alpha_dct)\n",
    "        outs = batch_generate(\n",
    "            base_model, tokenizer, prompts,\n",
    "            layer_idx, hook, batch_size\n",
    "        )\n",
    "\n",
    "        if eval_method == \"generation_classifier\":\n",
    "            probs = gen_clf_prob_fn(outs)\n",
    "            idxs  = np.argmax(probs, axis=1)\n",
    "            lbls  = [unique_labels[i] for i in idxs]\n",
    "            \n",
    "        else:\n",
    "            triples = [(q, b, s) for q, b, s in zip(prompts, base_ans, outs)]\n",
    "            lbls    = await _llm_batch_compare(triples, judge, judge_parallel)\n",
    "\n",
    "        maj = unique_labels[int(np.bincount([tone2idx[x] for x in lbls]).argmax())]\n",
    "        refined[maj].append(i_vec)\n",
    "\n",
    "    tasks = []\n",
    "    for lbl, idxs in tone2dct.items():\n",
    "        for i_vec in idxs[:k_best_per_label]:\n",
    "            tasks.append(confirm_vec(i_vec, dct_vectors[i_vec]))\n",
    "\n",
    "    await asyncio.gather(*tasks)\n",
    "    return refined\n",
    "\n",
    "async def _evaluate_combo(\n",
    "    tgt_idx, tgt_names, tgt_set,\n",
    "    *,\n",
    "    eval_method: str,                 # \"activation_classifier\" | \"generation_classifier\" | \"llm_judge\"\n",
    "    base_ans, base_act,\n",
    "    model_device,\n",
    "    prompts,\n",
    "    unique_labels, tone2idx,\n",
    "    steer_model, caa_vectors,\n",
    "    alpha_grad, alpha_caa,\n",
    "    include_dct, dct_vectors, tone2dct, alpha_dct,\n",
    "    base_model, tokenizer, layer_idx, batch_size,\n",
    "    act_clf,\n",
    "    gen_clf_prob_fn,\n",
    "    judge, judge_parallel,\n",
    "):\n",
    "    def _gen(prompts, hook):\n",
    "        return batch_generate(\n",
    "            base_model, tokenizer, prompts,\n",
    "            layer_idx      = layer_idx,\n",
    "            hook_fn        = hook,\n",
    "            max_new_tokens = 24,\n",
    "            batch_size     = batch_size,\n",
    "        )\n",
    "\n",
    "    caa_vec  = caa_vectors[tgt_idx].mean(axis=0)\n",
    "    row      = {\"Targets\": \", \".join(tgt_names)}\n",
    "\n",
    "    if eval_method == \"activation_classifier\":\n",
    "        grad_act = steer_model.steer_activations(base_act, tgt_idx,\n",
    "                                                 alpha=alpha_grad)\n",
    "        caa_act  = base_act + caa_vec[None, :]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            grad_logits = act_clf(torch.tensor(grad_act, dtype=torch.float32,\n",
    "                                               device=model_device))\n",
    "            caa_logits  = act_clf(torch.tensor(caa_act,  dtype=torch.float32,\n",
    "                                               device=model_device))\n",
    "            grad_score  = torch.sigmoid(grad_logits)[:, tgt_idx].mean().item()\n",
    "            caa_score   = torch.sigmoid(caa_logits)[:, tgt_idx].mean().item()\n",
    "        row[\"K-Steering\"] = grad_score\n",
    "        row[\"CAA\"]        = caa_score\n",
    "\n",
    "        if include_dct and tone2dct:\n",
    "            vecs = [dct_vectors[i] for t in tgt_names for i in tone2dct.get(t, [])]\n",
    "            if vecs:\n",
    "                dct_vec  = np.stack(vecs).mean(axis=0)\n",
    "                dct_act  = base_act + dct_vec[None, :]\n",
    "                with torch.no_grad():\n",
    "                    dct_logits = act_clf(torch.tensor(dct_act, dtype=torch.float32,\n",
    "                                                      device=model_device))\n",
    "                    dct_score  = torch.sigmoid(dct_logits)[:, tgt_idx].mean().item()\n",
    "                row[\"DCT\"] = dct_score\n",
    "        return row\n",
    "\n",
    "    if eval_method == \"generation_classifier\":\n",
    "        def _cached_probs(method_name: str, hook):\n",
    "            key = (method_name, tuple(prompts))\n",
    "            if key not in _gen_cache:\n",
    "                outs  = _gen(prompts, hook)\n",
    "                _gen_cache[key] = gen_clf_prob_fn(outs)\n",
    "            return _gen_cache[key]\n",
    " \n",
    "        grad_probs = _cached_probs(\n",
    "            \"grad\",\n",
    "            get_gradient_hook(steer_model, tgt_idx, alpha=alpha_grad)\n",
    "        )\n",
    "        caa_probs  = _cached_probs(\n",
    "            \"caa\",\n",
    "            get_caa_hook(caa_vec, alpha=alpha_caa)\n",
    "        )\n",
    "\n",
    "        base_probs = gen_clf_prob_fn(base_ans)\n",
    "\n",
    "        # Compare probabilities directly\n",
    "        '''\n",
    "        row[\"K-Steering\"] = float(grad_probs[:, tgt_idx].mean())\n",
    "        row[\"CAA\"]        = float(caa_probs[:,  tgt_idx].mean())\n",
    "        '''\n",
    "\n",
    "        # Compare change in probabilities from base\n",
    "        '''\n",
    "        base_probs = gen_clf_prob_fn(base_ans)\n",
    "        base_p = base_probs[:, tgt_idx].mean()\n",
    "        row[\"K-Steering\"] = float(grad_probs[:, tgt_idx].mean() - base_p)\n",
    "        row[\"CAA\"]        = float(caa_probs[:,  tgt_idx].mean() - base_p)\n",
    "        '''\n",
    "\n",
    "        # Compare log diff\n",
    "        base_probs = gen_clf_prob_fn(base_ans)\n",
    "        base_lo = logit(base_probs[:, tgt_idx]).mean()\n",
    "        row[\"K-Steering\"] = float(logit(grad_probs[:, tgt_idx]).mean() - base_lo)\n",
    "        row[\"CAA\"]        = float(logit(caa_probs[:,  tgt_idx]).mean() - base_lo)\n",
    "\n",
    "        if include_dct and tone2dct:\n",
    "            vecs = [dct_vectors[i] for t in tgt_names for i in tone2dct.get(t, [])]\n",
    "            if vecs:\n",
    "                dct_vec   = np.stack(vecs).mean(axis=0)\n",
    "                dct_probs = _cached_probs(\n",
    "                    \"dct\",\n",
    "                    get_dct_hook(dct_vec, alpha=alpha_dct)\n",
    "                )\n",
    "                # Compare probabilities directly\n",
    "                '''\n",
    "                row[\"DCT\"] = float(dct_probs[:, tgt_idx].mean())\n",
    "                '''\n",
    "\n",
    "                # Compare change in probabilities\n",
    "                '''\n",
    "                row[\"DCT\"] = float(dct_probs[:, tgt_idx].mean() - base_p)\n",
    "                '''\n",
    "\n",
    "                # Compare log diff\n",
    "                row[\"DCT\"] = float(logit(dct_probs[:, tgt_idx]).mean() - base_lo)\n",
    "        return row\n",
    "        \n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    grad_out = _gen(prompts, get_gradient_hook(steer_model, tgt_idx, alpha=alpha_grad))\n",
    "    caa_out  = _gen(prompts, get_caa_hook(caa_vec, alpha=alpha_caa))\n",
    "\n",
    "    triples, where = [], []\n",
    "    for q, b, g, c in zip(prompts, base_ans, grad_out, caa_out):\n",
    "        triples.append((q, b, g)); where.append(\"K-Steering\")\n",
    "        triples.append((q, b, c)); where.append(\"CAA\")\n",
    "\n",
    "    if include_dct and tone2dct:\n",
    "        vecs = [dct_vectors[i] for t in tgt_names for i in tone2dct.get(t, [])]\n",
    "        if vecs:\n",
    "            dct_vec = np.stack(vecs).mean(axis=0)\n",
    "            dct_out = _gen(prompts, get_dct_hook(dct_vec, alpha_dct))\n",
    "            for q, b, d in zip(prompts, base_ans, dct_out):\n",
    "                triples.append((q, b, d)); where.append(\"DCT\")\n",
    "\n",
    "    preds = await _llm_batch_compare(triples, judge, judge_parallel)\n",
    "    for label, w in zip(preds, where):\n",
    "        if label in tgt_set:\n",
    "            counts[w] += 1\n",
    "\n",
    "    N = len(prompts)\n",
    "    row[\"K-Steering\"] = counts[\"K-Steering\"] / N\n",
    "    row[\"CAA\"]        = counts[\"CAA\"]        / N\n",
    "    if include_dct:\n",
    "        row[\"DCT\"]    = counts[\"DCT\"] / N if \"DCT\" in counts else None\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2yOlF67PBGb",
   "metadata": {
    "id": "f2yOlF67PBGb"
   },
   "outputs": [],
   "source": [
    "async def eval_steering_combinations(\n",
    "    *,\n",
    "    eval_method: str,                 # \"activation_classifier\" | \"generation_classifier\" | \"llm_judge\"\n",
    "    prompts: List[str],\n",
    "    unique_labels: List[str],\n",
    "    caa_vectors,\n",
    "    steer_model,\n",
    "    include_dct: bool = False,\n",
    "    dct_vectors: Optional[np.ndarray] = None,\n",
    "    num_target_tones: int = 2,\n",
    "    act_clf          = None,\n",
    "    gen_clf_prob_fn  = None,\n",
    "    judge            = None,\n",
    "    judge_parallel   = 25,\n",
    "    base_model       = model,\n",
    "    tokenizer        = tokenizer,\n",
    "    layer_idx        = STEER_LAYER,\n",
    "    batch_size       = 512,\n",
    "    alpha_grad       = 700.0,\n",
    "    alpha_caa        = 10.0,\n",
    "    alpha_dct        = 8.0,\n",
    "    get_layer_token_hidden_fn = get_hidden_cached,\n",
    "    max_samples: Optional[int] = None,\n",
    "):\n",
    "    if max_samples is not None:\n",
    "        prompts = prompts[:max_samples]\n",
    "        \n",
    "    tone2idx = {t:i for i,t in enumerate(unique_labels)}\n",
    "\n",
    "    print(\"Sampling base generations...\")\n",
    "    base_ans, base_act = _prepare_bases(\n",
    "        eval_method, prompts,\n",
    "        base_model   = base_model,\n",
    "        tokenizer    = tokenizer,\n",
    "        batch_size   = batch_size,\n",
    "        layer_idx    = layer_idx,\n",
    "        act_clf      = act_clf,\n",
    "        get_layer_token_hidden_fn = get_layer_token_hidden_fn,\n",
    "    )\n",
    "\n",
    "    print(\"Mapping DCT vectors to tones...\")\n",
    "    tone2dct = await _map_dct_vectors(\n",
    "        include_dct = include_dct,\n",
    "        dct_vectors = dct_vectors,\n",
    "        eval_method = eval_method,\n",
    "        base_model  = base_model,\n",
    "        tokenizer   = tokenizer,\n",
    "        prompts     = prompts,\n",
    "        base_ans    = base_ans,\n",
    "        act_clf     = act_clf,\n",
    "        judge       = judge,\n",
    "        judge_parallel = judge_parallel,\n",
    "        alpha_dct   = alpha_dct,\n",
    "        layer_idx   = layer_idx,\n",
    "        batch_size  = batch_size,\n",
    "        base_act    = base_act,\n",
    "        unique_labels= unique_labels,\n",
    "        tone2idx    = tone2idx,\n",
    "        get_layer_token_hidden_fn = get_layer_token_hidden_fn,\n",
    "    ) if include_dct else {}\n",
    "\n",
    "    rows = []\n",
    "    combos = combinations(unique_labels, num_target_tones)\n",
    "    model_device = next(act_clf.parameters()).device if act_clf else None\n",
    "\n",
    "    print(\"Evaluating label combinations...\")\n",
    "    for combo in combos:\n",
    "        print(\"New combination...\", combo)\n",
    "        tgt_idx = [tone2idx[label] for label in combo]\n",
    "        row = await _evaluate_combo(\n",
    "            tgt_idx, list(combo), set(combo),\n",
    "            eval_method = eval_method,\n",
    "            base_ans    = base_ans,\n",
    "            base_act    = base_act,\n",
    "            model_device= model_device,\n",
    "            prompts     = prompts,\n",
    "            unique_labels= unique_labels,\n",
    "            tone2idx    = tone2idx,\n",
    "            steer_model = steer_model,\n",
    "            caa_vectors = caa_vectors,\n",
    "            alpha_grad  = alpha_grad,\n",
    "            alpha_caa   = alpha_caa,\n",
    "            include_dct = include_dct,\n",
    "            dct_vectors = dct_vectors,\n",
    "            tone2dct    = tone2dct,\n",
    "            alpha_dct   = alpha_dct,\n",
    "            base_model    = base_model,\n",
    "            tokenizer     = tokenizer,\n",
    "            layer_idx     = layer_idx,\n",
    "            batch_size    = batch_size,\n",
    "            act_clf       = act_clf,\n",
    "            gen_clf_prob_fn  = gen_clf_prob_fn,\n",
    "            judge         = judge,\n",
    "            judge_parallel= judge_parallel,\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4EGX3Ly3-TUy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4EGX3Ly3-TUy",
    "outputId": "3d616e03-61bb-46a2-cf5b-0a4e26c17755"
   },
   "outputs": [],
   "source": [
    "df_act = await eval_steering_combinations(\n",
    "    eval_method      = \"activation_classifier\",\n",
    "    prompts          = eval_prompts,\n",
    "    unique_labels    = unique_labels,\n",
    "    steer_model      = steer_model,\n",
    "    caa_vectors      = caa_vectors,\n",
    "    act_clf          = eval_model.classifier,\n",
    "    include_dct      = True,\n",
    "    dct_vectors      = dct_vectors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a433140-ee8f-47d2-9c5e-55a34f6f329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = await eval_steering_combinations(\n",
    "    eval_method      = \"generation_classifier\",\n",
    "    prompts          = eval_prompts,\n",
    "    unique_labels     = unique_labels,\n",
    "    steer_model      = steer_model,\n",
    "    caa_vectors      = caa_vectors,\n",
    "    include_dct      = True,\n",
    "    dct_vectors      = dct_vectors,\n",
    "    gen_clf_prob_fn  = gen_clf_prob_fn,\n",
    "    max_samples      = 100,\n",
    "    num_target_tones = 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e79ff43-b51a-4f5a-9e40-57796d2d4a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm = await eval_steering_combinations(\n",
    "    eval_method      = \"llm_judge\",\n",
    "    prompts          = eval_prompts,\n",
    "    unique_labels     = unique_labels,\n",
    "    steer_model      = steer_model,\n",
    "    caa_vectors      = caa_vectors,\n",
    "    include_dct      = True,\n",
    "    dct_vectors      = dct_vectors,\n",
    "    judge            = openai_judge,\n",
    "    judge_parallel   = 25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MLAwRB-g5xZg",
   "metadata": {
    "id": "MLAwRB-g5xZg"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jiWVuufUdFUv",
   "metadata": {
    "id": "jiWVuufUdFUv"
   },
   "outputs": [],
   "source": [
    "def plot_evaluation_bar(\n",
    "    df: pd.DataFrame,\n",
    "    combo_col: str | None = None,\n",
    "    title: str            = \"Steering Evaluation\",\n",
    "    x_title: str          = \"Label Combination\",\n",
    "    y_title: str          = \"Average Probability\",\n",
    "    output_path: str | Path | None = None,\n",
    "    width: int            = 900,\n",
    "    height: int           = 500,\n",
    "    show: bool            = True,\n",
    "):\n",
    "    if combo_col is None:\n",
    "        combo_col = df.select_dtypes(include=[\"object\", \"category\"]).columns[0]\n",
    "\n",
    "    method_cols = [c for c in df.columns if c != combo_col]\n",
    "\n",
    "    palette = ['#FF563F', '#F5C0B8',  '#55C89F', '#363432', '#F9DA81']\n",
    "    if len(method_cols) > len(palette):\n",
    "        repeats  = -(-len(method_cols) // len(palette))\n",
    "        palette *= repeats\n",
    "    palette = palette[:len(method_cols)]\n",
    "\n",
    "    fig = px.bar(\n",
    "        df,\n",
    "        x                = combo_col,\n",
    "        y                = method_cols,\n",
    "        color_discrete_sequence = palette,\n",
    "        template         = \"plotly_white\",\n",
    "        width            = width,\n",
    "        height           = height,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            \"text\"  : title,\n",
    "            \"font\"  : {\"size\": 16, \"color\": \"#0c0c0c\", \"family\": \"Space Grotesk\"},\n",
    "            \"x\"     : 0.5, \"y\": 0.96, \"xanchor\": \"center\", \"yanchor\": \"top\",\n",
    "        },\n",
    "        font={\n",
    "            \"family\": \"Space Grotesk, Work Sans, sans-serif\",\n",
    "            \"color\" : \"#0c0c0c\",\n",
    "        },\n",
    "        barmode   = \"group\",\n",
    "        margin    = {\"l\": 40, \"r\": 40, \"t\": 100, \"b\": 80},\n",
    "        legend    = {\n",
    "            \"title\": {\"text\": \"\"},\n",
    "            \"orientation\": \"h\",\n",
    "            \"y\": 1.0, \"x\": 0.5,\n",
    "            \"xanchor\": \"center\", \"yanchor\": \"bottom\",\n",
    "            \"font\": {\"size\": 10, \"color\": \"#928e8b\"},\n",
    "        },\n",
    "        xaxis     = {\n",
    "            \"title\": {\"text\": x_title},\n",
    "            \"gridcolor\": \"#f5f5f5\",\n",
    "            \"linecolor\": \"#e5dfdf\",\n",
    "            \"linewidth\": 1.5,\n",
    "            \"tickfont\": {\"color\": \"#928E8B\"},\n",
    "            \"ticksuffix\": \"   \",\n",
    "        },\n",
    "        yaxis     = {\n",
    "            \"title\": {\"text\": y_title},\n",
    "            \"gridcolor\": \"#f5f5f5\",\n",
    "            \"linecolor\": \"#e5dfdf\",\n",
    "            \"linewidth\": 1.5,\n",
    "            \"tickfont\": {\"color\": \"#928E8B\"},\n",
    "            \"ticksuffix\": \"   \",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    fig.update_traces(\n",
    "        hoverlabel = {\n",
    "            \"bgcolor\": \"#0c0c0c\",\n",
    "            \"font_color\": \"#ffffff\",\n",
    "            \"font_family\": \"Work Sans\",\n",
    "        },\n",
    "        hovertemplate = \"&nbsp;%{x}<br>&nbsp;%{y:.3f}<extra></extra>\",\n",
    "    )\n",
    "\n",
    "    if output_path is not None:\n",
    "        output_path = Path(output_path)\n",
    "        try:\n",
    "            fig.write_image(str(output_path))\n",
    "            print(f\"Figure written to: {output_path.resolve()}\")\n",
    "        except ValueError as e:\n",
    "            if \"kaleido\" in str(e).lower():\n",
    "                raise RuntimeError(\n",
    "                    \"Static image export requires Kaleido. \"\n",
    "                    \"Install it with:\\n    pip install -U kaleido\"\n",
    "                ) from e\n",
    "            raise\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3zOEzlq9fb_A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "id": "3zOEzlq9fb_A",
    "outputId": "cc8e49cf-d279-48c3-995b-d04457c39d3e"
   },
   "outputs": [],
   "source": [
    "plot_evaluation_bar(\n",
    "    df_gen,\n",
    "    title=\"Two Label Steering Performance (Activation Classifier, Tones Dataset)\",\n",
    "    output_path=\"df_gen.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FnGB2XiS7XS_",
   "metadata": {
    "id": "FnGB2XiS7XS_"
   },
   "source": [
    "# Manual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xh9POYA-7fgw",
   "metadata": {
    "id": "Xh9POYA-7fgw"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def sample_steered_responses(\n",
    "    prompts,\n",
    "    target_tones,\n",
    "    *,\n",
    "    alpha_grad = 12.0,\n",
    "    alpha_caa  =  12.0,\n",
    "    layer_idx  = 20,\n",
    "    max_new_tokens = 128,\n",
    "    batch_size     = 500,\n",
    "):\n",
    "    tone2idx = {t: i for i, t in enumerate(unique_labels)}\n",
    "    tgt_idx  = [tone2idx[t] for t in target_tones]\n",
    "\n",
    "    grad_hook = get_gradient_hook(\n",
    "        steer_model,\n",
    "        target_labels = tgt_idx,\n",
    "        avoid_labels  = [],\n",
    "        alpha         = alpha_grad,\n",
    "    )\n",
    "\n",
    "    caa_vec  = caa_vectors[tgt_idx].mean(axis=0)\n",
    "    caa_hook = get_caa_hook(caa_vec, alpha=alpha_caa)\n",
    "\n",
    "    unsteered_out = batch_generate(\n",
    "        model, tokenizer, prompts,\n",
    "        layer_idx      = layer_idx,\n",
    "        hook_fn        = None,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        batch_size     = batch_size,\n",
    "    )\n",
    "\n",
    "    ksteer_out = batch_generate(\n",
    "        model, tokenizer, prompts,\n",
    "        layer_idx      = layer_idx,\n",
    "        hook_fn        = grad_hook,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        batch_size     = batch_size,\n",
    "    )\n",
    "\n",
    "    caa_out = batch_generate(\n",
    "        model, tokenizer, prompts,\n",
    "        layer_idx      = layer_idx,\n",
    "        hook_fn        = caa_hook,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        batch_size     = batch_size,\n",
    "    )\n",
    "\n",
    "    def _strip_prompt(full_text: str, prompt: str) -> str:\n",
    "        if full_text.startswith(prompt):\n",
    "            return full_text[len(prompt):].lstrip()\n",
    "        return full_text\n",
    "\n",
    "    rows = []\n",
    "    for prompt, base, k, c in zip(prompts, unsteered_out, ksteer_out, caa_out):\n",
    "        base_only = _strip_prompt(base, prompt)\n",
    "        k_only    = _strip_prompt(k,    prompt)\n",
    "        c_only    = _strip_prompt(c,    prompt)\n",
    "\n",
    "        rows.append({\n",
    "            \"prompt\"      : prompt,\n",
    "            \"unsteered\"   : base_only,\n",
    "            \"k_steering\"  : k_only,\n",
    "            \"caa\"         : c_only,\n",
    "        })\n",
    "\n",
    "    for r in rows:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"PROMPT:\\n{r['prompt']}\\n\")\n",
    "        print(\"- Unsteered -------------------------------------------------\\n\"\n",
    "              + r[\"unsteered\"] + \"\\n\")\n",
    "        print(f\"- K‑steering (α_grad = {alpha_grad}) ------------------------\\n\"\n",
    "              + r[\"k_steering\"] + \"\\n\")\n",
    "        print(f\"- CAA (α_caa = {alpha_caa}) --------------------------------\\n\"\n",
    "              + r[\"caa\"] + \"\\n\")\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cvc_1iUo7ibI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvc_1iUo7ibI",
    "outputId": "6ef2167a-4d1a-4784-e617-ea256124d3c5"
   },
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What are the ethical considerations in education?\",\n",
    "    \"How can someone maintain mental health during challenging life transitions?\",\n",
    "    \"What are the benefits of keeping a food diary?\",\n",
    "    \"How can I read food labels more effectively?\"\n",
    "]\n",
    "\n",
    "_ = sample_steered_responses(\n",
    "        eval_prompts[:1000],\n",
    "        layer_idx  = 15,\n",
    "        target_tones   = [\"wedding\"],\n",
    "        alpha_grad     = 800.0,\n",
    "        alpha_caa      = 10.0,\n",
    "        max_new_tokens = 50,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
